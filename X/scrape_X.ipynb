{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ff3508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1d3685fcf84af8b643e76ff6f1ea5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='nyushanghai', description='Twitter Username:', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape tweets from @nyushanghai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 23:33:29,574 - INFO - Data Successfully Saved to nyushanghai.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Data saved to /Users/princess/Documents/RA/Web Scraping/X/nyushanghai.csv\n",
      "Reading CSV file: /Users/princess/Documents/RA/Web Scraping/X/nyushanghai.csv\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1184026399574654976\n",
      "Downloading image: https://pbs.twimg.com/media/EG6BulKU4AAdnge?format=jpg&name=medium as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 271k/271k [00:00<00:00, 2.64MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1184026399574654976/image_1.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1488038488528195584\n",
      "Downloading image: https://pbs.twimg.com/media/FKaSUKsUUAAPukp?format=jpg&name=large as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 515k/515k [00:00<00:00, 12.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1488038488528195584/image_1.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1266368706705125382\n",
      "Downloading image: https://pbs.twimg.com/media/EZK93q-U8AM6tjU?format=jpg&name=900x900 as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 74.2k/74.2k [00:00<00:00, 18.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1266368706705125382/image_1.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/EZK98B2VAAEgbx1?format=jpg&name=900x900 as image_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_2.jpg: 100%|██████████| 71.2k/71.2k [00:00<00:00, 10.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1266368706705125382/image_2.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1581148521473855488\n",
      "Downloading image: https://pbs.twimg.com/ext_tw_video_thumb/1581148256263802880/pu/img/WFziqAaO5AVTOGTS.jpg as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 27.5k/27.5k [00:00<00:00, 19.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1581148521473855488/image_1.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1697108903711109226\n",
      "Downloading image: https://pbs.twimg.com/media/F41XYhLbwAAVV4j?format=jpg&name=small as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 80.4k/80.4k [00:00<00:00, 27.0MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1697108903711109226/image_1.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/F41XcdGbUAAlI_i?format=jpg&name=small as image_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_2.jpg: 100%|██████████| 75.4k/75.4k [00:00<00:00, 17.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1697108903711109226/image_2.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/F41XUq1a8AAJA4s?format=jpg&name=small as image_3.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_3.jpg: 100%|██████████| 47.7k/47.7k [00:00<00:00, 39.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1697108903711109226/image_3.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/F41XgP6a4AAc0qg?format=jpg&name=small as image_4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_4.jpg: 100%|██████████| 84.1k/84.1k [00:00<00:00, 18.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1697108903711109226/image_4.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1783378627859751236\n",
      "Downloading image: https://pbs.twimg.com/media/GL_QzBuawAEl_13?format=jpg&name=small as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 68.3k/68.3k [00:00<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1783378627859751236/image_1.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/GL_Q0yha0AAPhwV?format=jpg&name=small as image_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_2.jpg: 100%|██████████| 70.6k/70.6k [00:00<00:00, 24.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1783378627859751236/image_2.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/GL_Q1P7agAEkFLD?format=jpg&name=small as image_3.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_3.jpg: 100%|██████████| 68.5k/68.5k [00:00<00:00, 826kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1783378627859751236/image_3.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/GL_Q2JeasAAGu8X?format=jpg&name=small as image_4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_4.jpg: 100%|██████████| 86.2k/86.2k [00:00<00:00, 15.0MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1783378627859751236/image_4.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1026302117890277378\n",
      "Downloading image: https://pbs.twimg.com/media/Dj4oMt_VsAAvRRi?format=jpg&name=medium as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 174k/174k [00:00<00:00, 9.92MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1026302117890277378/image_1.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1188294190460878848\n",
      "Downloading image: https://pbs.twimg.com/media/EH2rQ5qUwAAH1wu?format=jpg&name=medium as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 82.8k/82.8k [00:00<00:00, 26.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1188294190460878848/image_1.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/973822217732665344\n",
      "Downloading image: https://pbs.twimg.com/media/DYO1y60X4AEOez7?format=jpg&name=small as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 63.0k/63.0k [00:00<00:00, 42.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/973822217732665344/image_1.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/DYO18SJWsAAlq9D?format=jpg&name=small as image_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_2.jpg: 100%|██████████| 65.8k/65.8k [00:00<00:00, 63.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/973822217732665344/image_2.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/DYO1_LjXkAEix_G?format=jpg&name=small as image_3.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_3.jpg: 100%|██████████| 54.1k/54.1k [00:00<00:00, 13.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/973822217732665344/image_3.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/DYO2GQjXUAEL-v5?format=jpg&name=small as image_4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_4.jpg: 100%|██████████| 54.4k/54.4k [00:00<00:00, 26.0MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/973822217732665344/image_4.jpg\n",
      "Created directory: /Users/princess/Documents/RA/Web Scraping/X/data/1418055882579931141\n",
      "Downloading image: https://pbs.twimg.com/media/E63xNvzVUAMgSjP?format=jpg&name=900x900 as image_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_1.jpg: 100%|██████████| 133k/133k [00:00<00:00, 7.16MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1418055882579931141/image_1.jpg\n",
      "Downloading image: https://pbs.twimg.com/media/E63xOOdVUAAm64q?format=jpg&name=900x900 as image_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_2.jpg: 100%|██████████| 118k/118k [00:00<00:00, 27.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /Users/princess/Documents/RA/Web Scraping/X/data/1418055882579931141/image_2.jpg\n",
      "Process Completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from twitter_scraper_selenium import scrape_profile\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from threading import Thread\n",
    "\n",
    "def download_file(url, save_path, chunk_size=8192, log_callback=None):\n",
    "    \"\"\"\n",
    "    Downloads a file from the specified URL and saves it to the given path.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the file to download.\n",
    "        save_path (str): The local path where the file will be saved.\n",
    "        chunk_size (int, optional): Size of each chunk to read from the response. Defaults to 8192.\n",
    "        log_callback (function, optional): Function to call for logging messages. Defaults to None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            with open(save_path, 'wb') as f, tqdm(\n",
    "                desc=os.path.basename(save_path),\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk:  # filter out keep-alive new chunks\n",
    "                        size = f.write(chunk)\n",
    "                        bar.update(size)\n",
    "        if log_callback:\n",
    "            log_callback(f\"Successfully downloaded: {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        if log_callback:\n",
    "            log_callback(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"\n",
    "    Removes or replaces characters that are invalid in file names.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The original filename.\n",
    "\n",
    "    Returns:\n",
    "        str: The sanitized filename.\n",
    "    \"\"\"\n",
    "    keepcharacters = (' ', '.', '_', '-')\n",
    "    return \"\".join(c for c in filename if c.isalnum() or c in keepcharacters).rstrip()\n",
    "\n",
    "def get_file_extension(url, default='jpg'):\n",
    "    \"\"\"\n",
    "    Determines the file extension based on the URL or defaults to 'jpg'.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the file.\n",
    "        default (str, optional): The default file extension if none is found. Defaults to 'jpg'.\n",
    "\n",
    "    Returns:\n",
    "        str: The file extension.\n",
    "    \"\"\"\n",
    "    path = urlparse(url).path\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        return ext\n",
    "    elif ext in ['.mp4', '.mov', '.avi', '.wmv']:\n",
    "        return ext\n",
    "    else:\n",
    "        return f\".{default}\"\n",
    "\n",
    "def scrape_tweets(twitter_username, output_format, browser, tweets_count, filename, directory, log_callback=None):\n",
    "    \"\"\"\n",
    "    Scrapes tweets from a specified Twitter profile using selenium.\n",
    "\n",
    "    Args:\n",
    "        twitter_username (str): Twitter handle of the profile to scrape.\n",
    "        output_format (str): Format to save the scraped data (e.g., 'csv').\n",
    "        browser (str): Browser to use for scraping (e.g., 'firefox').\n",
    "        tweets_count (int): Number of tweets to scrape.\n",
    "        filename (str): Name of the output file without extension.\n",
    "        directory (str): Directory where the output file will be saved.\n",
    "        log_callback (function, optional): Function to call for logging messages. Defaults to None.\n",
    "    \"\"\"\n",
    "    if log_callback:\n",
    "        log_callback(f\"Starting to scrape tweets from @{twitter_username}...\")\n",
    "    scrape_profile(\n",
    "        twitter_username=twitter_username,\n",
    "        output_format=output_format,\n",
    "        browser=browser,\n",
    "        tweets_count=tweets_count,\n",
    "        filename=filename,\n",
    "        directory=directory\n",
    "    )\n",
    "    if log_callback:\n",
    "        log_callback(f\"Scraping completed. Data saved to {os.path.join(directory, filename + '.' + output_format)}\")\n",
    "\n",
    "def download_media(csv_file, output_base_dir, log_callback=None):\n",
    "    \"\"\"\n",
    "    Processes the CSV file to download images and videos from tweets.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing tweet data.\n",
    "        output_base_dir (str): Base directory where media will be downloaded.\n",
    "        log_callback (function, optional): Function to call for logging messages. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Create the base output directory if it doesn't exist\n",
    "    Path(output_base_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if log_callback:\n",
    "        log_callback(f\"Reading CSV file: {csv_file}\")\n",
    "\n",
    "    # Read the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, dtype=str)  # Read all data as strings to avoid issues\n",
    "    except Exception as e:\n",
    "        if log_callback:\n",
    "            log_callback(f\"Error reading CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        tweet_id = str(row.get('tweet_id', '')).strip()\n",
    "        if not tweet_id:\n",
    "            if log_callback:\n",
    "                log_callback(f\"Row {index} missing 'tweet_id'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        images = row.get('images', '[]')\n",
    "        videos = row.get('videos', '[]')\n",
    "\n",
    "        # Parse the string representation of lists\n",
    "        try:\n",
    "            image_urls = ast.literal_eval(images) if pd.notna(images) else []\n",
    "            if not isinstance(image_urls, list):\n",
    "                raise ValueError(\"Images field is not a list\")\n",
    "        except Exception as e:\n",
    "            if log_callback:\n",
    "                log_callback(f\"Error parsing images for tweet_id {tweet_id}: {e}\")\n",
    "            image_urls = []\n",
    "\n",
    "        try:\n",
    "            video_urls = ast.literal_eval(videos) if pd.notna(videos) else []\n",
    "            if not isinstance(video_urls, list):\n",
    "                raise ValueError(\"Videos field is not a list\")\n",
    "        except Exception as e:\n",
    "            if log_callback:\n",
    "                log_callback(f\"Error parsing videos for tweet_id {tweet_id}: {e}\")\n",
    "            video_urls = []\n",
    "\n",
    "        # Skip if there are no images and no videos\n",
    "        if not image_urls and not video_urls:\n",
    "            if log_callback:\n",
    "                log_callback(f\"No media found for tweet_id {tweet_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Create a directory for the tweet_id\n",
    "        tweet_dir = os.path.join(output_base_dir, tweet_id)\n",
    "        os.makedirs(tweet_dir, exist_ok=True)\n",
    "        if log_callback:\n",
    "            log_callback(f\"Created directory: {tweet_dir}\")\n",
    "\n",
    "        # Initialize counters for sequential naming\n",
    "        image_counter = 1\n",
    "        video_counter = 1\n",
    "\n",
    "        # Download images\n",
    "        for img_url in image_urls:\n",
    "            if not img_url or not img_url.strip():\n",
    "                continue  # Skip empty URLs\n",
    "            try:\n",
    "                # Assign sequential naming\n",
    "                ext = get_file_extension(img_url, default='jpg')\n",
    "                img_name = f\"image_{image_counter}{ext}\"\n",
    "                img_save_path = os.path.join(tweet_dir, img_name)\n",
    "                if os.path.exists(img_save_path):\n",
    "                    if log_callback:\n",
    "                        log_callback(f\"File already exists: {img_save_path}. Skipping download.\")\n",
    "                    image_counter += 1\n",
    "                    continue\n",
    "                if log_callback:\n",
    "                    log_callback(f\"Downloading image: {img_url} as {img_name}\")\n",
    "                download_file(img_url, img_save_path, log_callback=log_callback)\n",
    "                image_counter += 1\n",
    "            except Exception as e:\n",
    "                if log_callback:\n",
    "                    log_callback(f\"Error downloading image {img_url} for tweet_id {tweet_id}: {e}\")\n",
    "\n",
    "        # Download videos\n",
    "        for vid_url in video_urls:\n",
    "            if not vid_url or not vid_url.strip():\n",
    "                continue  # Skip empty URLs\n",
    "            try:\n",
    "                # Assign sequential naming\n",
    "                ext = get_file_extension(vid_url, default='mp4')\n",
    "                vid_name = f\"video_{video_counter}{ext}\"\n",
    "                vid_save_path = os.path.join(tweet_dir, vid_name)\n",
    "                if os.path.exists(vid_save_path):\n",
    "                    if log_callback:\n",
    "                        log_callback(f\"File already exists: {vid_save_path}. Skipping download.\")\n",
    "                    video_counter += 1\n",
    "                    continue\n",
    "                if log_callback:\n",
    "                    log_callback(f\"Downloading video: {vid_url} as {vid_name}\")\n",
    "                download_file(vid_url, vid_save_path, log_callback=log_callback)\n",
    "                video_counter += 1\n",
    "            except Exception as e:\n",
    "                if log_callback:\n",
    "                    log_callback(f\"Error downloading video {vid_url} for tweet_id {tweet_id}: {e}\")\n",
    "\n",
    "def start_process(twitter_username, output_format, browser, tweets_count, filename, directory, log_callback):\n",
    "    \"\"\"\n",
    "    Starts the scraping and downloading processes.\n",
    "\n",
    "    Args:\n",
    "        twitter_username (str): Twitter handle to scrape.\n",
    "        output_format (str): Output format for scraped data.\n",
    "        browser (str): Browser to use for scraping.\n",
    "        tweets_count (int): Number of tweets to scrape.\n",
    "        filename (str): Filename for the scraped data.\n",
    "        directory (str): Directory to save scraped data and downloads.\n",
    "        log_callback (function): Function to call for logging messages.\n",
    "    \"\"\"\n",
    "    # Paths for CSV and media downloads\n",
    "    csv_file = os.path.join(directory, f\"{filename}.{output_format}\")\n",
    "    output_base_dir = os.path.join(directory, \"data\")\n",
    "\n",
    "    # Step 1: Scrape tweets\n",
    "    scrape_tweets(\n",
    "        twitter_username=twitter_username,\n",
    "        output_format=output_format,\n",
    "        browser=browser,\n",
    "        tweets_count=tweets_count,\n",
    "        filename=filename,\n",
    "        directory=directory,\n",
    "        log_callback=log_callback\n",
    "    )\n",
    "\n",
    "    # Step 2: Download media from scraped tweets\n",
    "    download_media(csv_file, output_base_dir, log_callback=log_callback)\n",
    "\n",
    "def create_gui():\n",
    "    \"\"\"\n",
    "    Creates the GUI for user input using ipywidgets.\n",
    "    \"\"\"\n",
    "    # Define Widgets\n",
    "    twitter_username = widgets.Text(\n",
    "        value='nyushanghai',\n",
    "        placeholder='Enter Twitter username',\n",
    "        description='Twitter Username:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    output_format = widgets.Dropdown(\n",
    "        options=['csv', 'json'],\n",
    "        value='csv',\n",
    "        description='Output Format:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    browser = widgets.Dropdown(\n",
    "        options=['firefox', 'chrome'],\n",
    "        value='firefox',\n",
    "        description='Browser:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    tweets_count = widgets.IntText(\n",
    "        value=10,\n",
    "        description='Number of Tweets:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    filename = widgets.Text(\n",
    "        value='nyushanghai',\n",
    "        placeholder='Enter filename',\n",
    "        description='Filename:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    directory = widgets.Text(\n",
    "        value='/Users/princess/Documents/RA/X-scraper',  # Adjust as needed\n",
    "        placeholder='Enter save directory',\n",
    "        description='Save Directory:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    browse_button = widgets.Button(\n",
    "        description='Browse',\n",
    "        button_style='',\n",
    "        tooltip='Browse for directory',\n",
    "        icon='folder',\n",
    "        layout=widgets.Layout(width='10%')\n",
    "    )\n",
    "\n",
    "    # Log Output Widget\n",
    "    log_output = widgets.Output()\n",
    "\n",
    "    # Define Browse Function\n",
    "    def on_browse_clicked(b):\n",
    "        from IPython.display import display\n",
    "        import tkinter as tk\n",
    "        from tkinter import filedialog\n",
    "\n",
    "        # Hide the main tkinter window\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        selected_dir = filedialog.askdirectory()\n",
    "        if selected_dir:\n",
    "            directory.value = selected_dir\n",
    "\n",
    "    browse_button.on_click(on_browse_clicked)\n",
    "\n",
    "    # Define Log Callback\n",
    "    def log(message):\n",
    "        with log_output:\n",
    "            print(message)\n",
    "\n",
    "    # Define Start Button\n",
    "    start_button = widgets.Button(\n",
    "        description='Start',\n",
    "        button_style='success',\n",
    "        tooltip='Start scraping and downloading',\n",
    "        icon='play',\n",
    "        layout=widgets.Layout(width='20%')\n",
    "    )\n",
    "\n",
    "    # Define Start Button Callback\n",
    "    def on_start_clicked(b):\n",
    "        # Disable the start button to prevent multiple clicks\n",
    "        start_button.disabled = True\n",
    "\n",
    "        # Clear previous logs\n",
    "        with log_output:\n",
    "            clear_output()\n",
    "\n",
    "        # Retrieve widget values\n",
    "        twitter_username_val = twitter_username.value.strip()\n",
    "        output_format_val = output_format.value\n",
    "        browser_val = browser.value\n",
    "        tweets_count_val = tweets_count.value\n",
    "        filename_val = filename.value.strip()\n",
    "        directory_val = directory.value.strip()\n",
    "\n",
    "        # Input Validation\n",
    "        error_messages = []\n",
    "        if not twitter_username_val:\n",
    "            error_messages.append(\"Twitter Username cannot be empty.\")\n",
    "        if output_format_val not in ['csv', 'json']:\n",
    "            error_messages.append(\"Output Format must be 'csv' or 'json'.\")\n",
    "        if browser_val not in ['firefox', 'chrome']:\n",
    "            error_messages.append(\"Browser must be 'firefox' or 'chrome'.\")\n",
    "        if not isinstance(tweets_count_val, int) or tweets_count_val <= 0:\n",
    "            error_messages.append(\"Number of Tweets must be a positive integer.\")\n",
    "        if not filename_val:\n",
    "            error_messages.append(\"Filename cannot be empty.\")\n",
    "        if not os.path.isdir(directory_val):\n",
    "            error_messages.append(\"Save Directory is invalid or does not exist.\")\n",
    "\n",
    "        if error_messages:\n",
    "            with log_output:\n",
    "                for msg in error_messages:\n",
    "                    print(f\"Error: {msg}\")\n",
    "            start_button.disabled = False\n",
    "            return\n",
    "\n",
    "        # Start the scraping and downloading in a separate thread\n",
    "        thread = Thread(target=start_process, args=(\n",
    "            twitter_username_val,\n",
    "            output_format_val,\n",
    "            browser_val,\n",
    "            tweets_count_val,\n",
    "            filename_val,\n",
    "            directory_val,\n",
    "            log\n",
    "        ))\n",
    "        thread.start()\n",
    "\n",
    "        # Monitor the thread\n",
    "        def check_thread():\n",
    "            if thread.is_alive():\n",
    "                # Check again after 1 second\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "                check_thread()\n",
    "            else:\n",
    "                with log_output:\n",
    "                    print(\"Process Completed.\")\n",
    "                start_button.disabled = False\n",
    "\n",
    "        check_thread_thread = Thread(target=check_thread)\n",
    "        check_thread_thread.start()\n",
    "\n",
    "    start_button.on_click(on_start_clicked)\n",
    "\n",
    "    # Arrange Widgets in Layout\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([twitter_username]),\n",
    "        widgets.HBox([output_format]),\n",
    "        widgets.HBox([browser]),\n",
    "        widgets.HBox([tweets_count]),\n",
    "        widgets.HBox([filename]),\n",
    "        widgets.HBox([directory, browse_button]),\n",
    "        widgets.HBox([start_button]),\n",
    "        widgets.Label(\"Log:\"),\n",
    "        log_output\n",
    "    ])\n",
    "\n",
    "    display(ui)\n",
    "\n",
    "# Execute GUI Creation\n",
    "create_gui()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
