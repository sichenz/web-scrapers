{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "TjZl-4I7fsj5",
   "metadata": {
    "id": "TjZl-4I7fsj5"
   },
   "source": [
    "### Grab Tweets from User\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "yv_Mmk_7Ship",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5317,
     "status": "ok",
     "timestamp": 1739503642107,
     "user": {
      "displayName": "Sichen Zhong",
      "userId": "05976141553128755948"
     },
     "user_tz": 300
    },
    "id": "yv_Mmk_7Ship",
    "outputId": "dcd43fa2-e9ea-4860-d89c-6aee90750d4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|██████████| 9/9 [00:04<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ntscraper import Nitter\n",
    "\n",
    "scraper = Nitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "KmEANMqzSBBa",
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1739503642108,
     "user": {
      "displayName": "Sichen Zhong",
      "userId": "05976141553128755948"
     },
     "user_tz": 300
    },
    "id": "KmEANMqzSBBa"
   },
   "outputs": [],
   "source": [
    "# Function to scrape tweets from a specific user's timeline\n",
    "def get_user_tweets(username, tweet_count, from_date, to_date):\n",
    "    try:\n",
    "        tweets = scraper.get_tweets(username, number=tweet_count, mode='user', since=from_date, until=to_date)\n",
    "        return tweets\n",
    "    except Exception as e:\n",
    "        print(\"Can't fetch tweets:\", e)\n",
    "        return None\n",
    "\n",
    "# Function to process the tweet dictionary and save it to a CSV file\n",
    "def save_tweets_to_csv(tweets, filename='data.csv'):\n",
    "    if tweets is None:\n",
    "        print(\"No tweets to save.\")\n",
    "        return\n",
    "\n",
    "    dummy_arr = []\n",
    "    for tweet in tweets['tweets']:\n",
    "        data = [\n",
    "            tweet.get('link'),\n",
    "            tweet.get('text'),\n",
    "            tweet.get('user', {}).get('name'),\n",
    "            tweet.get('user', {}).get('username'),\n",
    "            tweet.get('date'),\n",
    "            tweet.get('stats', {}).get('likes'),\n",
    "            tweet.get('stats', {}).get('retweets'),\n",
    "            tweet.get('stats', {}).get('comments')\n",
    "        ]\n",
    "        dummy_arr.append(data)\n",
    "\n",
    "    df = pd.DataFrame(dummy_arr, columns=['link', 'text', 'to_name', 'to_username', 'date', 'likes', 'retweets', 'comments'])\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(\"Completed... 100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bQMgpYxKSmqr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19665,
     "status": "ok",
     "timestamp": 1739503661743,
     "user": {
      "displayName": "Sichen Zhong",
      "userId": "05976141553128755948"
     },
     "user_tz": 300
    },
    "id": "bQMgpYxKSmqr",
    "outputId": "ece3acd6-e24f-494d-c293-0e7d2e969793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-Feb-25 22:34:36 - No instance specified, using random instance https://nitter.privacydev.net\n",
      "13-Feb-25 22:34:41 - Current stats for nyushanghai: 10 tweets, 0 threads...\n",
      "Completed... 100%\n"
     ]
    }
   ],
   "source": [
    "# Take user inputs\n",
    "username = input(\"Username: \")\n",
    "tweet_count = int(input(\"No. of tweets: \"))\n",
    "from_date = input(\"From date (YYYY-MM-DD): \")\n",
    "to_date = input(\"To date (YYYY-MM-DD): \")\n",
    "\n",
    "tweets = get_user_tweets(username, tweet_count, from_date, to_date)\n",
    "save_tweets_to_csv(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WZbJAOQBf2oG",
   "metadata": {
    "id": "WZbJAOQBf2oG"
   },
   "source": [
    "### Download Media Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6qahCK8SYvRU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1739503662561,
     "user": {
      "displayName": "Sichen Zhong",
      "userId": "05976141553128755948"
     },
     "user_tz": 300
    },
    "id": "6qahCK8SYvRU",
    "outputId": "ffb60de0-87dd-4718-c145-77b38d0c63f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v23.2.0\n",
      "10.9.0\n",
      "\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!node -v\n",
    "!npm -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "typYP64NYxe-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5738,
     "status": "ok",
     "timestamp": 1739503668297,
     "user": {
      "displayName": "Sichen Zhong",
      "userId": "05976141553128755948"
     },
     "user_tz": 300
    },
    "id": "typYP64NYxe-",
    "outputId": "ac203bd7-dddd-46f0-9531-f500e94852e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(node:87044) ExperimentalWarning: CommonJS module /opt/homebrew/lib/node_modules/npm/node_modules/debug/src/node.js is loading ES Module /opt/homebrew/lib/node_modules/npm/node_modules/supports-color/index.js using require().\n",
      "Support for loading ES Module in require() is an experimental feature and might change at any time\n",
      "(Use `node --trace-warnings ...` to show where the warning was created)\n",
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
      "added 31 packages in 2s\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K16 packages are looking for funding\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.9.0\u001b[39m -> \u001b[34m11.1.0\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.1.0\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.1.0\u001b[24m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
      "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!npm install happy-dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7SXZc1fWZK6Q",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739503668297,
     "user": {
      "displayName": "Sichen Zhong",
      "userId": "05976141553128755948"
     },
     "user_tz": 300
    },
    "id": "7SXZc1fWZK6Q"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > test.js <<'EOF'\n",
    "const happyDL = require(\"happy-dl\");\n",
    "\n",
    "async function fetchTwitterData(url) {\n",
    "  try {\n",
    "    const result = await happyDL.twitterDownloader(url);\n",
    "    // Ensure we output valid JSON even if result is undefined.\n",
    "    if (result === undefined) {\n",
    "      console.log(JSON.stringify({ error: \"No media found or result undefined\" }));\n",
    "    } else {\n",
    "      console.log(JSON.stringify(result));\n",
    "    }\n",
    "  } catch (error) {\n",
    "    // Print the error to stderr and output a JSON object with the error message.\n",
    "    console.error(\"Error fetching Twitter media details:\", error);\n",
    "    console.log(JSON.stringify({ error: error.message }));\n",
    "  }\n",
    "}\n",
    "\n",
    "const twitterUrl = process.argv[2];\n",
    "if (!twitterUrl) {\n",
    "  console.error(\"No twitter URL provided\");\n",
    "  console.log(JSON.stringify({ error: \"No twitter URL provided\" }));\n",
    "  process.exit(1);\n",
    "}\n",
    "\n",
    "fetchTwitterData(twitterUrl);\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "SfeC06-idpPE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13782,
     "status": "ok",
     "timestamp": 1739503858017,
     "user": {
      "displayName": "Sichen Zhong",
      "userId": "05976141553128755948"
     },
     "user_tz": 300
    },
    "id": "SfeC06-idpPE",
    "outputId": "64059ae1-51a3-494b-c88c-d23d0e20a468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://twitter.com/nyushanghai/status/1874031782359949539#m\n",
      "Result for https://twitter.com/nyushanghai/status/1874031782359949539#m: types: ['video'], urls: ['https://video.twimg.com/ext_tw_video/1874030891418464256/pu/vid/avc1/720x1280/9sjw-W3Sb6Xrxchu.mp4?tag=12']\n",
      "Processing: https://twitter.com/nyushanghai/status/1869898005895131398#m\n",
      "Unexpected data type for https://twitter.com/nyushanghai/status/1869898005895131398#m\n",
      "Processing: https://twitter.com/nyushanghai/status/1868852614563266670#m\n",
      "Result for https://twitter.com/nyushanghai/status/1868852614563266670#m: types: ['image', 'image', 'image'], urls: ['https://pbs.twimg.com/media/Ge9_F6xbwAAmLcy.jpg', 'https://pbs.twimg.com/media/Ge9_HlfbwAAJaM7.jpg', 'https://pbs.twimg.com/media/Ge9_H3SasAA_LFF.jpg']\n",
      "Processing: https://twitter.com/nyushanghai/status/1868808330791035310#m\n",
      "Result for https://twitter.com/nyushanghai/status/1868808330791035310#m: types: ['image', 'image', 'image', 'image'], urls: ['https://pbs.twimg.com/media/Ge5qYjqaYAADgJo.jpg', 'https://pbs.twimg.com/media/Ge5qeNEbAAANvPc.jpg', 'https://pbs.twimg.com/media/Ge5qfy6bIAAnm_N.jpg', 'https://pbs.twimg.com/media/Ge5qjUEawAAd-Ar.jpg']\n",
      "Processing: https://twitter.com/nyushanghai/status/1868532517822963761#m\n",
      "Result for https://twitter.com/nyushanghai/status/1868532517822963761#m: types: ['image'], urls: ['https://pbs.twimg.com/media/Ge5bnSiaAAEN8eY.jpg']\n",
      "Processing: https://twitter.com/nyushanghai/status/1867414990430560761#m\n",
      "Result for https://twitter.com/nyushanghai/status/1867414990430560761#m: types: ['image'], urls: ['https://pbs.twimg.com/media/Gepj4-Ma8AAVHJw.jpg']\n",
      "Processing: https://twitter.com/nyushanghai/status/1867075186459283681#m\n",
      "Result for https://twitter.com/nyushanghai/status/1867075186459283681#m: types: ['image', 'image', 'image', 'image'], urls: ['https://pbs.twimg.com/media/Geku8GaaEAEyZ4H.jpg', 'https://pbs.twimg.com/media/Geku9OzbAAAhgO-.jpg', 'https://pbs.twimg.com/media/Geku-NDaEAMyj8I.jpg', 'https://pbs.twimg.com/media/Geku-6OaEAI9Baz.jpg']\n",
      "Processing: https://twitter.com/nyushanghai/status/1866748119884685789#m\n",
      "Result for https://twitter.com/nyushanghai/status/1866748119884685789#m: types: ['image'], urls: ['https://pbs.twimg.com/media/GegEFI6aAAAw2Hh.jpg']\n",
      "Processing: https://twitter.com/nyushanghai/status/1866287213845291171#m\n",
      "Result for https://twitter.com/nyushanghai/status/1866287213845291171#m: types: ['image', 'image'], urls: ['https://pbs.twimg.com/media/GeVxJjRawAAQtTp.jpg', 'https://pbs.twimg.com/media/GeVxKamaEAAZKXT.jpg']\n",
      "Processing: https://twitter.com/nyushanghai/status/1866018863739187346#m\n",
      "Result for https://twitter.com/nyushanghai/status/1866018863739187346#m: types: ['image'], urls: ['https://pbs.twimg.com/media/GeVuPSHbIAEHRY2.jpg']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Load your original CSV file (must contain a column named \"link\")\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "if 'link' not in df.columns:\n",
    "    raise ValueError(\"The CSV file must contain a 'link' column.\")\n",
    "\n",
    "# Prepare lists to hold our new column values\n",
    "media_types_all = []\n",
    "media_files_all = []\n",
    "\n",
    "# Process each Twitter URL in the CSV\n",
    "for idx, row in df.iterrows():\n",
    "    twitter_link = row['link']\n",
    "    print(f\"Processing: {twitter_link}\")\n",
    "    try:\n",
    "        # Run the Node.js script for the current Twitter link\n",
    "        completed = subprocess.run(\n",
    "            ['node', 'test.js', twitter_link],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        if completed.stderr:\n",
    "            print(\"Stderr:\", completed.stderr)\n",
    "\n",
    "        output = completed.stdout.strip()\n",
    "        if output:\n",
    "            try:\n",
    "                data = json.loads(output)\n",
    "                # If an error occurred in test.js, skip this row\n",
    "                if \"error\" in data:\n",
    "                    print(f\"Error in result for {twitter_link}: {data['error']}\")\n",
    "                    media_types_all.append(json.dumps([]))\n",
    "                    media_files_all.append(json.dumps([]))\n",
    "                    continue\n",
    "\n",
    "                # If data is not a list, check if it is a dict with a \"results\" key that is a list.\n",
    "                if not isinstance(data, list):\n",
    "                    if isinstance(data, dict) and \"results\" in data and isinstance(data[\"results\"], list):\n",
    "                        data = data[\"results\"]\n",
    "                    else:\n",
    "                        print(f\"Unexpected data type for {twitter_link}\")\n",
    "                        media_types_all.append(json.dumps([]))\n",
    "                        media_files_all.append(json.dumps([]))\n",
    "                        continue\n",
    "\n",
    "                current_media_types = []\n",
    "                current_media_files = []\n",
    "                for media in data:\n",
    "                    mtype = media.get(\"type\")\n",
    "                    variants = media.get(\"variants\", [])\n",
    "                    if not variants:\n",
    "                        continue\n",
    "                    if mtype == \"photo\":\n",
    "                        # For photos, we use the first variant URL\n",
    "                        url = variants[0].get(\"url\")\n",
    "                        if url:\n",
    "                            current_media_types.append(\"image\")\n",
    "                            current_media_files.append(url)\n",
    "                    elif mtype == \"video\":\n",
    "                        # For videos, choose the variant with quality \"1280p\" if available; otherwise, use the last variant\n",
    "                        chosen_variant = None\n",
    "                        for variant in variants:\n",
    "                            if variant.get(\"quality\") == \"1280p\":\n",
    "                                chosen_variant = variant\n",
    "                                break\n",
    "                        if not chosen_variant:\n",
    "                            chosen_variant = variants[-1]\n",
    "                        url = chosen_variant.get(\"url\")\n",
    "                        if url:\n",
    "                            current_media_types.append(\"video\")\n",
    "                            current_media_files.append(url)\n",
    "                    else:\n",
    "                        print(f\"Unrecognized media type {mtype} for {twitter_link}. Defaulting to 'text'.\")\n",
    "                        # Default to \"text\" regardless; use the first variant's URL if available, else an empty string.\n",
    "                        url = variants[0].get(\"url\") if (variants and variants[0].get(\"url\")) else \"\"\n",
    "                        current_media_types.append(\"text\")\n",
    "                        current_media_files.append(url)\n",
    "\n",
    "                print(f\"Result for {twitter_link}: types: {current_media_types}, urls: {current_media_files}\")\n",
    "                # Save the results as JSON strings so we can easily parse them later\n",
    "                media_types_all.append(json.dumps(current_media_types))\n",
    "                media_files_all.append(json.dumps(current_media_files))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON decode error:\", e, \"Output was:\", output)\n",
    "                media_types_all.append(json.dumps([]))\n",
    "                media_files_all.append(json.dumps([]))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Subprocess error:\", e)\n",
    "        media_types_all.append(json.dumps([]))\n",
    "        media_files_all.append(json.dumps([]))\n",
    "\n",
    "# Add the new columns to the DataFrame\n",
    "df[\"media type\"] = media_types_all\n",
    "df[\"media file\"] = media_files_all\n",
    "\n",
    "# Save the updated DataFrame back to data.csv (or choose a different filename)\n",
    "df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dkm6luT5eDgw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13627,
     "status": "ok",
     "timestamp": 1739503695868,
     "user": {
      "displayName": "Sichen Zhong",
      "userId": "05976141553128755948"
     },
     "user_tz": 300
    },
    "id": "dkm6luT5eDgw",
    "outputId": "c88d002b-8dd4-4168-c3aa-eae61539842d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded video: videos/video_0_0.mp4\n",
      "Downloaded image: images/image_2_0.jpg\n",
      "Downloaded image: images/image_2_1.jpg\n",
      "Downloaded image: images/image_2_2.jpg\n",
      "Downloaded image: images/image_3_0.jpg\n",
      "Downloaded image: images/image_3_1.jpg\n",
      "Downloaded image: images/image_3_2.jpg\n",
      "Downloaded image: images/image_3_3.jpg\n",
      "Downloaded image: images/image_4_0.jpg\n",
      "Downloaded image: images/image_5_0.jpg\n",
      "Downloaded image: images/image_6_0.jpg\n",
      "Downloaded image: images/image_6_1.jpg\n",
      "Downloaded image: images/image_6_2.jpg\n",
      "Downloaded image: images/image_6_3.jpg\n",
      "Downloaded image: images/image_7_0.jpg\n",
      "Downloaded image: images/image_8_0.jpg\n",
      "Downloaded image: images/image_8_1.jpg\n",
      "Downloaded image: images/image_9_0.jpg\n",
      "Downloaded 17 images and 1 videos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Create directories for downloads if they don't exist\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"videos\", exist_ok=True)\n",
    "\n",
    "# Read the updated data.csv\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "downloaded_images = 0\n",
    "downloaded_videos = 0\n",
    "\n",
    "# Process each row in the CSV\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        media_types = json.loads(row[\"media type\"])\n",
    "        media_files = json.loads(row[\"media file\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON in row {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(media_types, list) or not isinstance(media_files, list):\n",
    "        print(f\"Row {idx} media columns are not lists.\")\n",
    "        continue\n",
    "\n",
    "    # For each media file in this row\n",
    "    for j, url in enumerate(media_files):\n",
    "        # Ensure we have a corresponding media type\n",
    "        if j >= len(media_types):\n",
    "            continue\n",
    "        mtype = media_types[j]\n",
    "        if not url:\n",
    "            continue\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            parsed = urlparse(url)\n",
    "            ext = os.path.splitext(parsed.path)[1]\n",
    "            if mtype == \"image\":\n",
    "                if not ext:\n",
    "                    ext = \".jpg\"\n",
    "                filename = f\"images/image_{idx}_{j}{ext}\"\n",
    "            elif mtype == \"video\":\n",
    "                if not ext:\n",
    "                    ext = \".mp4\"\n",
    "                filename = f\"videos/video_{idx}_{j}{ext}\"\n",
    "            else:\n",
    "                print(f\"Row {idx}, item {j}: Unrecognized media type: {mtype}\")\n",
    "                continue\n",
    "            with open(filename, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded {mtype}: {filename}\")\n",
    "            if mtype == \"image\":\n",
    "                downloaded_images += 1\n",
    "            elif mtype == \"video\":\n",
    "                downloaded_videos += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading from {url}: {e}\")\n",
    "\n",
    "print(f\"Downloaded {downloaded_images} images and {downloaded_videos} videos.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
