{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在运行本笔记本之前，请先下载以下工具：\n",
    "\n",
    "1. **查找当前的 Google Chrome 版本**  \n",
    "   - 打开 Google Chrome，并在地址栏输入：  \n",
    "     ```\n",
    "     chrome://settings/help\n",
    "     ```\n",
    "   - 建议使用 **133** 版本的 Google Chrome（或最新可用的稳定版本）。\n",
    "\n",
    "2. **下载对应版本的 ChromeDriver**  \n",
    "   - 前往官方的 Chrome 测试下载页面：  \n",
    "     [https://googlechromelabs.github.io/chrome-for-testing/#stable](https://googlechromelabs.github.io/chrome-for-testing/#stable)  \n",
    "   - 下载与您的 Chrome 版本相匹配的 ChromeDriver。  \n",
    "   - 例如，如果您的 Chrome 版本是 133，请下载 **ChromeDriver 133**。\n",
    "\n",
    "3. **查找已下载的 ChromeDriver**  \n",
    "   - 如果您使用 macOS，可在终端 (Terminal) 中运行以下命令来查找 `chromedriver` 的位置：\n",
    "     ```bash\n",
    "     mdfind -name chromedriver\n",
    "     ```\n",
    "   - 如果使用其他操作系统，请检查默认的 **下载** 目录或您保存该文件的目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import requests  # 新增：用于下载图片\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 启动 Chrome 浏览器实例：\n",
    "\n",
    "打开 **terminal**, 下载 Chrome Driver (假的 Google Chrome)\n",
    "\n",
    "```bash\n",
    "brew install chromedriver\n",
    "chmod +x /opt/homebrew/bin/chromedriver\n",
    "```\n",
    "\n",
    "输入以下命令（将 `your Chrome.exe path` 替换为您的 Google Chrome 浏览器路径）：\n",
    "```bash\n",
    "<your Chrome.exe path> --remote-debugging-port=9222 --user-data-dir=\"/Users/<your home folder name>/selenium/AutomationProfile\"\n",
    "```\n",
    "\n",
    "- 请将your Chrome.exe path替换为您的Chrome浏览器所在路径，例如<br>`C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe`\n",
    "- 配置 chromedriver 相关信息，请参考官方文档：[ChromeDriver](https://developer.chrome.com/docs/chromedriver)\n",
    "- 来做个比方， 我的 *terminal command* 会是:\n",
    "\n",
    "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
    "  --remote-debugging-port=9222 \\\n",
    "  --user-data-dir=\"/Users/princess/selenium/AutomationProfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置Chrome浏览器\n",
    "options = Options()\n",
    "options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')\n",
    "options.add_argument('--incognito')\n",
    "browser = webdriver.Chrome(options=options)\n",
    "action = ActionChains(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将开始检查小红书登录状态...\n",
      "爬取数据有账户封禁的风险，建议使用非主账号登录。\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb  5 01:07:38 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb  5 01:07:48 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb  5 01:07:58 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb  5 01:08:08 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb  5 01:08:18 2025\n",
      "登录成功\n",
      "检查时间: Wed Feb  5 01:08:28 2025\n",
      "请在文本框中根据提示输入搜索关键词和笔记爬取数量。\n",
      "即将开始检查网页加载状态...\n",
      "如果网页进入人机验证页面，请先手动完成验证。\n",
      "加载成功\n",
      "检查时间: Wed Feb  5 01:08:39 2025\n"
     ]
    }
   ],
   "source": [
    "key_word = \"\"\n",
    "num = 0\n",
    "\n",
    "def check_login_status(browser):\n",
    "    print(\"即将开始检查小红书登录状态...\")\n",
    "    print(\"爬取数据有账户封禁的风险，建议使用非主账号登录。\")\n",
    "    \n",
    "    while True:\n",
    "        page_source = browser.page_source\n",
    "        if '登录探索更多内容' in page_source:\n",
    "            print('暂未登录，请手动登录')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print('登录成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "def check_page_load_status(browser, keyword):\n",
    "    print(\"即将开始检查网页加载状态...\")\n",
    "    print(\"如果网页进入人机验证页面，请先手动完成验证。\")\n",
    "    \n",
    "    while True:\n",
    "        if keyword in browser.title:\n",
    "            print('加载成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "\n",
    "def selenium_test():\n",
    "    \"\"\"\n",
    "    登录状态检查，网页加载检查，根据用户输入进行搜索\n",
    "    \"\"\"\n",
    "    global key_word, num\n",
    "    browser.get('https://www.xiaohongshu.com/explore')\n",
    "    \n",
    "    check_login_status(browser)\n",
    "    \n",
    "    print(\"请在文本框中根据提示输入搜索关键词和笔记爬取数量。\")\n",
    "    keyword = input(\"搜索关键词：\")\n",
    "    try:\n",
    "        num = int(input(\"笔记爬取数量：\"))\n",
    "    except ValueError:\n",
    "        print(\"请输入有效的整数作为爬取数量。\")\n",
    "        return\n",
    "    \n",
    "    url = f'https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_explore_feed'\n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    check_page_load_status(browser, keyword)\n",
    "\n",
    "selenium_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已自动更改模式为图文。\n",
      "请选择排序方式:\n",
      "1. 综合\n",
      "2. 最新\n",
      "3. 最热\n",
      "请输入有效的排序方式...\n"
     ]
    }
   ],
   "source": [
    "def change_mode(browser):\n",
    "    # 更改模式为图文\n",
    "    try:\n",
    "        mode_button = browser.find_element(By.XPATH, '//*[@id=\"search-type\"]/div/div/div[2]')\n",
    "        mode_button.click()\n",
    "        print('已自动更改模式为图文。')\n",
    "    except Exception as e:\n",
    "        print(f\"更改模式失败: {e}\")\n",
    "\n",
    "selected_order_text = ''\n",
    "def change_sort_order(browser, action):\n",
    "    # 更改排序方式\n",
    "    sort_order = {\n",
    "        '综合': 1,\n",
    "        '最新': 2,\n",
    "        '最热': 3\n",
    "    }\n",
    "    print(\"请选择排序方式:\")\n",
    "    for idx, order in sort_order.items():\n",
    "        print(f'{order}. {idx}')\n",
    "    \n",
    "    try:\n",
    "        global selected_order_text\n",
    "        selected_order_text = input(\"请输入排序方式对应的名称: \").strip()\n",
    "        if selected_order_text not in sort_order:\n",
    "            print(\"请输入有效的排序方式...\")\n",
    "            return\n",
    "        \n",
    "        selected_order_index = sort_order[selected_order_text]\n",
    "    except Exception as e:\n",
    "        print(f\"处理排序选择时出错: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        element = browser.find_element(By.XPATH, '//*[@id=\"global\"]/div[2]/div[2]/div/div[1]/div[2]')\n",
    "        action.move_to_element(element).perform()# 模拟鼠标悬停\n",
    "        menu = browser.find_element(By.CLASS_NAME, 'dropdown-items')\n",
    "        option = menu.find_element(By.XPATH, f'/html/body/div[4]/div/li[{selected_order_index}]')\n",
    "        option.click()# 模拟鼠标点击\n",
    "\n",
    "        print('已选择排序方式为:',selected_order_text)\n",
    "        print('检查时间:',time.ctime())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"更改排序方式失败: {e}\")\n",
    "\n",
    "change_mode(browser)\n",
    "change_sort_order(browser, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1721d4e78eae4287901dd0ae4299b653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "爬取进度:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前URL: https://www.xiaohongshu.com/search_result?keyword=nyu&source=web_explore_feed&type=51\n",
      "已导航到搜索结果页面。\n",
      "找到元素，选择器: //*[contains(text(), '图文')]\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 16\n",
      "总共收集的条目数: 16\n",
      "收集的数据样本:\n",
      "URL: 67681b79000000000b0169c4\n",
      "URL: 676cac02000000000b017d51\n",
      "URL: 67a27d1f000000002602c12d\n",
      "URL: 67a2e5d9000000002901aa32\n",
      "URL: 67a29575000000002803d8b0\n",
      "截断后的总条目数: 10\n",
      "收集的数据样本:\n",
      "作者: AiDa, 点赞: 15, URL: 67681b79000000000b0169c4\n",
      "作者: momo要早睡, 点赞: 11, URL: 676cac02000000000b017d51\n",
      "作者: this_is_kang_kang, 点赞: 94, URL: 67a27d1f000000002602c12d\n",
      "作者: 瑞伊, 点赞: 赞, URL: 67a2e5d9000000002901aa32\n",
      "作者: 1只懒茶茶, 点赞: 6, URL: 67a29575000000002803d8b0\n",
      "开始提取附加字段，包括帖子内容、日期发布和评论数量...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7a04dbcdf946d1a36c486197fe85a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "已获取的笔记数量...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 67681b79000000000b0169c4\n",
      "已提取附加字段，笔记ID: 676cac02000000000b017d51\n",
      "已提取附加字段，笔记ID: 67a27d1f000000002602c12d\n",
      "已提取附加字段，笔记ID: 67a2e5d9000000002901aa32\n",
      "已提取附加字段，笔记ID: 67a29575000000002803d8b0\n",
      "已提取附加字段，笔记ID: 6646f897000000000c01973c\n",
      "已提取附加字段，笔记ID: 675f9bb4000000000700bf04\n",
      "已提取附加字段，笔记ID: 673c5abe0000000007034973\n",
      "已提取附加字段，笔记ID: 67a2354d000000002602c25f\n",
      "已提取附加字段，笔记ID: 66f6d09b000000002c029af5\n",
      "开始下载主图片和头像图片...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3316ebca6cc7456aa6dc15a12507c2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "下载主图片:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e5f9c6c4e74483b2d89b3bc01ec21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "下载头像图片:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有图片下载完成。\n",
      "                                Author Name Likes  Comments  \\\n",
      "URL                                                           \n",
      "67681b79000000000b0169c4               AiDa    15  共 79 条评论   \n",
      "676cac02000000000b017d51            momo要早睡    11  共 41 条评论   \n",
      "67a27d1f000000002602c12d  this_is_kang_kang    94   共 7 条评论   \n",
      "67a2e5d9000000002901aa32                 瑞伊     赞         0   \n",
      "67a29575000000002803d8b0              1只懒茶茶     6   共 2 条评论   \n",
      "\n",
      "                                 Post Title  \\\n",
      "URL                                           \n",
      "67681b79000000000b0169c4  NYU新生交流group！还能进！   \n",
      "676cac02000000000b017d51  NYU新生交流group！还能进！   \n",
      "67a27d1f000000002602c12d   那个让我入学NYU然后退学的男孩   \n",
      "67a2e5d9000000002901aa32  nyu meal plan 8🔪出   \n",
      "67a29575000000002803d8b0           nyu找课友🥲🥲   \n",
      "\n",
      "                                                                    Caption  \\\n",
      "URL                                                                           \n",
      "67681b79000000000b0169c4  这边组了个group，里面非常活跃，NYU的uu都可以来~ . 目前位置快满了，想进的速来 ...   \n",
      "676cac02000000000b017d51  这边组了个group，里面非常活跃，NYU的uu都可以来~ 欢迎进来认识新朋友，不管是学术问...   \n",
      "67a27d1f000000002602c12d  初识是小学时期的外教课 后来同一所初中 走廊上打过照面 再后来高中SAT补习给reconne...   \n",
      "67a2e5d9000000002901aa32  8🔪一个swipe 买多有优惠 grubhub线上代点 点完自取 可以zelle可以zfb ...   \n",
      "67a29575000000002803d8b0  intro to psych calculus 1 Trending mental heal...   \n",
      "\n",
      "                           Date Published  \\\n",
      "URL                                         \n",
      "67681b79000000000b0169c4       2024-12-22   \n",
      "676cac02000000000b017d51       2024-12-25   \n",
      "67a27d1f000000002602c12d  编辑于 昨天 16:23 美国   \n",
      "67a2e5d9000000002901aa32      昨天 23:15 美国   \n",
      "67a29575000000002803d8b0      昨天 17:32 美国   \n",
      "\n",
      "                                                       Images  \\\n",
      "URL                                                             \n",
      "67681b79000000000b0169c4  images/67681b79000000000b0169c4.jpg   \n",
      "676cac02000000000b017d51  images/676cac02000000000b017d51.jpg   \n",
      "67a27d1f000000002602c12d  images/67a27d1f000000002602c12d.jpg   \n",
      "67a2e5d9000000002901aa32  images/67a2e5d9000000002901aa32.jpg   \n",
      "67a29575000000002803d8b0  images/67a29575000000002803d8b0.jpg   \n",
      "\n",
      "                                                 Author Avatar Stars  \\\n",
      "URL                                                                    \n",
      "67681b79000000000b0169c4  avatars/67681b79000000000b0169c4.jpg     3   \n",
      "676cac02000000000b017d51  avatars/676cac02000000000b017d51.jpg     赞   \n",
      "67a27d1f000000002602c12d  avatars/67a27d1f000000002602c12d.jpg     赞   \n",
      "67a2e5d9000000002901aa32  avatars/67a2e5d9000000002901aa32.jpg    点赞   \n",
      "67a29575000000002803d8b0  avatars/67a29575000000002803d8b0.jpg     赞   \n",
      "\n",
      "                         Author Collect Nr Author Fans Nr Author Note Nr  \\\n",
      "URL                                                                        \n",
      "67681b79000000000b0169c4                 0              0              0   \n",
      "676cac02000000000b017d51                 0              0              0   \n",
      "67a27d1f000000002602c12d                 0              0              0   \n",
      "67a2e5d9000000002901aa32                 0              0              0   \n",
      "67a29575000000002803d8b0                 0              0              0   \n",
      "\n",
      "                         Video URL  \\\n",
      "URL                                  \n",
      "67681b79000000000b0169c4       N/A   \n",
      "676cac02000000000b017d51       N/A   \n",
      "67a27d1f000000002602c12d       N/A   \n",
      "67a2e5d9000000002901aa32       N/A   \n",
      "67a29575000000002803d8b0       N/A   \n",
      "\n",
      "                                                                   User URL  \n",
      "URL                                                                          \n",
      "67681b79000000000b0169c4  /search_result/67681b79000000000b0169c4?xsec_t...  \n",
      "676cac02000000000b017d51  /search_result/676cac02000000000b017d51?xsec_t...  \n",
      "67a27d1f000000002602c12d  /search_result/67a27d1f000000002602c12d?xsec_t...  \n",
      "67a2e5d9000000002901aa32  /search_result/67a2e5d9000000002901aa32?xsec_t...  \n",
      "67a29575000000002803d8b0  /search_result/67a29575000000002803d8b0?xsec_t...  \n",
      "数据已保存到 'scraped_xhs_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据存储列表\n",
    "authorName_list = []\n",
    "likeNr_list = []\n",
    "URL_list = []\n",
    "userURL_list = []\n",
    "commentNr_list = []\n",
    "post_title_list = [] \n",
    "caption_list = []  # 已初始化，用于存储帖子内容\n",
    "datePublished_list = []\n",
    "images_list = []\n",
    "author_avatar_list = []  # 单独初始化用于存储头像图片\n",
    "starNr_list = []\n",
    "authorCollectNr_list = []\n",
    "authorFansNr_list = []\n",
    "authorNoteNr_list = []\n",
    "video_urls = [] \n",
    "\n",
    "def parsePage(page_source):\n",
    "    \"\"\"\n",
    "    解析当前页面的HTML内容，提取笔记的基本信息并更新对应的列表。\n",
    "    \n",
    "    Args:\n",
    "        page_source (str): 当前页面的HTML内容\n",
    "    \"\"\"\n",
    "    response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "    selector = Selector(response)\n",
    "\n",
    "    print(\"正在分析页面结构...\")\n",
    "\n",
    "    containers = [\n",
    "        '//div[contains(@class, \"note-item\")]'\n",
    "    ]\n",
    "\n",
    "    for container in containers:\n",
    "        elements = selector.xpath(container)\n",
    "        if elements:\n",
    "            print(f\"找到容器，xpath: {container}\")\n",
    "\n",
    "    content_elements = selector.xpath('//section[contains(@class, \"note-item\")]')\n",
    "    if content_elements:\n",
    "        print(f\"找到 {len(content_elements)} 个内容元素\")\n",
    "\n",
    "        for element in content_elements:\n",
    "            try:\n",
    "                # 提取用户URL\n",
    "                user_url = element.xpath('.//a[contains(@class, \"cover\")]/@href').get()\n",
    "                if user_url:\n",
    "                    note_id = user_url.split('/')[-1].split('?')[0]\n",
    "                    if note_id in URL_list:\n",
    "                        continue  # 避免重复\n",
    "                    URL_list.append(note_id)\n",
    "                    userURL_list.append(user_url)\n",
    "\n",
    "                    # 提取作者名字\n",
    "                    author = element.xpath('.//div[contains(@class, \"author-wrapper\")]//span[contains(@class, \"name\")]/text()').get()\n",
    "                    authorName_list.append(author.strip() if author else \"N/A\")\n",
    "\n",
    "                    # 提取点赞数量\n",
    "                    likes = element.xpath('.//span[contains(@class, \"like-wrapper\")]/span[contains(@class, \"count\")]/text()').get()\n",
    "                    likeNr_list.append(likes.strip() if likes else \"0\")\n",
    "\n",
    "                    # 提取帖子标题\n",
    "                    post_title = element.xpath('.//a[contains(@class, \"title\")]//span/text()').getall()\n",
    "                    post_title_cleaned = ' '.join([c.strip() for c in post_title if c.strip()])\n",
    "                    post_title_list.append(post_title_cleaned if post_title_cleaned else \"N/A\")\n",
    "\n",
    "                    # 提取图片（主图）\n",
    "                    main_image = element.xpath('.//a[contains(@class, \"cover\")]/img/@src').get()\n",
    "                    images_list.append(main_image.strip() if main_image else \"N/A\")\n",
    "\n",
    "                    # 提取头像图片\n",
    "                    avatar_image = element.xpath('.//a[contains(@class, \"author\")]/img/@src').get()\n",
    "                    author_avatar_list.append(avatar_image.strip() if avatar_image else \"N/A\")\n",
    "\n",
    "                    # 初始化附加字段的默认值\n",
    "                    commentNr_list.append(\"0\")\n",
    "                    datePublished_list.append(\"N/A\")\n",
    "                    starNr_list.append(\"0\")\n",
    "                    authorCollectNr_list.append(\"0\")\n",
    "                    authorFansNr_list.append(\"0\")\n",
    "                    authorNoteNr_list.append(\"0\")\n",
    "                    video_urls.append(\"N/A\")  \n",
    "                    caption_list.append(\"N/A\")  # 初始化为默认值\n",
    "\n",
    "                    qbar.update(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理元素时出错: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"当前已爬取总数: {len(URL_list)}\")\n",
    "\n",
    "def extract_additional_fields(note_url, note_id):\n",
    "    \"\"\"\n",
    "    访问每个笔记的页面，提取附加字段，包括评论数量、发布时间、收藏数量、粉丝数量、笔记数量、视频URL以及帖子内容（Caption）。\n",
    "    \n",
    "    Args:\n",
    "        note_url (str): 笔记的相对URL\n",
    "        note_id (str): 笔记的唯一ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_note_url = f'https://www.xiaohongshu.com{note_url}'\n",
    "        browser.get(full_note_url)\n",
    "        \n",
    "        # 使用显式等待代替固定的等待时间，等待描述meta标签加载完成\n",
    "        wait = WebDriverWait(browser, 15)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@name=\"description\"]')))\n",
    "        \n",
    "        # 获取页面源代码\n",
    "        page_source = browser.page_source\n",
    "        response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "        selector = Selector(response)\n",
    "\n",
    "        # 提取评论数量\n",
    "        comments = selector.xpath('//*[@class=\"total\"]/text()').get()\n",
    "        comments = comments.strip() if comments else \"0\"\n",
    "\n",
    "        # 提取发布时间，使用多个XPath策略\n",
    "        date_published = selector.xpath('//*[@class=\"date\"]/text()').get()\n",
    "        if not date_published:\n",
    "            # 替代XPath，如果第一个失败\n",
    "            date_published = selector.xpath('//time/@datetime').get()\n",
    "        date_published = date_published.strip() if date_published else \"N/A\"\n",
    "\n",
    "        # 提取收藏数量\n",
    "        stars = selector.xpath('//*[@class=\"count\"]/text()').get()\n",
    "        stars = stars.strip() if stars else \"0\"\n",
    "\n",
    "        # 提取作者收藏数量\n",
    "        collect_nr = selector.xpath('//span[contains(@class, \"collect\") or contains(@class, \"saved\")]/text()').get()\n",
    "        collect_nr = collect_nr.strip() if collect_nr else \"0\"\n",
    "\n",
    "        # 提取作者粉丝数量\n",
    "        fans_nr = selector.xpath('//span[contains(@class, \"fans\") or contains(@class, \"followers\")]/text()').get()\n",
    "        fans_nr = fans_nr.strip() if fans_nr else \"0\"\n",
    "\n",
    "        # 提取作者笔记数量\n",
    "        note_nr = selector.xpath('//span[contains(@class, \"notes\") or contains(@class, \"posts\")]/text()').get()\n",
    "        note_nr = note_nr.strip() if note_nr else \"0\"\n",
    "\n",
    "        # 提取视频URL（如果有）\n",
    "        video_url = selector.xpath('//video/@src').get()\n",
    "        video_url = video_url.strip() if video_url else \"N/A\"\n",
    "\n",
    "        # 提取帖子内容（Caption）\n",
    "        caption = selector.xpath('//*[@name=\"description\"]/@content').get()\n",
    "        caption = caption.strip() if caption else \"N/A\"\n",
    "\n",
    "        # 更新全局列表\n",
    "        if note_id in URL_list:\n",
    "            index = URL_list.index(note_id)\n",
    "            commentNr_list[index] = comments\n",
    "            datePublished_list[index] = date_published\n",
    "            starNr_list[index] = stars\n",
    "            authorCollectNr_list[index] = collect_nr\n",
    "            authorFansNr_list[index] = fans_nr\n",
    "            authorNoteNr_list[index] = note_nr\n",
    "            video_urls[index] = video_url\n",
    "            caption_list[index] = caption  # 存储提取的帖子内容\n",
    "            print(f\"已提取附加字段，笔记ID: {note_id}\")\n",
    "        else:\n",
    "            print(f\"笔记ID {note_id} 未在URL_list中找到。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"提取附加字段时出错，笔记ID: {note_id}, 错误: {str(e)}\")\n",
    "\n",
    "def ensure_search_results():\n",
    "    \"\"\"\n",
    "    确保已导航到搜索结果页面，并选择“图文”模式。\n",
    "    \"\"\"\n",
    "    current_url = browser.current_url\n",
    "    print(f\"当前URL: {current_url}\")\n",
    "\n",
    "    search_url = f'https://www.xiaohongshu.com/search_result?keyword={key_word}&source=web_explore_feed'\n",
    "    browser.get(search_url)\n",
    "\n",
    "    try:\n",
    "        # 等待页面标题包含关键词\n",
    "        wait = WebDriverWait(browser, 15)\n",
    "        wait.until(EC.title_contains(key_word))\n",
    "        print(\"已导航到搜索结果页面。\")\n",
    "    except:\n",
    "        print(\"导航到搜索结果页面时超时。\")\n",
    "        browser.quit()\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        selectors = [\n",
    "            \"//div[text()='图文']\",\n",
    "            \"//div[contains(@class, 'tab')]//span[text()='图文']\",\n",
    "            \"//div[contains(@class, 'filter')]//div[text()='图文']\",\n",
    "            \"//*[contains(text(), '图文')]\"\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                element = WebDriverWait(browser, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, selector))\n",
    "                )\n",
    "                print(f\"找到元素，选择器: {selector}\")\n",
    "                element.click()\n",
    "                time.sleep(2)  # 等待模式切换\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            print(\"未找到“图文”标签，可能页面结构已更改。\")\n",
    "    except Exception as e:\n",
    "        print(f\"切换视图时出错: {e}\")\n",
    "\n",
    "    # 等待任何内容加载完成\n",
    "    time.sleep(3)\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"\n",
    "    下载图片并保存到指定路径。\n",
    "    \n",
    "    Args:\n",
    "        url (str): 图片的URL地址。\n",
    "        save_path (str): 图片保存的本地路径。\n",
    "    \n",
    "    Returns:\n",
    "        bool: 下载是否成功。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"下载图片时出错，URL: {url}, 错误: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# 创建目录用于存储图片\n",
    "def create_directories():\n",
    "    \"\"\"\n",
    "    创建用于存储主图片和头像图片的目录。\n",
    "    \"\"\"\n",
    "    if not os.path.exists('images'):\n",
    "        os.makedirs('images')\n",
    "    if not os.path.exists('avatars'):\n",
    "        os.makedirs('avatars')\n",
    "\n",
    "# 调用目录创建函数\n",
    "create_directories()\n",
    "\n",
    "# 定义进度条:实时跟踪已爬取的笔记数量\n",
    "qbar = tqdm(total=num, desc=\"爬取进度\")\n",
    "\n",
    "ensure_search_results()\n",
    "\n",
    "while len(URL_list) < num:\n",
    "    for _ in range(3):\n",
    "        browser.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    parsePage(browser.page_source)\n",
    "\n",
    "    if '- THE END -' in browser.page_source or 'No more content' in browser.page_source:\n",
    "        print(f\"已到达内容末尾。总共收集: {len(URL_list)} 条\")\n",
    "        break\n",
    "\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "print(f\"总共收集的条目数: {len(URL_list)}\")\n",
    "if URL_list:\n",
    "    print(\"收集的数据样本:\")\n",
    "    for i in range(min(5, len(URL_list))):\n",
    "        print(f\"URL: {URL_list[i]}\")\n",
    "\n",
    "if len(URL_list) > num:\n",
    "    URL_list = URL_list[:num]\n",
    "    authorName_list = authorName_list[:num]\n",
    "    likeNr_list = likeNr_list[:num]\n",
    "    userURL_list = userURL_list[:num]\n",
    "    commentNr_list = commentNr_list[:num]\n",
    "    post_title_list = post_title_list[:num]\n",
    "    datePublished_list = datePublished_list[:num]\n",
    "    images_list = images_list[:num]\n",
    "    author_avatar_list = author_avatar_list[:num]  # 截断头像列表\n",
    "    starNr_list = starNr_list[:num]\n",
    "    authorCollectNr_list = authorCollectNr_list[:num]\n",
    "    authorFansNr_list = authorFansNr_list[:num]\n",
    "    authorNoteNr_list = authorNoteNr_list[:num]\n",
    "    video_urls = video_urls[:num]\n",
    "    caption_list = caption_list[:num]  # 截断帖子内容列表\n",
    "\n",
    "print(f\"截断后的总条目数: {len(URL_list)}\")\n",
    "print(\"收集的数据样本:\")\n",
    "for i in range(min(5, len(URL_list))):\n",
    "    print(f\"作者: {authorName_list[i]}, 点赞: {likeNr_list[i]}, URL: {URL_list[i]}\")\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "# 提取附加字段，包括帖子内容、日期发布和评论数量\n",
    "print(\"开始提取附加字段，包括帖子内容、日期发布和评论数量...\")\n",
    "qbar = tqdm(total=len(URL_list), desc=\"已获取的笔记数量...\")\n",
    "\n",
    "for note_id, note_url in zip(URL_list, userURL_list):\n",
    "    extract_additional_fields(note_url, note_id)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(2, 4))  # 礼貌等待，避免服务器压力过大\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "# 下载图片\n",
    "print(\"开始下载主图片和头像图片...\")\n",
    "\n",
    "# 下载主图片\n",
    "image_download_bar = tqdm(total=len(images_list), desc=\"下载主图片\")\n",
    "for idx, image_url in enumerate(images_list):\n",
    "    if image_url == \"N/A\":\n",
    "        image_download_bar.update(1)\n",
    "        continue\n",
    "    # 构造图片保存路径，使用note_id作为文件名\n",
    "    image_extension = os.path.splitext(image_url)[1].split('?')[0]  # 获取文件扩展名\n",
    "    if image_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        image_extension = '.jpg'  # 默认使用.jpg\n",
    "    image_filename = f\"images/{URL_list[idx]}{image_extension}\"\n",
    "    success = download_image(image_url, image_filename)\n",
    "    if not success:\n",
    "        image_filename = \"N/A\"  # 如果下载失败，标记为N/A\n",
    "    images_list[idx] = image_filename  # 更新为本地路径\n",
    "    image_download_bar.update(1)\n",
    "image_download_bar.close()\n",
    "\n",
    "# 下载头像图片\n",
    "avatar_download_bar = tqdm(total=len(author_avatar_list), desc=\"下载头像图片\")\n",
    "for idx, avatar_url in enumerate(author_avatar_list):\n",
    "    if avatar_url == \"N/A\":\n",
    "        avatar_download_bar.update(1)\n",
    "        continue\n",
    "    # 构造头像保存路径，使用note_id作为文件名\n",
    "    avatar_extension = os.path.splitext(avatar_url)[1].split('?')[0]  # 获取文件扩展名\n",
    "    if avatar_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        avatar_extension = '.jpg'  # 默认使用.jpg\n",
    "    avatar_filename = f\"avatars/{URL_list[idx]}{avatar_extension}\"\n",
    "    success = download_image(avatar_url, avatar_filename)\n",
    "    if not success:\n",
    "        avatar_filename = \"N/A\"  # 如果下载失败，标记为N/A\n",
    "    author_avatar_list[idx] = avatar_filename  # 更新为本地路径\n",
    "    avatar_download_bar.update(1)\n",
    "avatar_download_bar.close()\n",
    "\n",
    "print(\"所有图片下载完成。\")\n",
    "\n",
    "# 创建数据字典，包括“Author Avatar”和“Caption”\n",
    "data = {\n",
    "    'Author Name': authorName_list,\n",
    "    'Likes': likeNr_list,\n",
    "    'Comments': commentNr_list,\n",
    "    'Post Title': post_title_list, \n",
    "    'Caption': caption_list,  # 包含帖子内容\n",
    "    'Date Published': datePublished_list,\n",
    "    'Images': images_list,  # 仅主图的本地路径\n",
    "    'Author Avatar': author_avatar_list,  # 单独的头像列表的本地路径\n",
    "    'Stars': starNr_list,\n",
    "    'Author Collect Nr': authorCollectNr_list,\n",
    "    'Author Fans Nr': authorFansNr_list,\n",
    "    'Author Note Nr': authorNoteNr_list,\n",
    "    'Video URL': video_urls,\n",
    "    'URL': URL_list,\n",
    "    'User URL': userURL_list\n",
    "}\n",
    "\n",
    "# 创建DataFrame并保存为CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('URL', inplace=True)\n",
    "print(df.head())\n",
    "df.to_csv('scraped_xhs_data.csv', encoding='utf-8-sig')\n",
    "print(\"数据已保存到 'scraped_xhs_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
