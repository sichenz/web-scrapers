{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在运行本笔记本之前，请先下载以下工具：\n",
    "\n",
    "1. **查找当前的 Google Chrome 版本**  \n",
    "   - 打开 Google Chrome，并在地址栏输入：  \n",
    "     ```\n",
    "     chrome://settings/help\n",
    "     ```\n",
    "   - 建议使用 **133** 版本的 Google Chrome（或最新可用的稳定版本）。\n",
    "\n",
    "2. **下载对应版本的 ChromeDriver**  \n",
    "   - 前往官方的 Chrome 测试下载页面：  \n",
    "     [https://googlechromelabs.github.io/chrome-for-testing/#stable](https://googlechromelabs.github.io/chrome-for-testing/#stable)  \n",
    "   - 下载与您的 Chrome 版本相匹配的 ChromeDriver。  \n",
    "   - 例如，如果您的 Chrome 版本是 133，请下载 **ChromeDriver 133**。\n",
    "\n",
    "3. **查找已下载的 ChromeDriver**  \n",
    "   - 如果您使用 macOS，可在终端 (Terminal) 中运行以下命令来查找 `chromedriver` 的位置：\n",
    "     ```bash\n",
    "     mdfind -name chromedriver\n",
    "     ```\n",
    "   - 如果使用其他操作系统，请检查默认的 **下载** 目录或您保存该文件的目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import TextResponse\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 启动 Chrome 浏览器实例：\n",
    "\n",
    "打开 **terminal**, 下载 Chrome Driver (假的 Google Chrome)\n",
    "\n",
    "```bash\n",
    "brew install chromedriver\n",
    "chmod +x /opt/homebrew/bin/chromedriver\n",
    "```\n",
    "\n",
    "输入以下命令（将 `your Chrome.exe path` 替换为您的 Google Chrome 浏览器路径）：\n",
    "```bash\n",
    "<your Chrome.exe path> --remote-debugging-port=9222 --user-data-dir=\"/Users/<your home folder name>/selenium/AutomationProfile\"\n",
    "```\n",
    "\n",
    "- 请将your Chrome.exe path替换为您的Chrome浏览器所在路径，例如<br>`C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe`\n",
    "- 配置 chromedriver 相关信息，请参考官方文档：[ChromeDriver](https://developer.chrome.com/docs/chromedriver)\n",
    "- 来做个比方， 我的 *terminal command* 会是:\n",
    "\n",
    "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
    "  --remote-debugging-port=9222 \\\n",
    "  --user-data-dir=\"/Users/princess/selenium/AutomationProfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置Chrome浏览器\n",
    "options = Options()\n",
    "options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')\n",
    "options.add_argument('--incognito')\n",
    "browser = webdriver.Chrome(options=options)\n",
    "action = ActionChains(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将开始检查小红书登录状态...\n",
      "爬取数据有账户封禁的风险，建议使用非主账号登录。\n",
      "登录成功\n",
      "检查时间: Thu Feb  6 18:43:21 2025\n",
      "请根据提示输入用户主页URL和笔记爬取数量。\n",
      "检查用户主页加载状态...\n",
      "用户主页加载成功\n"
     ]
    }
   ],
   "source": [
    "user_url = \"\"\n",
    "num = 0\n",
    "\n",
    "def check_login_status(browser):\n",
    "    print(\"即将开始检查小红书登录状态...\")\n",
    "    print(\"爬取数据有账户封禁的风险，建议使用非主账号登录。\")\n",
    "    \n",
    "    while True:\n",
    "        page_source = browser.page_source\n",
    "        if '登录探索更多内容' in page_source:\n",
    "            print('暂未登录，请手动登录')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print('登录成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "def check_user_page_load_status(browser):\n",
    "    \"\"\"\n",
    "    检查用户主页是否加载成功。\n",
    "    这里等待页面中出现至少一个帖子（包含 \"note-item\" 的 section）。\n",
    "    \"\"\"\n",
    "    print(\"检查用户主页加载状态...\")\n",
    "    try:\n",
    "        wait = WebDriverWait(browser, 15)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//section[contains(@class, \"note-item\")]')))\n",
    "        print(\"用户主页加载成功\")\n",
    "    except Exception as e:\n",
    "        print(\"用户主页加载超时或出错:\", e)\n",
    "\n",
    "def selenium_test():\n",
    "    \"\"\"\n",
    "    登录状态检查、页面加载检查，并根据用户输入跳转到指定的用户主页进行爬取\n",
    "    \"\"\"\n",
    "    global user_url, num\n",
    "    # 首先打开探索页以触发登录检查\n",
    "    browser.get('https://www.xiaohongshu.com/explore')\n",
    "    check_login_status(browser)\n",
    "    \n",
    "    print(\"请根据提示输入用户主页URL和笔记爬取数量。\")\n",
    "    user_url = input(\"用户主页URL：\").strip()\n",
    "    try:\n",
    "        num = int(input(\"笔记爬取数量：\"))\n",
    "    except ValueError:\n",
    "        print(\"请输入有效的整数作为爬取数量。\")\n",
    "        return\n",
    "    \n",
    "    # 跳转到指定用户主页\n",
    "    browser.get(user_url)\n",
    "    time.sleep(3)\n",
    "    check_user_page_load_status(browser)\n",
    "\n",
    "# 调用 selenium_test() 进行初始化（确保此处 browser 已创建）\n",
    "selenium_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "爬取进度:  20%|██        | 1/5 [00:03<00:12,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "爬取进度: 21it [00:06,  3.29it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共收集的条目数: 21\n",
      "收集的数据样本:\n",
      "URL: 676f71ba000000000b0227ac\n",
      "URL: 63158aee0000000011036e6e\n",
      "URL: 62724ac20000000001027847\n",
      "URL: 62639a4100000000010294ec\n",
      "URL: 625a9529000000002103e554\n",
      "截断后的总条目数: 5\n",
      "收集的数据样本:\n",
      "作者: 黄子韬, 点赞: 1.5万, URL: 676f71ba000000000b0227ac\n",
      "作者: 黄子韬, 点赞: 5620, URL: 63158aee0000000011036e6e\n",
      "作者: 黄子韬, 点赞: 1万, URL: 62724ac20000000001027847\n",
      "作者: 黄子韬, 点赞: 1万, URL: 62639a4100000000010294ec\n",
      "作者: 黄子韬, 点赞: 2.4万, URL: 625a9529000000002103e554\n",
      "开始提取附加字段，包括帖子内容、日期发布和评论数量...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已获取的笔记数量...:  20%|██        | 1/5 [00:05<00:22,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 676f71ba000000000b0227ac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已获取的笔记数量...:  40%|████      | 2/5 [00:14<00:21,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 63158aee0000000011036e6e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已获取的笔记数量...:  60%|██████    | 3/5 [00:19<00:12,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 62724ac20000000001027847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已获取的笔记数量...:  80%|████████  | 4/5 [00:26<00:06,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 62639a4100000000010294ec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已获取的笔记数量...: 100%|██████████| 5/5 [00:35<00:00,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 625a9529000000002103e554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已获取的笔记数量...: 100%|██████████| 5/5 [00:39<00:00,  7.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始下载主图片和头像图片...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "下载主图片: 100%|██████████| 5/5 [00:01<00:00,  3.95it/s]\n",
      "下载头像图片: 100%|██████████| 5/5 [00:00<00:00, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有图片下载完成。\n",
      "                         Author Name Likes    Comments  \\\n",
      "URL                                                      \n",
      "676f71ba000000000b0227ac         黄子韬  1.5万  共 2094 条评论   \n",
      "63158aee0000000011036e6e         黄子韬  5620  共 1228 条评论   \n",
      "62724ac20000000001027847         黄子韬    1万           0   \n",
      "62639a4100000000010294ec         黄子韬    1万  共 1431 条评论   \n",
      "625a9529000000002103e554         黄子韬  2.4万  共 3317 条评论   \n",
      "\n",
      "                                                        Post Title  \\\n",
      "URL                                                                  \n",
      "676f71ba000000000b0227ac  谁熬夜，call me ，我用绝色宠你！很高兴成为@绝色JIASEE 品牌全球代   \n",
      "63158aee0000000011036e6e                                    新款即将降世   \n",
      "62724ac20000000001027847                                         🎥   \n",
      "62639a4100000000010294ec                                打工👨🏻‍🏭OOTD   \n",
      "625a9529000000002103e554                               好久不见 🍠的兄弟们！   \n",
      "\n",
      "                                                                    Caption  \\\n",
      "URL                                                                           \n",
      "676f71ba000000000b0227ac  谁熬夜，call me ，我用绝色宠你！很高兴成为@绝色JIASEE 品牌全球代言人#黄子韬...   \n",
      "63158aee0000000011036e6e                                              #YKYB   \n",
      "62724ac20000000001027847                                           #爱冒险夏日计划   \n",
      "62639a4100000000010294ec                                                N/A   \n",
      "625a9529000000002103e554                                                N/A   \n",
      "\n",
      "                          Date Published  \\\n",
      "URL                                        \n",
      "676f71ba000000000b0227ac      2024-12-27   \n",
      "63158aee0000000011036e6e      2022-09-05   \n",
      "62724ac20000000001027847      2022-05-04   \n",
      "62639a4100000000010294ec  编辑于 2022-04-23   \n",
      "625a9529000000002103e554      2022-04-16   \n",
      "\n",
      "                                                            Images  \\\n",
      "URL                                                                  \n",
      "676f71ba000000000b0227ac  images_user/676f71ba000000000b0227ac.jpg   \n",
      "63158aee0000000011036e6e  images_user/63158aee0000000011036e6e.jpg   \n",
      "62724ac20000000001027847  images_user/62724ac20000000001027847.jpg   \n",
      "62639a4100000000010294ec  images_user/62639a4100000000010294ec.jpg   \n",
      "625a9529000000002103e554  images_user/625a9529000000002103e554.jpg   \n",
      "\n",
      "                                                      Author Avatar Stars  \\\n",
      "URL                                                                         \n",
      "676f71ba000000000b0227ac  avatars_user/676f71ba000000000b0227ac.jpg  2376   \n",
      "63158aee0000000011036e6e  avatars_user/63158aee0000000011036e6e.jpg  3628   \n",
      "62724ac20000000001027847  avatars_user/62724ac20000000001027847.jpg    1万   \n",
      "62639a4100000000010294ec  avatars_user/62639a4100000000010294ec.jpg   761   \n",
      "625a9529000000002103e554  avatars_user/625a9529000000002103e554.jpg   937   \n",
      "\n",
      "                         Author Collect Nr Author Fans Nr Author Note Nr  \\\n",
      "URL                                                                        \n",
      "676f71ba000000000b0227ac                 0              0              0   \n",
      "63158aee0000000011036e6e                 0              0              0   \n",
      "62724ac20000000001027847                 0              0              0   \n",
      "62639a4100000000010294ec                 0              0              0   \n",
      "625a9529000000002103e554                 0              0              0   \n",
      "\n",
      "                                                                  Video URL  \\\n",
      "URL                                                                           \n",
      "676f71ba000000000b0227ac  blob:https://www.xiaohongshu.com/25053225-e9cd...   \n",
      "63158aee0000000011036e6e                                                N/A   \n",
      "62724ac20000000001027847                                                N/A   \n",
      "62639a4100000000010294ec                                                N/A   \n",
      "625a9529000000002103e554                                                N/A   \n",
      "\n",
      "                                                                   User URL  \n",
      "URL                                                                          \n",
      "676f71ba000000000b0227ac  /user/profile/5aa6061f4eacab09f53a8598/676f71b...  \n",
      "63158aee0000000011036e6e  /user/profile/5aa6061f4eacab09f53a8598/63158ae...  \n",
      "62724ac20000000001027847  /user/profile/5aa6061f4eacab09f53a8598/62724ac...  \n",
      "62639a4100000000010294ec  /user/profile/5aa6061f4eacab09f53a8598/62639a4...  \n",
      "625a9529000000002103e554  /user/profile/5aa6061f4eacab09f53a8598/625a952...  \n",
      "数据已保存到 'scraped_xhs_user.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据存储列表\n",
    "authorName_list = []\n",
    "likeNr_list = []\n",
    "URL_list = []\n",
    "userURL_list = []\n",
    "commentNr_list = []\n",
    "post_title_list = [] \n",
    "caption_list = []  # 存储帖子内容\n",
    "datePublished_list = []\n",
    "images_list = []\n",
    "author_avatar_list = []  # 用于存储头像图片\n",
    "starNr_list = []\n",
    "authorCollectNr_list = []\n",
    "authorFansNr_list = []\n",
    "authorNoteNr_list = []\n",
    "video_urls = [] \n",
    "\n",
    "def parsePage(page_source):\n",
    "    \"\"\"\n",
    "    解析当前页面的HTML内容，提取用户笔记的基本信息并更新对应的列表。\n",
    "    \n",
    "    Args:\n",
    "        page_source (str): 当前页面的HTML内容\n",
    "    \"\"\"\n",
    "    response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "    selector = Selector(response)\n",
    "\n",
    "    print(\"正在分析页面结构...\")\n",
    "\n",
    "    content_elements = selector.xpath('//section[contains(@class, \"note-item\")]')\n",
    "    if content_elements:\n",
    "        print(f\"找到 {len(content_elements)} 个内容元素\")\n",
    "\n",
    "        for element in content_elements:\n",
    "            try:\n",
    "                # 提取帖子（笔记）的相对URL\n",
    "                note_url = element.xpath('.//a[contains(@class, \"cover\")]/@href').get()\n",
    "                if note_url:\n",
    "                    note_id = note_url.split('/')[-1].split('?')[0]\n",
    "                    if note_id in URL_list:\n",
    "                        continue  # 避免重复爬取\n",
    "                    URL_list.append(note_id)\n",
    "                    userURL_list.append(note_url)\n",
    "\n",
    "                    # 提取作者名字（通常与用户主页相同）\n",
    "                    author = element.xpath('.//div[contains(@class, \"author-wrapper\")]//span[contains(@class, \"name\")]/text()').get()\n",
    "                    authorName_list.append(author.strip() if author else \"N/A\")\n",
    "\n",
    "                    # 提取点赞数量\n",
    "                    likes = element.xpath('.//span[contains(@class, \"like-wrapper\")]/span[contains(@class, \"count\")]/text()').get()\n",
    "                    likeNr_list.append(likes.strip() if likes else \"0\")\n",
    "\n",
    "                    # 提取帖子标题\n",
    "                    post_title = element.xpath('.//a[contains(@class, \"title\")]//span/text()').getall()\n",
    "                    post_title_cleaned = ' '.join([c.strip() for c in post_title if c.strip()])\n",
    "                    post_title_list.append(post_title_cleaned if post_title_cleaned else \"N/A\")\n",
    "\n",
    "                    # 提取图片（主图）\n",
    "                    main_image = element.xpath('.//a[contains(@class, \"cover\")]/img/@src').get()\n",
    "                    images_list.append(main_image.strip() if main_image else \"N/A\")\n",
    "\n",
    "                    # 提取头像图片\n",
    "                    avatar_image = element.xpath('.//a[contains(@class, \"author\")]/img/@src').get()\n",
    "                    author_avatar_list.append(avatar_image.strip() if avatar_image else \"N/A\")\n",
    "\n",
    "                    # 初始化附加字段的默认值\n",
    "                    commentNr_list.append(\"0\")\n",
    "                    datePublished_list.append(\"N/A\")\n",
    "                    starNr_list.append(\"0\")\n",
    "                    authorCollectNr_list.append(\"0\")\n",
    "                    authorFansNr_list.append(\"0\")\n",
    "                    authorNoteNr_list.append(\"0\")\n",
    "                    video_urls.append(\"N/A\")\n",
    "                    caption_list.append(\"N/A\")\n",
    "\n",
    "                    qbar.update(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理元素时出错: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"当前已爬取总数: {len(URL_list)}\")\n",
    "\n",
    "def extract_additional_fields(note_url, note_id):\n",
    "    \"\"\"\n",
    "    访问每个笔记的页面，提取附加字段，包括评论数量、发布时间、收藏数量、粉丝数量、笔记数量、视频URL以及帖子内容（Caption）。\n",
    "    \n",
    "    Args:\n",
    "        note_url (str): 笔记的相对URL\n",
    "        note_id (str): 笔记的唯一ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_note_url = f'https://www.xiaohongshu.com{note_url}'\n",
    "        browser.get(full_note_url)\n",
    "        \n",
    "        # 等待页面加载中关键的描述 meta 标签\n",
    "        wait = WebDriverWait(browser, 15)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@name=\"description\"]')))\n",
    "        \n",
    "        page_source = browser.page_source\n",
    "        response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "        selector = Selector(response)\n",
    "\n",
    "        # 提取评论数量\n",
    "        comments = selector.xpath('//*[@class=\"total\"]/text()').get()\n",
    "        comments = comments.strip() if comments else \"0\"\n",
    "\n",
    "        # 提取发布时间（采用多种 XPath 尝试）\n",
    "        date_published = selector.xpath('//*[@class=\"date\"]/text()').get()\n",
    "        if not date_published:\n",
    "            date_published = selector.xpath('//time/@datetime').get()\n",
    "        date_published = date_published.strip() if date_published else \"N/A\"\n",
    "\n",
    "        # 提取收藏数量\n",
    "        stars = selector.xpath('//*[@class=\"count\"]/text()').get()\n",
    "        stars = stars.strip() if stars else \"0\"\n",
    "\n",
    "        # 提取作者收藏数量\n",
    "        collect_nr = selector.xpath('//span[contains(@class, \"collect\") or contains(@class, \"saved\")]/text()').get()\n",
    "        collect_nr = collect_nr.strip() if collect_nr else \"0\"\n",
    "\n",
    "        # 提取作者粉丝数量\n",
    "        fans_nr = selector.xpath('//span[contains(@class, \"fans\") or contains(@class, \"followers\")]/text()').get()\n",
    "        fans_nr = fans_nr.strip() if fans_nr else \"0\"\n",
    "\n",
    "        # 提取作者笔记数量\n",
    "        note_nr = selector.xpath('//span[contains(@class, \"notes\") or contains(@class, \"posts\")]/text()').get()\n",
    "        note_nr = note_nr.strip() if note_nr else \"0\"\n",
    "\n",
    "        # 提取视频 URL（如果有）\n",
    "        video_url = selector.xpath('//video/@src').get()\n",
    "        video_url = video_url.strip() if video_url else \"N/A\"\n",
    "\n",
    "        # 提取帖子内容（Caption）\n",
    "        caption = selector.xpath('//*[@name=\"description\"]/@content').get()\n",
    "        caption = caption.strip() if caption else \"N/A\"\n",
    "\n",
    "        # 更新全局列表中的相应条目\n",
    "        if note_id in URL_list:\n",
    "            index = URL_list.index(note_id)\n",
    "            commentNr_list[index] = comments\n",
    "            datePublished_list[index] = date_published\n",
    "            starNr_list[index] = stars\n",
    "            authorCollectNr_list[index] = collect_nr\n",
    "            authorFansNr_list[index] = fans_nr\n",
    "            authorNoteNr_list[index] = note_nr\n",
    "            video_urls[index] = video_url\n",
    "            caption_list[index] = caption\n",
    "            print(f\"已提取附加字段，笔记ID: {note_id}\")\n",
    "        else:\n",
    "            print(f\"笔记ID {note_id} 未在 URL_list 中找到。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"提取附加字段时出错，笔记ID: {note_id}, 错误: {str(e)}\")\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"\n",
    "    下载图片并保存到指定路径。\n",
    "    \n",
    "    Args:\n",
    "        url (str): 图片的 URL 地址\n",
    "        save_path (str): 图片保存的本地路径\n",
    "    \n",
    "    Returns:\n",
    "        bool: 下载是否成功\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"下载图片时出错，URL: {url}, 错误: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"\n",
    "    创建用于存储主图片和头像图片的目录。\n",
    "    \"\"\"\n",
    "    if not os.path.exists('images_user'):\n",
    "        os.makedirs('images_user')\n",
    "    if not os.path.exists('avatars_user'):\n",
    "        os.makedirs('avatars_user')\n",
    "\n",
    "# 创建目录以存储图片\n",
    "create_directories()\n",
    "\n",
    "# 定义进度条用于跟踪已爬取的笔记数量\n",
    "qbar = tqdm(total=num, desc=\"爬取进度\")\n",
    "\n",
    "# 开始下拉加载页面直至达到所需笔记数量\n",
    "while len(URL_list) < num:\n",
    "    for _ in range(3):\n",
    "        browser.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    parsePage(browser.page_source)\n",
    "\n",
    "    # 检测是否到达页面末尾（根据页面提示文本判断）\n",
    "    if '- THE END -' in browser.page_source or 'No more content' in browser.page_source:\n",
    "        print(f\"已到达内容末尾。总共收集: {len(URL_list)} 条\")\n",
    "        break\n",
    "\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "print(f\"总共收集的条目数: {len(URL_list)}\")\n",
    "if URL_list:\n",
    "    print(\"收集的数据样本:\")\n",
    "    for i in range(min(5, len(URL_list))):\n",
    "        print(f\"URL: {URL_list[i]}\")\n",
    "\n",
    "# 如果收集的条目超过用户所需数量，则截断各个列表\n",
    "if len(URL_list) > num:\n",
    "    URL_list = URL_list[:num]\n",
    "    authorName_list = authorName_list[:num]\n",
    "    likeNr_list = likeNr_list[:num]\n",
    "    userURL_list = userURL_list[:num]\n",
    "    commentNr_list = commentNr_list[:num]\n",
    "    post_title_list = post_title_list[:num]\n",
    "    datePublished_list = datePublished_list[:num]\n",
    "    images_list = images_list[:num]\n",
    "    author_avatar_list = author_avatar_list[:num]\n",
    "    starNr_list = starNr_list[:num]\n",
    "    authorCollectNr_list = authorCollectNr_list[:num]\n",
    "    authorFansNr_list = authorFansNr_list[:num]\n",
    "    authorNoteNr_list = authorNoteNr_list[:num]\n",
    "    video_urls = video_urls[:num]\n",
    "    caption_list = caption_list[:num]\n",
    "\n",
    "print(f\"截断后的总条目数: {len(URL_list)}\")\n",
    "print(\"收集的数据样本:\")\n",
    "for i in range(min(5, len(URL_list))):\n",
    "    print(f\"作者: {authorName_list[i]}, 点赞: {likeNr_list[i]}, URL: {URL_list[i]}\")\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "# 提取每个笔记的附加字段（例如帖子内容、发布时间和评论数量）\n",
    "print(\"开始提取附加字段，包括帖子内容、日期发布和评论数量...\")\n",
    "qbar = tqdm(total=len(URL_list), desc=\"已获取的笔记数量...\")\n",
    "\n",
    "for note_id, note_url in zip(URL_list, userURL_list):\n",
    "    extract_additional_fields(note_url, note_id)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(2, 4))  # 礼貌等待，避免服务器过载\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "# 下载主图片和头像图片\n",
    "print(\"开始下载主图片和头像图片...\")\n",
    "\n",
    "# 下载主图片\n",
    "image_download_bar = tqdm(total=len(images_list), desc=\"下载主图片\")\n",
    "for idx, image_url in enumerate(images_list):\n",
    "    if image_url == \"N/A\":\n",
    "        image_download_bar.update(1)\n",
    "        continue\n",
    "    # 构造图片保存路径，使用 note_id 作为文件名\n",
    "    image_extension = os.path.splitext(image_url)[1].split('?')[0]\n",
    "    if image_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        image_extension = '.jpg'\n",
    "    image_filename = f\"images_user/{URL_list[idx]}{image_extension}\"\n",
    "    success = download_image(image_url, image_filename)\n",
    "    if not success:\n",
    "        image_filename = \"N/A\"\n",
    "    images_list[idx] = image_filename\n",
    "    image_download_bar.update(1)\n",
    "image_download_bar.close()\n",
    "\n",
    "# 下载头像图片\n",
    "avatar_download_bar = tqdm(total=len(author_avatar_list), desc=\"下载头像图片\")\n",
    "for idx, avatar_url in enumerate(author_avatar_list):\n",
    "    if avatar_url == \"N/A\":\n",
    "        avatar_download_bar.update(1)\n",
    "        continue\n",
    "    avatar_extension = os.path.splitext(avatar_url)[1].split('?')[0]\n",
    "    if avatar_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        avatar_extension = '.jpg'\n",
    "    avatar_filename = f\"avatars_user/{URL_list[idx]}{avatar_extension}\"\n",
    "    success = download_image(avatar_url, avatar_filename)\n",
    "    if not success:\n",
    "        avatar_filename = \"N/A\"\n",
    "    author_avatar_list[idx] = avatar_filename\n",
    "    avatar_download_bar.update(1)\n",
    "avatar_download_bar.close()\n",
    "\n",
    "print(\"所有图片下载完成。\")\n",
    "\n",
    "# 将数据整理为字典（包括“Author Avatar”与“Caption”字段）\n",
    "data = {\n",
    "    'Author Name': authorName_list,\n",
    "    'Likes': likeNr_list,\n",
    "    'Comments': commentNr_list,\n",
    "    'Post Title': post_title_list, \n",
    "    'Caption': caption_list,\n",
    "    'Date Published': datePublished_list,\n",
    "    'Images': images_list,          # 主图片的本地路径\n",
    "    'Author Avatar': author_avatar_list,  # 头像图片的本地路径\n",
    "    'Stars': starNr_list,\n",
    "    'Author Collect Nr': authorCollectNr_list,\n",
    "    'Author Fans Nr': authorFansNr_list,\n",
    "    'Author Note Nr': authorNoteNr_list,\n",
    "    'Video URL': video_urls,\n",
    "    'URL': URL_list,\n",
    "    'User URL': userURL_list\n",
    "}\n",
    "\n",
    "# 将数据存储为 DataFrame 并导出 CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('URL', inplace=True)\n",
    "print(df.head())\n",
    "df.to_csv('scraped_xhs_user.csv', encoding='utf-8-sig')\n",
    "print(\"数据已保存到 'scraped_xhs_user.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
