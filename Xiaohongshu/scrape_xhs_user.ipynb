{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在运行本笔记本之前，请先下载以下工具：\n",
    "\n",
    "1. **查找当前的 Google Chrome 版本**  \n",
    "   - 打开 Google Chrome，并在地址栏输入：  \n",
    "     ```\n",
    "     chrome://settings/help\n",
    "     ```\n",
    "   - 建议使用 **133** 版本的 Google Chrome（或最新可用的稳定版本）。\n",
    "\n",
    "2. **下载对应版本的 ChromeDriver**  \n",
    "   - 前往官方的 Chrome 测试下载页面：  \n",
    "     [https://googlechromelabs.github.io/chrome-for-testing/#stable](https://googlechromelabs.github.io/chrome-for-testing/#stable)  \n",
    "   - 下载与您的 Chrome 版本相匹配的 ChromeDriver。  \n",
    "   - 例如，如果您的 Chrome 版本是 133，请下载 **ChromeDriver 133**。\n",
    "\n",
    "3. **查找已下载的 ChromeDriver**  \n",
    "   - 如果您使用 macOS，可在终端 (Terminal) 中运行以下命令来查找 `chromedriver` 的位置：\n",
    "     ```bash\n",
    "     mdfind -name chromedriver\n",
    "     ```\n",
    "   - 如果使用其他操作系统，请检查默认的 **下载** 目录或您保存该文件的目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import requests  # 新增：用于下载图片\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 启动 Chrome 浏览器实例：\n",
    "\n",
    "打开 **terminal**, 下载 Chrome Driver (假的 Google Chrome)\n",
    "\n",
    "```bash\n",
    "brew install chromedriver\n",
    "chmod +x /opt/homebrew/bin/chromedriver\n",
    "```\n",
    "\n",
    "输入以下命令（将 `your Chrome.exe path` 替换为您的 Google Chrome 浏览器路径）：\n",
    "```bash\n",
    "<your Chrome.exe path> --remote-debugging-port=9222 --user-data-dir=\"/Users/<your home folder name>/selenium/AutomationProfile\"\n",
    "```\n",
    "\n",
    "- 请将your Chrome.exe path替换为您的Chrome浏览器所在路径，例如<br>`C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe`\n",
    "- 配置 chromedriver 相关信息，请参考官方文档：[ChromeDriver](https://developer.chrome.com/docs/chromedriver)\n",
    "- 来做个比方， 我的 *terminal command* 会是:\n",
    "\n",
    "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
    "  --remote-debugging-port=9222 \\\n",
    "  --user-data-dir=\"/Users/princess/selenium/AutomationProfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置Chrome浏览器\n",
    "options = Options()\n",
    "options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')\n",
    "options.add_argument('--incognito')\n",
    "browser = webdriver.Chrome(options=options)\n",
    "action = ActionChains(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将开始检查小红书登录状态...\n",
      "爬取数据有账户封禁的风险，建议使用非主账号登录。\n",
      "登录成功\n",
      "检查时间: Tue Feb 11 20:47:00 2025\n",
      "请根据提示输入用户主页URL和笔记爬取数量。\n",
      "检查用户主页加载状态...\n",
      "用户主页加载成功\n"
     ]
    }
   ],
   "source": [
    "user_url = \"\"\n",
    "num = 0\n",
    "\n",
    "def check_login_status(browser):\n",
    "    print(\"即将开始检查小红书登录状态...\")\n",
    "    print(\"爬取数据有账户封禁的风险，建议使用非主账号登录。\")\n",
    "    \n",
    "    while True:\n",
    "        page_source = browser.page_source\n",
    "        if '登录探索更多内容' in page_source:\n",
    "            print('暂未登录，请手动登录')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print('登录成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "def check_user_page_load_status(browser):\n",
    "    \"\"\"\n",
    "    检查用户主页是否加载成功。\n",
    "    这里等待页面中出现至少一个帖子（包含 \"note-item\" 的 section）。\n",
    "    \"\"\"\n",
    "    print(\"检查用户主页加载状态...\")\n",
    "    try:\n",
    "        wait = WebDriverWait(browser, 15)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//section[contains(@class, \"note-item\")]')))\n",
    "        print(\"用户主页加载成功\")\n",
    "    except Exception as e:\n",
    "        print(\"用户主页加载超时或出错:\", e)\n",
    "\n",
    "def selenium_test():\n",
    "    \"\"\"\n",
    "    登录状态检查、页面加载检查，并根据用户输入跳转到指定的用户主页进行爬取\n",
    "    \"\"\"\n",
    "    global user_url, num\n",
    "    # 首先打开探索页以触发登录检查\n",
    "    browser.get('https://www.xiaohongshu.com/explore')\n",
    "    check_login_status(browser)\n",
    "    \n",
    "    print(\"请根据提示输入用户主页URL和笔记爬取数量。\")\n",
    "    user_url = input(\"用户主页URL：\").strip()\n",
    "    try:\n",
    "        num = int(input(\"笔记爬取数量：\"))\n",
    "    except ValueError:\n",
    "        print(\"请输入有效的整数作为爬取数量。\")\n",
    "        return\n",
    "    \n",
    "    # 跳转到指定用户主页\n",
    "    browser.get(user_url)\n",
    "    time.sleep(3)\n",
    "    check_user_page_load_status(browser)\n",
    "\n",
    "# 调用 selenium_test() 进行初始化（确保此处 browser 已创建）\n",
    "selenium_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966246fb1af8443fb634c7027f7bf2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "爬取进度:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 21\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 42\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 42\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 42\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 63\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 73\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 73\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 86\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 97\n",
      "正在分析页面结构...\n",
      "找到 21 个内容元素\n",
      "当前已爬取总数: 107\n",
      "总共收集的条目数: 107\n",
      "收集的数据样本:\n",
      "URL: 678a28b50000000017003474\n",
      "URL: 6792206c0000000029034b18\n",
      "URL: 67ab22a5000000002901451f\n",
      "URL: 67a72569000000002903dbee\n",
      "URL: 6792fa6a000000002900d1a9\n",
      "截断后的总条目数: 100\n",
      "收集的数据样本:\n",
      "作者: 皇家宠物食品, 点赞: 21, URL: 678a28b50000000017003474\n",
      "作者: 皇家宠物食品, 点赞: 241, URL: 6792206c0000000029034b18\n",
      "作者: 皇家宠物食品, 点赞: 12, URL: 67ab22a5000000002901451f\n",
      "作者: 皇家宠物食品, 点赞: 36, URL: 67a72569000000002903dbee\n",
      "作者: 皇家宠物食品, 点赞: 40, URL: 6792fa6a000000002900d1a9\n",
      "开始提取附加字段，包括帖子内容、日期发布和评论数量...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39199c38fd69481f883a90bd88e08db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "已获取的笔记数量...:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 678a28b50000000017003474\n",
      "已提取附加字段，笔记ID: 6792206c0000000029034b18\n",
      "已提取附加字段，笔记ID: 67ab22a5000000002901451f\n",
      "已提取附加字段，笔记ID: 67a72569000000002903dbee\n",
      "已提取附加字段，笔记ID: 6792fa6a000000002900d1a9\n",
      "已提取附加字段，笔记ID: 6792f9d5000000002a00ce51\n",
      "已提取附加字段，笔记ID: 67936d94000000002902884a\n",
      "已提取附加字段，笔记ID: 6791e51000000000280350a1\n",
      "已提取附加字段，笔记ID: 678f5cbf0000000029008b82\n",
      "已提取附加字段，笔记ID: 678a2ff6000000001901b6ec\n",
      "已提取附加字段，笔记ID: 67874af50000000019006f5f\n",
      "已提取附加字段，笔记ID: 6787142b000000001603e621\n",
      "已提取附加字段，笔记ID: 677f9560000000000b038b88\n",
      "已提取附加字段，笔记ID: 6777b89a000000000902fcff\n",
      "已提取附加字段，笔记ID: 67775dee000000001301be86\n",
      "已提取附加字段，笔记ID: 6773bc94000000001300f133\n",
      "已提取附加字段，笔记ID: 67740a1700000000140265e9\n",
      "已提取附加字段，笔记ID: 67736c08000000000902dcfd\n",
      "已提取附加字段，笔记ID: 676cd4f1000000001301b94a\n",
      "已提取附加字段，笔记ID: 676be337000000000902e705\n",
      "已提取附加字段，笔记ID: 676bacb1000000001402505a\n",
      "已提取附加字段，笔记ID: 6766293c000000000902ec2f\n",
      "已提取附加字段，笔记ID: 676530c900000000130086e0\n",
      "已提取附加字段，笔记ID: 6764e385000000001301bda3\n",
      "已提取附加字段，笔记ID: 6763f35e00000000130181a5\n",
      "已提取附加字段，笔记ID: 6763e990000000000902e49d\n",
      "已提取附加字段，笔记ID: 676008680000000013001e5f\n",
      "已提取附加字段，笔记ID: 6758f5910000000006038389\n",
      "已提取附加字段，笔记ID: 675801ef000000000603a634\n",
      "已提取附加字段，笔记ID: 67511fa3000000000800492b\n",
      "已提取附加字段，笔记ID: 674e700a000000000702747b\n",
      "已提取附加字段，笔记ID: 674982ac000000000702793e\n",
      "已提取附加字段，笔记ID: 67493a6a0000000007037051\n",
      "已提取附加字段，笔记ID: 674567dd00000000070259dc\n",
      "已提取附加字段，笔记ID: 67456dd4000000000703adbf\n",
      "已提取附加字段，笔记ID: 67440f45000000000703884b\n",
      "已提取附加字段，笔记ID: 673f1d4300000000070313e9\n",
      "已提取附加字段，笔记ID: 673e9f6600000000080075f2\n",
      "已提取附加字段，笔记ID: 673d99730000000008004cd8\n",
      "已提取附加字段，笔记ID: 673d4fec0000000008006a9f\n",
      "已提取附加字段，笔记ID: 673c74fb000000000702486c\n",
      "已提取附加字段，笔记ID: 673a9b4c000000001a01cb6c\n",
      "已提取附加字段，笔记ID: 673055a5000000001b0117be\n",
      "已提取附加字段，笔记ID: 672c55d00000000019014a94\n",
      "已提取附加字段，笔记ID: 6729f0ff000000001b012846\n",
      "已提取附加字段，笔记ID: 672881bd000000001b011f53\n",
      "已提取附加字段，笔记ID: 67285cfb000000001b02c539\n",
      "已提取附加字段，笔记ID: 6727096f000000001901ab46\n",
      "已提取附加字段，笔记ID: 67247ba00000000019018d8d\n",
      "已提取附加字段，笔记ID: 67247afc000000001b02b315\n",
      "已提取附加字段，笔记ID: 67236dab000000001b02917f\n",
      "已提取附加字段，笔记ID: 6722e1d8000000001b028606\n",
      "已提取附加字段，笔记ID: 6721a09e000000001901bdbd\n",
      "已提取附加字段，笔记ID: 6720a2bc000000001b012a91\n",
      "已提取附加字段，笔记ID: 671b7dec0000000024014c16\n",
      "已提取附加字段，笔记ID: 671740bc000000002100361e\n",
      "已提取附加字段，笔记ID: 671214fc000000002401bbdc\n",
      "已提取附加字段，笔记ID: 6710d83900000000210014ed\n",
      "已提取附加字段，笔记ID: 6710adea0000000021002d07\n",
      "已提取附加字段，笔记ID: 670f3272000000001b03db6e\n",
      "已提取附加字段，笔记ID: 670e07a0000000002401addb\n",
      "已提取附加字段，笔记ID: 670d0ac20000000021008b9f\n",
      "已提取附加字段，笔记ID: 670c859a00000000210051d9\n",
      "已提取附加字段，笔记ID: 670cb30f0000000021003efc\n",
      "已提取附加字段，笔记ID: 670a4ce00000000021002d63\n",
      "已提取附加字段，笔记ID: 6708d2ee000000001a022d41\n",
      "已提取附加字段，笔记ID: 67079216000000001902d404\n",
      "已提取附加字段，笔记ID: 67079120000000001902fca3\n",
      "已提取附加字段，笔记ID: 6707829e000000001b021b8f\n",
      "已提取附加字段，笔记ID: 66fb7a5f000000001902f1d5\n",
      "已提取附加字段，笔记ID: 66f65aae000000001b0221c9\n",
      "已提取附加字段，笔记ID: 66f538cf000000001902e2ba\n",
      "已提取附加字段，笔记ID: 66f0cd6f000000001e018246\n",
      "已提取附加字段，笔记ID: 66eecfb400000000120106a4\n",
      "已提取附加字段，笔记ID: 66ecf3060000000012012948\n",
      "已提取附加字段，笔记ID: 66e8117d000000001e0181a0\n",
      "已提取附加字段，笔记ID: 66e53ffd000000001e01a272\n",
      "已提取附加字段，笔记ID: 66e530d5000000000c01ae7f\n",
      "已提取附加字段，笔记ID: 66e3b6960000000012013f08\n",
      "已提取附加字段，笔记ID: 66e2ba10000000001e019adb\n",
      "已提取附加字段，笔记ID: 66e2a0c1000000001e01a0e9\n",
      "已提取附加字段，笔记ID: 66e03b46000000001e018229\n",
      "已提取附加字段，笔记ID: 66de931a0000000012010c4d\n",
      "已提取附加字段，笔记ID: 66dd8e4600000000120117af\n",
      "已提取附加字段，笔记ID: 66dd341c0000000012010449\n",
      "已提取附加字段，笔记ID: 66dc4ead000000000c018764\n",
      "已提取附加字段，笔记ID: 66dc22f1000000001201191f\n",
      "已提取附加字段，笔记ID: 66d6ed120000000012011316\n",
      "已提取附加字段，笔记ID: 66d2c9e4000000001d018583\n",
      "已提取附加字段，笔记ID: 66d15512000000001d03b399\n",
      "已提取附加字段，笔记ID: 66d03e07000000001d01acc4\n",
      "已提取附加字段，笔记ID: 66cff59f000000001d01994b\n",
      "已提取附加字段，笔记ID: 66cee59e000000001d01a6ff\n",
      "已提取附加字段，笔记ID: 66c7051d000000001d0166c4\n",
      "已提取附加字段，笔记ID: 66c6a45e000000001d038617\n",
      "已提取附加字段，笔记ID: 66c48ff0000000001d03b43d\n",
      "已提取附加字段，笔记ID: 66c40566000000001d017232\n",
      "已提取附加字段，笔记ID: 66c30d31000000001d0394ff\n",
      "已提取附加字段，笔记ID: 66c20fe2000000002503008b\n",
      "已提取附加字段，笔记ID: 66c211fc0000000025032827\n",
      "开始下载主图片和头像图片...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922db603eb684f1dbf23bf69c29c4cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "下载主图片:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c214e91769f46bcac0aba5315320486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "下载头像图片:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有图片下载完成。\n",
      "                         Author Name Likes  Comments           Post Title  \\\n",
      "Post ID                                                                     \n",
      "678a28b50000000017003474      皇家宠物食品    21  共 14 条评论       新春喵宴！宠物年夜饭🐱皇包了   \n",
      "6792206c0000000029034b18      皇家宠物食品   241         0   📒体验官招募｜幼猫幼犬，就吃皇家❗️   \n",
      "67ab22a5000000002901451f      皇家宠物食品    12  共 10 条评论   2.14不止撒糖，还要给主子撒福利！   \n",
      "67a72569000000002903dbee      皇家宠物食品    36         0  复工精神状态良好...只是想体验猫生🐱   \n",
      "6792fa6a000000002900d1a9      皇家宠物食品    40  共 69 条评论       大年初一|阿皇来给您拜年啦！   \n",
      "\n",
      "                                                                    Caption  \\\n",
      "Post ID                                                                       \n",
      "678a28b50000000017003474  新春来临，阿皇给毛孩子送年夜饭🎁 干粮吃腻了？马上呈上湿粮餐包🍚 小香包和小鲜包香味四溢，饭...   \n",
      "6792206c0000000029034b18  “新手养宠如何养出健康小猫小狗？” 相信是很多铲屎官面临的难题🤯 毕竟打好身体基础真的重要！...   \n",
      "67ab22a5000000002901451f  铲屎官紧急集合 🐾 2月14日不止是情人节，更是【皇家宠爱日】👑 下午4点-6点，锁定直播间...   \n",
      "67a72569000000002903dbee  阿皇复工后的精神状态be like： 身体在工位，灵魂还在床上 眼睛盯着屏幕，脑子已经飞到周...   \n",
      "6792fa6a000000002900d1a9  新年新岁，万象更新 走街串巷都会迎来那一句：新年快乐！ 阿皇也在这里祝毛孩子和铲屎官们蛇年大...   \n",
      "\n",
      "                           Date Published  \\\n",
      "Post ID                                     \n",
      "678a28b50000000017003474      编辑于 5 天前 江苏   \n",
      "6792206c0000000029034b18      编辑于 5 天前 江苏   \n",
      "67ab22a5000000002901451f      今天 05:12 江苏   \n",
      "67a72569000000002903dbee  编辑于 昨天 01:18 江苏   \n",
      "6792fa6a000000002900d1a9      编辑于 5 天前 江苏   \n",
      "\n",
      "                                                            Images  \\\n",
      "Post ID                                                              \n",
      "678a28b50000000017003474  images_user/678a28b50000000017003474.jpg   \n",
      "6792206c0000000029034b18  images_user/6792206c0000000029034b18.jpg   \n",
      "67ab22a5000000002901451f  images_user/67ab22a5000000002901451f.jpg   \n",
      "67a72569000000002903dbee  images_user/67a72569000000002903dbee.jpg   \n",
      "6792fa6a000000002900d1a9  images_user/6792fa6a000000002900d1a9.jpg   \n",
      "\n",
      "                                                      Author Avatar Stars  \\\n",
      "Post ID                                                                     \n",
      "678a28b50000000017003474  avatars_user/678a28b50000000017003474.jpg     赞   \n",
      "6792206c0000000029034b18  avatars_user/6792206c0000000029034b18.jpg   241   \n",
      "67ab22a5000000002901451f  avatars_user/67ab22a5000000002901451f.jpg     赞   \n",
      "67a72569000000002903dbee  avatars_user/67a72569000000002903dbee.jpg    36   \n",
      "6792fa6a000000002900d1a9  avatars_user/6792fa6a000000002900d1a9.jpg     赞   \n",
      "\n",
      "                         Author Collect Nr Author Fans Nr  Author Note Nr  \\\n",
      "Post ID                                                                     \n",
      "678a28b50000000017003474              8.5万           2.2万              21   \n",
      "6792206c0000000029034b18              8.5万           2.2万              21   \n",
      "67ab22a5000000002901451f              8.5万           2.2万              21   \n",
      "67a72569000000002903dbee              8.5万           2.2万              21   \n",
      "6792fa6a000000002900d1a9              8.5万           2.2万              21   \n",
      "\n",
      "                         Video URL  \\\n",
      "Post ID                              \n",
      "678a28b50000000017003474       N/A   \n",
      "6792206c0000000029034b18       N/A   \n",
      "67ab22a5000000002901451f       N/A   \n",
      "67a72569000000002903dbee       N/A   \n",
      "6792fa6a000000002900d1a9       N/A   \n",
      "\n",
      "                                                                   Note URL  \n",
      "Post ID                                                                      \n",
      "678a28b50000000017003474  /user/profile/5c03825e000000000500e9ce/678a28b...  \n",
      "6792206c0000000029034b18  /user/profile/5c03825e000000000500e9ce/6792206...  \n",
      "67ab22a5000000002901451f  /user/profile/5c03825e000000000500e9ce/67ab22a...  \n",
      "67a72569000000002903dbee  /user/profile/5c03825e000000000500e9ce/67a7256...  \n",
      "6792fa6a000000002900d1a9  /user/profile/5c03825e000000000500e9ce/6792fa6...  \n",
      "数据已保存到 'scraped_xhs_user.csv'\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据存储列表\n",
    "authorName_list = []\n",
    "likeNr_list = []\n",
    "id_list = []\n",
    "noteURL_list = []\n",
    "commentNr_list = []\n",
    "post_title_list = [] \n",
    "caption_list = []  # 存储帖子内容\n",
    "datePublished_list = []\n",
    "images_list = []\n",
    "author_avatar_list = []  # 用于存储头像图片\n",
    "starNr_list = []\n",
    "authorCollectNr_list = []\n",
    "authorFansNr_list = []\n",
    "authorNoteNr_list = []\n",
    "video_urls = [] \n",
    "\n",
    "def parseUserPage(page_source):\n",
    "    \"\"\"\n",
    "    解析当前页面的HTML内容，提取用户笔记的基本信息并更新对应的列表。\n",
    "    同时，利用页面中提取一次作者的个人数据（收藏、粉丝和笔记数量）\n",
    "    来填充每个笔记记录对应的作者数据。\n",
    "    \n",
    "    Args:\n",
    "        page_source (str): 当前页面的HTML内容\n",
    "    \"\"\"\n",
    "    response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "    selector = Selector(response)\n",
    "\n",
    "    print(\"正在分析页面结构...\")\n",
    "\n",
    "    content_elements = selector.xpath('//section[contains(@class, \"note-item\")]')\n",
    "    if content_elements:\n",
    "        print(f\"找到 {len(content_elements)} 个内容元素\")\n",
    "        for element in content_elements:\n",
    "            try:\n",
    "                # 提取帖子（笔记）的相对URL及 note_id\n",
    "                note_url = element.xpath('.//a[contains(@class, \"cover\")]/@href').get()\n",
    "                if note_url:\n",
    "                    note_id = note_url.split('/')[-1].split('?')[0]\n",
    "                    if note_id in id_list:\n",
    "                        continue  # 避免重复爬取\n",
    "                    id_list.append(note_id)\n",
    "                    noteURL_list.append(note_url)\n",
    "\n",
    "                    # 提取作者名字（通常与用户主页相同）\n",
    "                    author = element.xpath('.//div[contains(@class, \"author-wrapper\")]//span[contains(@class, \"name\")]/text()').get()\n",
    "                    authorName_list.append(author.strip() if author else \"N/A\")\n",
    "\n",
    "                    # 提取点赞数量\n",
    "                    likes = element.xpath('.//span[contains(@class, \"like-wrapper\")]/span[contains(@class, \"count\")]/text()').get()\n",
    "                    likeNr_list.append(likes.strip() if likes else \"0\")\n",
    "\n",
    "                    # 提取帖子标题\n",
    "                    post_title = element.xpath('.//a[contains(@class, \"title\")]//span/text()').getall()\n",
    "                    post_title_cleaned = ' '.join([c.strip() for c in post_title if c.strip()])\n",
    "                    post_title_list.append(post_title_cleaned if post_title_cleaned else \"N/A\")\n",
    "\n",
    "                    # 提取图片（主图）\n",
    "                    main_image = element.xpath('.//a[contains(@class, \"cover\")]/img/@src').get()\n",
    "                    images_list.append(main_image.strip() if main_image else \"N/A\")\n",
    "\n",
    "                    # 提取头像图片\n",
    "                    avatar_image = element.xpath('.//a[contains(@class, \"author\")]/img/@src').get()\n",
    "                    author_avatar_list.append(avatar_image.strip() if avatar_image else \"N/A\")\n",
    "\n",
    "                    # 选择并提取网页数据\n",
    "                    author_collect_nr = selector.xpath('//*[@class=\"data-info\"]/div[1]/div[3]/span[@class=\"count\"]/text()').extract_first()# 作者获赞与收藏数量\n",
    "                    author_fans_nr = selector.xpath('//*[@class=\"data-info\"]/div[1]/div[2]/span[@class=\"count\"]/text()').extract_first()# 作者粉丝数量\n",
    "                    author_note_nr = len(selector.xpath('//*[@id=\"userPostedFeeds\"]//section'))# 作者笔记数量\n",
    "\n",
    "                    authorCollectNr_list.append(author_collect_nr)\n",
    "                    authorFansNr_list.append(author_fans_nr)\n",
    "                    authorNoteNr_list.append(author_note_nr)\n",
    "\n",
    "                    # 初始化附加字段：\n",
    "                    # note‑specific fields (后面会在 parseNotePage 中更新)\n",
    "                    commentNr_list.append(\"0\")\n",
    "                    datePublished_list.append(\"N/A\")\n",
    "                    starNr_list.append(\"0\")\n",
    "                    video_urls.append(\"N/A\")\n",
    "                    caption_list.append(\"N/A\")\n",
    "                    \n",
    "                    qbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"处理元素时出错: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"当前已爬取总数: {len(id_list)}\")\n",
    "\n",
    "def parseNotePage(note_url, note_id):\n",
    "    \"\"\"\n",
    "    访问每个笔记的页面，提取附加字段，包括评论数量、发布时间、收藏数量（针对笔记）、\n",
    "    视频 URL 以及帖子内容（Caption）。\n",
    "    注意：原来提取作者个人数据的代码已移入 parseUserPage。\n",
    "    \n",
    "    Args:\n",
    "        note_url (str): 笔记的相对URL\n",
    "        note_id (str): 笔记的唯一ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_note_url = f'https://www.xiaohongshu.com{note_url}'\n",
    "        browser.get(full_note_url)\n",
    "        \n",
    "        # 等待页面加载中关键的描述 meta 标签\n",
    "        wait = WebDriverWait(browser, 15)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@name=\"description\"]')))\n",
    "        \n",
    "        page_source = browser.page_source\n",
    "        response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "        selector = Selector(response)\n",
    "\n",
    "        # 提取评论数量\n",
    "        comments = selector.xpath('//*[@class=\"total\"]/text()').get()\n",
    "        comments = comments.strip() if comments else \"0\"\n",
    "\n",
    "        # 提取发布时间（采用多种 XPath 尝试）\n",
    "        date_published = selector.xpath('//*[@class=\"date\"]/text()').get()\n",
    "        if not date_published:\n",
    "            date_published = selector.xpath('//time/@datetime').get()\n",
    "        date_published = date_published.strip() if date_published else \"N/A\"\n",
    "\n",
    "        # 提取笔记的收藏数量（stars）\n",
    "        stars = selector.xpath('//*[@class=\"count\"]/text()').get()\n",
    "        stars = stars.strip() if stars else \"0\"\n",
    "\n",
    "        # 提取视频 URL（如果有）\n",
    "        video_url = selector.xpath('//video/@src').get()\n",
    "        video_url = video_url.strip() if video_url else \"N/A\"\n",
    "\n",
    "        # 提取帖子内容（Caption）\n",
    "        caption = selector.xpath('//*[@name=\"description\"]/@content').get()\n",
    "        caption = caption.strip() if caption else \"N/A\"\n",
    "\n",
    "        # 更新全局列表中的相应条目\n",
    "        if note_id in id_list:\n",
    "            index = id_list.index(note_id)\n",
    "            commentNr_list[index] = comments\n",
    "            datePublished_list[index] = date_published\n",
    "            starNr_list[index] = stars\n",
    "            video_urls[index] = video_url\n",
    "            caption_list[index] = caption\n",
    "            print(f\"已提取附加字段，笔记ID: {note_id}\")\n",
    "        else:\n",
    "            print(f\"笔记ID {note_id} 未在 id_list 中找到。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"提取附加字段时出错，笔记ID: {note_id}, 错误: {str(e)}\")\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"\n",
    "    下载图片并保存到指定路径。\n",
    "    \n",
    "    Args:\n",
    "        url (str): 图片的 URL 地址\n",
    "        save_path (str): 图片保存的本地路径\n",
    "    \n",
    "    Returns:\n",
    "        bool: 下载是否成功\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"下载图片时出错，URL: {url}, 错误: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"\n",
    "    创建用于存储主图片和头像图片的目录。\n",
    "    \"\"\"\n",
    "    if not os.path.exists('images_user'):\n",
    "        os.makedirs('images_user')\n",
    "    if not os.path.exists('avatars_user'):\n",
    "        os.makedirs('avatars_user')\n",
    "\n",
    "# 创建目录以存储图片\n",
    "create_directories()\n",
    "\n",
    "# 定义进度条用于跟踪已爬取的笔记数量\n",
    "qbar = tqdm(total=num, desc=\"爬取进度\")\n",
    "\n",
    "# 开始下拉加载页面直至达到所需笔记数量\n",
    "while len(id_list) < num:\n",
    "    for _ in range(3):\n",
    "        browser.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    parseUserPage(browser.page_source)\n",
    "\n",
    "    # 检测是否到达页面末尾（根据页面提示文本判断）\n",
    "    if '- THE END -' in browser.page_source or 'No more content' in browser.page_source:\n",
    "        print(f\"已到达内容末尾。总共收集: {len(id_list)} 条\")\n",
    "        break\n",
    "\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "print(f\"总共收集的条目数: {len(id_list)}\")\n",
    "if id_list:\n",
    "    print(\"收集的数据样本:\")\n",
    "    for i in range(min(5, len(id_list))):\n",
    "        print(f\"URL: {id_list[i]}\")\n",
    "\n",
    "# 如果收集的条目超过用户所需数量，则截断各个列表\n",
    "if len(id_list) > num:\n",
    "    id_list = id_list[:num]\n",
    "    authorName_list = authorName_list[:num]\n",
    "    likeNr_list = likeNr_list[:num]\n",
    "    noteURL_list = noteURL_list[:num]\n",
    "    commentNr_list = commentNr_list[:num]\n",
    "    post_title_list = post_title_list[:num]\n",
    "    datePublished_list = datePublished_list[:num]\n",
    "    images_list = images_list[:num]\n",
    "    author_avatar_list = author_avatar_list[:num]\n",
    "    starNr_list = starNr_list[:num]\n",
    "    authorCollectNr_list = authorCollectNr_list[:num]\n",
    "    authorFansNr_list = authorFansNr_list[:num]\n",
    "    authorNoteNr_list = authorNoteNr_list[:num]\n",
    "    video_urls = video_urls[:num]\n",
    "    caption_list = caption_list[:num]\n",
    "\n",
    "print(f\"截断后的总条目数: {len(id_list)}\")\n",
    "print(\"收集的数据样本:\")\n",
    "for i in range(min(5, len(id_list))):\n",
    "    print(f\"作者: {authorName_list[i]}, 点赞: {likeNr_list[i]}, URL: {id_list[i]}\")\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "# 提取每个笔记的附加字段（例如帖子内容、发布时间和评论数量）\n",
    "print(\"开始提取附加字段，包括帖子内容、日期发布和评论数量...\")\n",
    "qbar = tqdm(total=len(id_list), desc=\"已获取的笔记数量...\")\n",
    "\n",
    "for note_id, note_url in zip(id_list, noteURL_list):\n",
    "    parseNotePage(note_url, note_id)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(2, 4))  # 礼貌等待，避免服务器过载\n",
    "\n",
    "qbar.close()\n",
    "\n",
    "# 下载主图片和头像图片\n",
    "print(\"开始下载主图片和头像图片...\")\n",
    "\n",
    "# 下载主图片\n",
    "image_download_bar = tqdm(total=len(images_list), desc=\"下载主图片\")\n",
    "for idx, image_url in enumerate(images_list):\n",
    "    if image_url == \"N/A\":\n",
    "        image_download_bar.update(1)\n",
    "        continue\n",
    "    # 构造图片保存路径，使用 note_id 作为文件名\n",
    "    image_extension = os.path.splitext(image_url)[1].split('?')[0]\n",
    "    if image_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        image_extension = '.jpg'\n",
    "    image_filename = f\"images_user/{id_list[idx]}{image_extension}\"\n",
    "    success = download_image(image_url, image_filename)\n",
    "    if not success:\n",
    "        image_filename = \"N/A\"\n",
    "    images_list[idx] = image_filename\n",
    "    image_download_bar.update(1)\n",
    "image_download_bar.close()\n",
    "\n",
    "# 下载头像图片\n",
    "avatar_download_bar = tqdm(total=len(author_avatar_list), desc=\"下载头像图片\")\n",
    "for idx, avatar_url in enumerate(author_avatar_list):\n",
    "    if avatar_url == \"N/A\":\n",
    "        avatar_download_bar.update(1)\n",
    "        continue\n",
    "    avatar_extension = os.path.splitext(avatar_url)[1].split('?')[0]\n",
    "    if avatar_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        avatar_extension = '.jpg'\n",
    "    avatar_filename = f\"avatars_user/{id_list[idx]}{avatar_extension}\"\n",
    "    success = download_image(avatar_url, avatar_filename)\n",
    "    if not success:\n",
    "        avatar_filename = \"N/A\"\n",
    "    author_avatar_list[idx] = avatar_filename\n",
    "    avatar_download_bar.update(1)\n",
    "avatar_download_bar.close()\n",
    "\n",
    "print(\"所有图片下载完成。\")\n",
    "\n",
    "# 将数据整理为字典（包括“Author Avatar”与“Caption”字段）\n",
    "data = {\n",
    "    'Author Name': authorName_list,\n",
    "    'Likes': likeNr_list,\n",
    "    'Comments': commentNr_list,\n",
    "    'Post Title': post_title_list, \n",
    "    'Caption': caption_list,\n",
    "    'Date Published': datePublished_list,\n",
    "    'Images': images_list,          # 主图片的本地路径\n",
    "    'Author Avatar': author_avatar_list,  # 头像图片的本地路径\n",
    "    'Stars': starNr_list,\n",
    "    'Author Collect Nr': authorCollectNr_list,\n",
    "    'Author Fans Nr': authorFansNr_list,\n",
    "    'Author Note Nr': authorNoteNr_list,\n",
    "    'Video URL': video_urls,\n",
    "    'Post ID': id_list,\n",
    "    'Note URL': noteURL_list\n",
    "}\n",
    "\n",
    "# 将数据存储为 DataFrame 并导出 CSV\n",
    "df = pd.DataFrame(data)\n",
    "# 若你想以某个字段作为索引，请确保该字段在 data 中存在\n",
    "df.set_index('Post ID', inplace=True)\n",
    "print(df.head())\n",
    "df.to_csv('scraped_xhs_user.csv', encoding='utf-8-sig')\n",
    "print(\"数据已保存到 'scraped_xhs_user.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
