{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在运行本笔记本之前，请先下载以下工具：\n",
    "\n",
    "1. **查找当前的 Google Chrome 版本**  \n",
    "   - 打开 Google Chrome，并在地址栏输入：  \n",
    "     ```\n",
    "     chrome://settings/help\n",
    "     ```\n",
    "   - 建议使用 **133** 版本的 Google Chrome（或最新可用的稳定版本）。\n",
    "\n",
    "2. **下载对应版本的 ChromeDriver**  \n",
    "   - 前往官方的 Chrome 测试下载页面：  \n",
    "     [https://googlechromelabs.github.io/chrome-for-testing/#stable](https://googlechromelabs.github.io/chrome-for-testing/#stable)  \n",
    "   - 下载与您的 Chrome 版本相匹配的 ChromeDriver。  \n",
    "   - 例如，如果您的 Chrome 版本是 133，请下载 **ChromeDriver 133**。\n",
    "\n",
    "3. **查找已下载的 ChromeDriver**  \n",
    "   - 如果您使用 macOS，可在终端 (Terminal) 中运行以下命令来查找 `chromedriver` 的位置：\n",
    "     ```bash\n",
    "     mdfind -name chromedriver\n",
    "     ```\n",
    "   - 如果使用其他操作系统，请检查默认的 **下载** 目录或您保存该文件的目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import requests  # 新增：用于下载图片\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 启动 Chrome 浏览器实例：\n",
    "\n",
    "打开 **terminal**, 下载 Chrome Driver (假的 Google Chrome)\n",
    "\n",
    "```bash\n",
    "brew install chromedriver\n",
    "chmod +x /opt/homebrew/bin/chromedriver\n",
    "```\n",
    "\n",
    "输入以下命令（将 `your Chrome.exe path` 替换为您的 Google Chrome 浏览器路径）：\n",
    "```bash\n",
    "<your Chrome.exe path> --remote-debugging-port=9222 --user-data-dir=\"/Users/<your home folder name>/selenium/AutomationProfile\"\n",
    "```\n",
    "\n",
    "- 请将your Chrome.exe path替换为您的Chrome浏览器所在路径，例如<br>`C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe`\n",
    "- 配置 chromedriver 相关信息，请参考官方文档：[ChromeDriver](https://developer.chrome.com/docs/chromedriver)\n",
    "- 来做个比方， 我的 *terminal command* 会是:\n",
    "\n",
    "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
    "  --remote-debugging-port=9222 \\\n",
    "  --user-data-dir=\"/Users/princess/selenium/AutomationProfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置Chrome浏览器\n",
    "options = Options()\n",
    "options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument(\"--headless\")\n",
    "browser = webdriver.Chrome(options=options)\n",
    "action = ActionChains(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将开始检查小红书登录状态...\n",
      "爬取数据有账户封禁的风险，建议使用非主账号登录。\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:34 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:35 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:36 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:37 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:38 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:39 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:40 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:41 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:42 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:43 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:44 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:45 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:46 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:47 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:48 2025\n",
      "暂未登录，请手动登录\n",
      "检查时间: Wed Feb 12 19:57:49 2025\n",
      "登录成功\n",
      "检查时间: Wed Feb 12 19:57:50 2025\n",
      "请在文本框中根据提示输入搜索关键词和笔记爬取数量。\n",
      "即将开始检查网页加载状态...\n",
      "如果网页进入人机验证页面，请先手动完成验证。\n",
      "加载成功\n",
      "检查时间: Wed Feb 12 19:58:01 2025\n"
     ]
    }
   ],
   "source": [
    "key_word = \"\"\n",
    "num = 0\n",
    "\n",
    "def check_login_status(browser):\n",
    "    print(\"即将开始检查小红书登录状态...\")\n",
    "    print(\"爬取数据有账户封禁的风险，建议使用非主账号登录。\")\n",
    "    \n",
    "    while True:\n",
    "        page_source = browser.page_source\n",
    "        if '登录探索更多内容' in page_source:\n",
    "            print('暂未登录，请手动登录')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print('登录成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            time.sleep(1)\n",
    "            break\n",
    "\n",
    "def check_page_load_status(browser, keyword):\n",
    "    print(\"即将开始检查网页加载状态...\")\n",
    "    print(\"如果网页进入人机验证页面，请先手动完成验证。\")\n",
    "    \n",
    "    while True:\n",
    "        if keyword in browser.title:\n",
    "            print('加载成功')\n",
    "            print('检查时间:', time.ctime())\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "def selenium_test():\n",
    "    \"\"\"\n",
    "    登录状态检查，网页加载检查，根据用户输入进行搜索\n",
    "    \"\"\"\n",
    "    global key_word, num\n",
    "    browser.get('https://www.xiaohongshu.com/explore')\n",
    "    \n",
    "    check_login_status(browser)\n",
    "    \n",
    "    print(\"请在文本框中根据提示输入搜索关键词和笔记爬取数量。\")\n",
    "    keyword = input(\"搜索关键词：\")\n",
    "    key_word = keyword\n",
    "    \n",
    "    try:\n",
    "        num = int(input(\"笔记爬取数量：\"))\n",
    "    except ValueError:\n",
    "        print(\"请输入有效的整数作为爬取数量。\")\n",
    "        return\n",
    "    \n",
    "    url = f'https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_explore_feed'\n",
    "    browser.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    check_page_load_status(browser, keyword)\n",
    "\n",
    "selenium_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已自动更改模式为图文。\n",
      "请选择排序方式:\n",
      "1. 综合\n",
      "2. 最新\n",
      "3. 最热\n",
      "请输入有效的排序方式...\n"
     ]
    }
   ],
   "source": [
    "def change_mode(browser):\n",
    "    # 更改模式为图文\n",
    "    try:\n",
    "        mode_button = browser.find_element(By.XPATH, '//*[@id=\"search-type\"]/div/div/div[2]')\n",
    "        mode_button.click()\n",
    "        print('已自动更改模式为图文。')\n",
    "    except Exception as e:\n",
    "        print(f\"更改模式失败: {e}\")\n",
    "\n",
    "selected_order_text = ''\n",
    "def change_sort_order(browser, action):\n",
    "    # 更改排序方式\n",
    "    sort_order = {\n",
    "        '综合': 1,\n",
    "        '最新': 2,\n",
    "        '最热': 3\n",
    "    }\n",
    "    print(\"请选择排序方式:\")\n",
    "    for idx, order in sort_order.items():\n",
    "        print(f'{order}. {idx}')\n",
    "    \n",
    "    try:\n",
    "        global selected_order_text\n",
    "        selected_order_text = input(\"请输入排序方式对应的名称: \").strip()\n",
    "        if selected_order_text not in sort_order:\n",
    "            print(\"请输入有效的排序方式...\")\n",
    "            return\n",
    "        \n",
    "        selected_order_index = sort_order[selected_order_text]\n",
    "    except Exception as e:\n",
    "        print(f\"处理排序选择时出错: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        element = browser.find_element(By.XPATH, '//*[@id=\"global\"]/div[2]/div[2]/div/div[1]/div[2]')\n",
    "        action.move_to_element(element).perform()# 模拟鼠标悬停\n",
    "        menu = browser.find_element(By.CLASS_NAME, 'dropdown-items')\n",
    "        option = menu.find_element(By.XPATH, f'/html/body/div[4]/div/li[{selected_order_index}]')\n",
    "        option.click()# 模拟鼠标点击\n",
    "\n",
    "        print('已选择排序方式为:',selected_order_text)\n",
    "        print('检查时间:',time.ctime())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"更改排序方式失败: {e}\")\n",
    "\n",
    "change_mode(browser)\n",
    "change_sort_order(browser, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55ba8f19d0247cca1e799d95450e26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "爬取进度:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前URL: https://www.xiaohongshu.com/search_result?keyword=%E7%8B%97%E7%B2%AE&source=web_explore_feed&type=51\n",
      "已导航到搜索结果页面。\n",
      "找到元素，选择器: //*[contains(text(), '图文')]\n",
      "正在分析页面结构...\n",
      "找到容器，xpath: //div[contains(@class, \"note-item\")]\n",
      "找到 18 个内容元素\n",
      "当前已爬取总数: 16\n",
      "总共收集的条目数: 16\n",
      "收集的数据样本:\n",
      "URL: 67a9ba5a000000001902e37f\n",
      "URL: 67ac08b6000000001703ab73\n",
      "URL: 673845cc000000001b011357\n",
      "URL: 666fef14000000000e032cb5\n",
      "URL: 640c28810000000013012bb8\n",
      "截断后的总条目数: 10\n",
      "收集的数据样本:\n",
      "作者: 祁花椒, 点赞: 6, URL: 67a9ba5a000000001902e37f\n",
      "作者: 还好老子可爱, 点赞: 2, URL: 67ac08b6000000001703ab73\n",
      "作者: 不冷狗狗, 点赞: 121, URL: 673845cc000000001b011357\n",
      "作者: Hello Pets Plus 宠物日用, 点赞: 78, URL: 666fef14000000000e032cb5\n",
      "作者: 柴布丁日常, 点赞: 565, URL: 640c28810000000013012bb8\n",
      "开始提取用户信息，包括收藏数量、粉丝数量和笔记数量...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9497f03e9b4768a5254cacb92f4ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "已提取的用户信息:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "已提取作者附加字段。\n",
      "开始提取附加字段，包括帖子内容、日期发布和评论数量...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9bb9ae79904fc09505c41322e85570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "已获取的笔记数量...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已提取附加字段，笔记ID: 67a9ba5a000000001902e37f\n",
      "已提取附加字段，笔记ID: 67ac08b6000000001703ab73\n",
      "已提取附加字段，笔记ID: 673845cc000000001b011357\n",
      "已提取附加字段，笔记ID: 666fef14000000000e032cb5\n",
      "已提取附加字段，笔记ID: 640c28810000000013012bb8\n",
      "已提取附加字段，笔记ID: 64982849000000001300ab83\n",
      "已提取附加字段，笔记ID: 66978f81000000000d00d491\n",
      "已提取附加字段，笔记ID: 667bcd97000000001f004b81\n",
      "已提取附加字段，笔记ID: 6662df6d000000001303ec3a\n",
      "已提取附加字段，笔记ID: 66b9db5e0000000009017f85\n",
      "开始下载主图片和头像图片...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897c4cae50ea41feb8be6d168584b062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "下载主图片:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4c5f73e2e44313ab8a6b0ee082e97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "下载头像图片:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有图片下载完成。\n",
      "                                   Author Name Likes Comments  \\\n",
      "Post ID                                                         \n",
      "67a9ba5a000000001902e37f                   祁花椒     6        0   \n",
      "67ac08b6000000001703ab73                还好老子可爱     2        0   \n",
      "673845cc000000001b011357                  不冷狗狗   121        0   \n",
      "666fef14000000000e032cb5  Hello Pets Plus 宠物日用    78        0   \n",
      "640c28810000000013012bb8                 柴布丁日常   565        0   \n",
      "\n",
      "                                      Post Title          Caption  \\\n",
      "Post ID                                                             \n",
      "67a9ba5a000000001902e37f       喂了很久这几款狗粮，觉得效果很夸张  3 亿人的生活经验，都在小红书   \n",
      "67ac08b6000000001703ab73      不同品种中小型犬狗粮推荐 | 实惠篇  3 亿人的生活经验，都在小红书   \n",
      "673845cc000000001b011357      真心建议！幼犬粮都按这个标准挑选！！  3 亿人的生活经验，都在小红书   \n",
      "666fef14000000000e032cb5  🇲🇾鱼肉犬粮 | 不再头疼选狗粮，养狗人必看  3 亿人的生活经验，都在小红书   \n",
      "640c28810000000013012bb8       亲测 | 前🔟进口狗粮品牌测评推荐  3 亿人的生活经验，都在小红书   \n",
      "\n",
      "                         Date Published  \\\n",
      "Post ID                                   \n",
      "67a9ba5a000000001902e37f            N/A   \n",
      "67ac08b6000000001703ab73            N/A   \n",
      "673845cc000000001b011357            N/A   \n",
      "666fef14000000000e032cb5            N/A   \n",
      "640c28810000000013012bb8            N/A   \n",
      "\n",
      "                                                               Images  \\\n",
      "Post ID                                                                 \n",
      "67a9ba5a000000001902e37f  images_keyword/67a9ba5a000000001902e37f.jpg   \n",
      "67ac08b6000000001703ab73  images_keyword/67ac08b6000000001703ab73.jpg   \n",
      "673845cc000000001b011357  images_keyword/673845cc000000001b011357.jpg   \n",
      "666fef14000000000e032cb5  images_keyword/666fef14000000000e032cb5.jpg   \n",
      "640c28810000000013012bb8  images_keyword/640c28810000000013012bb8.jpg   \n",
      "\n",
      "                                                         Author Avatar Stars  \\\n",
      "Post ID                                                                        \n",
      "67a9ba5a000000001902e37f  avatars_keyword/67a9ba5a000000001902e37f.jpg    52   \n",
      "67ac08b6000000001703ab73  avatars_keyword/67ac08b6000000001703ab73.jpg   175   \n",
      "673845cc000000001b011357  avatars_keyword/673845cc000000001b011357.jpg    37   \n",
      "666fef14000000000e032cb5  avatars_keyword/666fef14000000000e032cb5.jpg  1.6万   \n",
      "640c28810000000013012bb8  avatars_keyword/640c28810000000013012bb8.jpg   957   \n",
      "\n",
      "                         Author Collect Nr Author Fans Nr Author Note Nr  \\\n",
      "Post ID                                                                    \n",
      "67a9ba5a000000001902e37f                 0              0              0   \n",
      "67ac08b6000000001703ab73                 0              0              0   \n",
      "673845cc000000001b011357                 0              0              0   \n",
      "666fef14000000000e032cb5                 0              0              0   \n",
      "640c28810000000013012bb8                 0              0              0   \n",
      "\n",
      "                         Video URL  \\\n",
      "Post ID                              \n",
      "67a9ba5a000000001902e37f       N/A   \n",
      "67ac08b6000000001703ab73       N/A   \n",
      "673845cc000000001b011357       N/A   \n",
      "666fef14000000000e032cb5       N/A   \n",
      "640c28810000000013012bb8       N/A   \n",
      "\n",
      "                                                                   User URL  \\\n",
      "Post ID                                                                       \n",
      "67a9ba5a000000001902e37f  /user/profile/590143e450c4b451db18264f?channel...   \n",
      "67ac08b6000000001703ab73  /user/profile/58a5e6ca82ec396b0862a0d8?channel...   \n",
      "673845cc000000001b011357  /user/profile/5b6268e4f64e0b00019326df?channel...   \n",
      "666fef14000000000e032cb5  /user/profile/6142b30c000000000202561e?channel...   \n",
      "640c28810000000013012bb8  /user/profile/6318db20000000002303d312?channel...   \n",
      "\n",
      "                                                                   Note URL  \n",
      "Post ID                                                                      \n",
      "67a9ba5a000000001902e37f  /search_result/67a9ba5a000000001902e37f?xsec_t...  \n",
      "67ac08b6000000001703ab73  /search_result/67ac08b6000000001703ab73?xsec_t...  \n",
      "673845cc000000001b011357  /search_result/673845cc000000001b011357?xsec_t...  \n",
      "666fef14000000000e032cb5  /search_result/666fef14000000000e032cb5?xsec_t...  \n",
      "640c28810000000013012bb8  /search_result/640c28810000000013012bb8?xsec_t...  \n",
      "数据已保存到 'scraped_xhs_keyword.csv'\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据存储列表\n",
    "authorName_list = []\n",
    "likeNr_list = []\n",
    "id_list = []\n",
    "noteURL_list = []\n",
    "userURL_list = []\n",
    "commentNr_list = []\n",
    "post_title_list = [] \n",
    "caption_list = []  \n",
    "datePublished_list = []\n",
    "images_list = []\n",
    "author_avatar_list = [] \n",
    "starNr_list = []\n",
    "authorCollectNr_list = []\n",
    "authorFansNr_list = []\n",
    "authorNoteNr_list = []\n",
    "video_urls = [] \n",
    "\n",
    "def parsePage(page_source):\n",
    "    \"\"\"\n",
    "    解析当前页面的HTML内容，提取笔记的基本信息并更新对应的列表。\n",
    "    \"\"\"\n",
    "    response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "    selector = Selector(response)\n",
    "    print(\"正在分析页面结构...\")\n",
    "\n",
    "    containers = ['//div[contains(@class, \"note-item\")]']\n",
    "    for container in containers:\n",
    "        elements = selector.xpath(container)\n",
    "        if elements:\n",
    "            print(f\"找到容器，xpath: {container}\")\n",
    "\n",
    "    content_elements = selector.xpath('//section[contains(@class, \"note-item\")]')\n",
    "    if content_elements:\n",
    "        print(f\"找到 {len(content_elements)} 个内容元素\")\n",
    "        for element in content_elements:\n",
    "            try:\n",
    "                # 提取URL\n",
    "                note_url = element.xpath('.//a[contains(@class, \"cover\")]/@href').get()\n",
    "                if note_url:\n",
    "                    note_id = note_url.split('/')[-1].split('?')[0]\n",
    "                    if note_id in id_list:\n",
    "                        continue  # 避免重复\n",
    "                    id_list.append(note_id)\n",
    "                    noteURL_list.append(note_url)\n",
    "\n",
    "                    # 提取作者名字\n",
    "                    author = element.xpath('.//div[contains(@class, \"author-wrapper\")]//span[contains(@class, \"name\")]/text()').get()\n",
    "                    authorName_list.append(author.strip() if author else \"N/A\")\n",
    "\n",
    "                    # 提取点赞数量\n",
    "                    likes = element.xpath('.//span[contains(@class, \"count\")]/text()').get()\n",
    "                    likeNr_list.append(likes.strip() if likes else \"0\")\n",
    "\n",
    "                    # 提取帖子标题\n",
    "                    post_title = element.xpath('.//a[contains(@class, \"title\")]//span/text()').getall()\n",
    "                    post_title_cleaned = ' '.join([c.strip() for c in post_title if c.strip()])\n",
    "                    post_title_list.append(post_title_cleaned if post_title_cleaned else \"N/A\")\n",
    "\n",
    "                    # 提取图片（主图）\n",
    "                    main_image = element.xpath('.//a[contains(@class, \"cover\")]/img/@src').get()\n",
    "                    images_list.append(main_image.strip() if main_image else \"N/A\")\n",
    "\n",
    "                    # 提取头像图片\n",
    "                    avatar_image = element.xpath('.//a[contains(@class, \"author\")]/img/@src').get()\n",
    "                    author_avatar_list.append(avatar_image.strip() if avatar_image else \"N/A\")\n",
    "\n",
    "                    # 提取用户URL\n",
    "                    user_url = element.xpath('.//a[contains(@class, \"author\")]/@href').get()# 用户URL\n",
    "                    userURL_list.append(user_url)\n",
    "\n",
    "                    qbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"处理元素时出错: {str(e)}\")\n",
    "                continue\n",
    "    print(f\"当前已爬取总数: {len(id_list)}\")\n",
    "\n",
    "def parseUserPage(user_url):\n",
    "    \"\"\"\n",
    "    访问用户页面，提取附加字段，包括作者获赞/收藏数量、粉丝数量和笔记数量。\n",
    "    同时初始化后续需要更新的 note-specific 字段：\n",
    "        commentNr_list, datePublished_list, starNr_list, video_urls, caption_list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_user_url = f'https://www.xiaohongshu.com{user_url}'\n",
    "        browser.get(full_user_url)\n",
    "\n",
    "        # 模拟滚动页面直到加载完成\n",
    "        while True:\n",
    "            previous_page_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")# 滚动到页面底部\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "\n",
    "            current_page_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if current_page_height == previous_page_height:\n",
    "                break\n",
    "            previous_page_height = current_page_height\n",
    "\n",
    "        WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"userPostedFeeds\"]//section')))# 等待页面加载完成\n",
    "        html = browser.page_source\n",
    "        selector = Selector(text=html)\n",
    "        \n",
    "        # 提取作者的获赞/收藏数量\n",
    "        author_collect_nr = selector.xpath('//*[@class=\"data-info\"]/div[1]/div[3]/span[@class=\"count\"]/text()').extract_first()# 作者获赞与收藏数量\n",
    "        author_collect_nr = author_collect_nr.strip() if author_collect_nr else \"0\"\n",
    "\n",
    "        # 提取作者的粉丝数量\n",
    "        author_fans_nr = selector.xpath('//*[@class=\"data-info\"]/div[1]/div[2]/span[@class=\"count\"]/text()').extract_first()# 作者粉丝数量\n",
    "        author_fans_nr = author_fans_nr.strip() if author_fans_nr else \"0\"\n",
    "\n",
    "        # 提取作者的笔记数量（基于页面中 note 项的数量）\n",
    "        author_note_nr = len(selector.xpath('//*[@id=\"userPostedFeeds\"]//section'))# 作者笔记数量\n",
    "        author_note_nr = str(author_note_nr)\n",
    "\n",
    "        # 更新全局列表\n",
    "        authorCollectNr_list.append(author_collect_nr)\n",
    "        authorFansNr_list.append(author_fans_nr)\n",
    "        authorNoteNr_list.append(author_note_nr)\n",
    "\n",
    "        # 初始化后续待更新的 note‑specific 字段\n",
    "        commentNr_list.append(\"0\")\n",
    "        datePublished_list.append(\"N/A\")\n",
    "        starNr_list.append(\"0\")\n",
    "        video_urls.append(\"N/A\")\n",
    "        caption_list.append(\"N/A\")\n",
    "        \n",
    "        print(\"已提取作者附加字段。\")\n",
    "    except Exception as e:\n",
    "        print(f\"提取作者附加字段时出错, 错误: {str(e)}\")\n",
    "\n",
    "def parseNotePage(note_url, note_id):\n",
    "    \"\"\"\n",
    "    访问每个笔记的页面，提取附加字段，包括评论数量、发布时间、收藏数量、粉丝数量、笔记数量、视频URL以及帖子内容（Caption）。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_note_url = f'https://www.xiaohongshu.com{note_url}'\n",
    "        browser.get(full_note_url)\n",
    "        \n",
    "        # 显式等待meta标签加载完成\n",
    "        wait = WebDriverWait(browser, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@name=\"description\"]')))\n",
    "        \n",
    "        page_source = browser.page_source\n",
    "        response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "        selector = Selector(response)\n",
    "\n",
    "        # 提取各字段\n",
    "        comments = selector.xpath('//*[@class=\"total\"]/text()').get()\n",
    "        comments = comments.strip() if comments else \"0\"\n",
    "\n",
    "        date_published = selector.xpath('//*[@class=\"date\"]/text()').get()\n",
    "        if not date_published:\n",
    "            date_published = selector.xpath('//time/@datetime').get()\n",
    "        date_published = date_published.strip() if date_published else \"N/A\"\n",
    "\n",
    "        stars = selector.xpath('//*[@class=\"count\"]/text()').get()\n",
    "        stars = stars.strip() if stars else \"0\"\n",
    "\n",
    "        collect_nr = selector.xpath('//span[contains(@class, \"collect\") or contains(@class, \"saved\")]/text()').get()\n",
    "        collect_nr = collect_nr.strip() if collect_nr else \"0\"\n",
    "\n",
    "        fans_nr = selector.xpath('//span[contains(@class, \"fans\") or contains(@class, \"followers\")]/text()').get()\n",
    "        fans_nr = fans_nr.strip() if fans_nr else \"0\"\n",
    "\n",
    "        note_nr = selector.xpath('//span[contains(@class, \"notes\") or contains(@class, \"posts\")]/text()').get()\n",
    "        note_nr = note_nr.strip() if note_nr else \"0\"\n",
    "\n",
    "        video_url = selector.xpath('//video/@src').get()\n",
    "        video_url = video_url.strip() if video_url else \"N/A\"\n",
    "\n",
    "        caption = selector.xpath('//*[@name=\"description\"]/@content').get()\n",
    "        caption = caption.strip() if caption else \"N/A\"\n",
    "\n",
    "        # 更新全局列表\n",
    "        if note_id in id_list:\n",
    "            index = id_list.index(note_id)\n",
    "            commentNr_list[index] = comments\n",
    "            datePublished_list[index] = date_published\n",
    "            starNr_list[index] = stars\n",
    "            authorCollectNr_list[index] = collect_nr\n",
    "            authorFansNr_list[index] = fans_nr\n",
    "            authorNoteNr_list[index] = note_nr\n",
    "            video_urls[index] = video_url\n",
    "            caption_list[index] = caption\n",
    "            print(f\"已提取附加字段，笔记ID: {note_id}\")\n",
    "        else:\n",
    "            print(f\"笔记ID {note_id} 未在id_list中找到。\")\n",
    "    except Exception as e:\n",
    "        print(f\"提取附加字段时出错，笔记ID: {note_id}, 错误: {str(e)}\")\n",
    "\n",
    "def ensure_search_results():\n",
    "    \"\"\"\n",
    "    确保已导航到搜索结果页面，并选择“图文”模式。\n",
    "    \"\"\"\n",
    "    current_url = browser.current_url\n",
    "    print(f\"当前URL: {current_url}\")\n",
    "\n",
    "    search_url = f'https://www.xiaohongshu.com/search_result?keyword={key_word}&source=web_explore_feed'\n",
    "    browser.get(search_url)\n",
    "\n",
    "    try:\n",
    "        wait = WebDriverWait(browser, 10)\n",
    "        wait.until(EC.title_contains(key_word))\n",
    "        print(\"已导航到搜索结果页面。\")\n",
    "    except:\n",
    "        print(\"导航到搜索结果页面时超时。\")\n",
    "        browser.quit()\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        selectors = [\n",
    "            \"//div[text()='图文']\",\n",
    "            \"//div[contains(@class, 'tab')]//span[text()='图文']\",\n",
    "            \"//div[contains(@class, 'filter')]//div[text()='图文']\",\n",
    "            \"//*[contains(text(), '图文')]\"\n",
    "        ]\n",
    "        for sel in selectors:\n",
    "            try:\n",
    "                element = WebDriverWait(browser, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, sel))\n",
    "                )\n",
    "                print(f\"找到元素，选择器: {sel}\")\n",
    "                element.click()\n",
    "                time.sleep(1)\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            print(\"未找到“图文”标签，可能页面结构已更改。\")\n",
    "    except Exception as e:\n",
    "        print(f\"切换视图时出错: {e}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"\n",
    "    下载图片并保存到指定路径。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"下载图片时出错，URL: {url}, 错误: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"\n",
    "    创建用于存储主图片和头像图片的目录。\n",
    "    \"\"\"\n",
    "    if not os.path.exists('images_keyword'):\n",
    "        os.makedirs('images_keyword')\n",
    "    if not os.path.exists('avatars_keyword'):\n",
    "        os.makedirs('avatars_keyword')\n",
    "\n",
    "# 创建图片保存目录\n",
    "create_directories()\n",
    "\n",
    "# 定义进度条：用于实时跟踪已爬取的笔记数量\n",
    "qbar = tqdm(total=num, desc=\"爬取进度\")\n",
    "\n",
    "ensure_search_results()\n",
    "\n",
    "while len(id_list) < num:\n",
    "    for _ in range(3):\n",
    "        browser.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(1)\n",
    "    parsePage(browser.page_source)\n",
    "    if '- THE END -' in browser.page_source or 'No more content' in browser.page_source:\n",
    "        print(f\"已到达内容末尾。总共收集: {len(id_list)} 条\")\n",
    "        break\n",
    "    time.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "print(f\"总共收集的条目数: {len(id_list)}\")\n",
    "if id_list:\n",
    "    print(\"收集的数据样本:\")\n",
    "    for i in range(min(5, len(id_list))):\n",
    "        print(f\"URL: {id_list[i]}\")\n",
    "\n",
    "if len(id_list) > num:\n",
    "    id_list = id_list[:num]\n",
    "    authorName_list = authorName_list[:num]\n",
    "    likeNr_list = likeNr_list[:num]\n",
    "    userURL_list = userURL_list[:num]\n",
    "    noteURL_list = noteURL_list[:num]  \n",
    "    commentNr_list = commentNr_list[:num]\n",
    "    post_title_list = post_title_list[:num]\n",
    "    datePublished_list = datePublished_list[:num]\n",
    "    images_list = images_list[:num]\n",
    "    author_avatar_list = author_avatar_list[:num]\n",
    "    starNr_list = starNr_list[:num]\n",
    "    authorCollectNr_list = authorCollectNr_list[:num]\n",
    "    authorFansNr_list = authorFansNr_list[:num]\n",
    "    authorNoteNr_list = authorNoteNr_list[:num]\n",
    "    video_urls = video_urls[:num]\n",
    "    caption_list = caption_list[:num]\n",
    "\n",
    "print(f\"截断后的总条目数: {len(id_list)}\")\n",
    "print(\"收集的数据样本:\")\n",
    "for i in range(min(5, len(id_list))):\n",
    "    print(f\"作者: {authorName_list[i]}, 点赞: {likeNr_list[i]}, URL: {id_list[i]}\")\n",
    "qbar.close()\n",
    "\n",
    "# 提取附加字段（帖子内容、日期发布、评论数量等）\n",
    "print(\"开始提取用户信息，包括收藏数量、粉丝数量和笔记数量...\")\n",
    "qbar = tqdm(total=len(userURL_list), desc=\"已提取的用户信息\")\n",
    "for user_url in userURL_list:\n",
    "    parseUserPage(user_url)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(0.5, 1))\n",
    "qbar.close()\n",
    "\n",
    "print(\"开始提取附加字段，包括帖子内容、日期发布和评论数量...\")\n",
    "qbar = tqdm(total=len(id_list), desc=\"已获取的笔记数量...\")\n",
    "for note_id, note_url in zip(id_list, noteURL_list):\n",
    "    parseNotePage(note_url, note_id)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(0.5, 1))\n",
    "qbar.close()\n",
    "\n",
    "# 下载图片\n",
    "print(\"开始下载主图片和头像图片...\")\n",
    "\n",
    "# 下载主图片\n",
    "image_download_bar = tqdm(total=len(images_list), desc=\"下载主图片\")\n",
    "for idx, image_url in enumerate(images_list):\n",
    "    if image_url == \"N/A\":\n",
    "        image_download_bar.update(1)\n",
    "        continue\n",
    "    image_extension = os.path.splitext(image_url)[1].split('?')[0]\n",
    "    if image_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        image_extension = '.jpg'\n",
    "    image_filename = f\"images_keyword/{id_list[idx]}{image_extension}\"\n",
    "    success = download_image(image_url, image_filename)\n",
    "    if not success:\n",
    "        image_filename = \"N/A\"\n",
    "    images_list[idx] = image_filename\n",
    "    image_download_bar.update(1)\n",
    "image_download_bar.close()\n",
    "\n",
    "# 下载头像图片\n",
    "avatar_download_bar = tqdm(total=len(author_avatar_list), desc=\"下载头像图片\")\n",
    "for idx, avatar_url in enumerate(author_avatar_list):\n",
    "    if avatar_url == \"N/A\":\n",
    "        avatar_download_bar.update(1)\n",
    "        continue\n",
    "    avatar_extension = os.path.splitext(avatar_url)[1].split('?')[0]\n",
    "    if avatar_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        avatar_extension = '.jpg'\n",
    "    avatar_filename = f\"avatars_keyword/{id_list[idx]}{avatar_extension}\"\n",
    "    success = download_image(avatar_url, avatar_filename)\n",
    "    if not success:\n",
    "        avatar_filename = \"N/A\"\n",
    "    author_avatar_list[idx] = avatar_filename\n",
    "    avatar_download_bar.update(1)\n",
    "avatar_download_bar.close()\n",
    "\n",
    "print(\"所有图片下载完成。\")\n",
    "\n",
    "# 创建数据字典\n",
    "data = {\n",
    "    'Author Name': authorName_list,\n",
    "    'Likes': likeNr_list,\n",
    "    'Comments': commentNr_list,\n",
    "    'Post Title': post_title_list, \n",
    "    'Caption': caption_list,\n",
    "    'Date Published': datePublished_list,\n",
    "    'Images': images_list,\n",
    "    'Author Avatar': author_avatar_list,\n",
    "    'Stars': starNr_list,\n",
    "    'Author Collect Nr': authorCollectNr_list,\n",
    "    'Author Fans Nr': authorFansNr_list,\n",
    "    'Author Note Nr': authorNoteNr_list,\n",
    "    'Video URL': video_urls,\n",
    "    'Post ID': id_list,\n",
    "    'User URL': userURL_list,\n",
    "    'Note URL': noteURL_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Post ID', inplace=True)\n",
    "print(df.head())\n",
    "df.to_csv('scraped_xhs_keyword.csv', encoding='utf-8-sig')\n",
    "print(\"数据已保存到 'scraped_xhs_keyword.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
