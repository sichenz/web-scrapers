{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨è¿è¡Œæœ¬ç¬”è®°æœ¬ä¹‹å‰ï¼Œè¯·å…ˆä¸‹è½½ä»¥ä¸‹å·¥å…·ï¼š\n",
    "\n",
    "1. **æŸ¥æ‰¾å½“å‰çš„ Google Chrome ç‰ˆæœ¬**  \n",
    "   - æ‰“å¼€ Google Chromeï¼Œå¹¶åœ¨åœ°å€æ è¾“å…¥ï¼š  \n",
    "     ```\n",
    "     chrome://settings/help\n",
    "     ```\n",
    "   - å»ºè®®ä½¿ç”¨ **133** ç‰ˆæœ¬çš„ Google Chromeï¼ˆæˆ–æœ€æ–°å¯ç”¨çš„ç¨³å®šç‰ˆæœ¬ï¼‰ã€‚\n",
    "\n",
    "2. **ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„ ChromeDriver**  \n",
    "   - å‰å¾€å®˜æ–¹çš„ Chrome æµ‹è¯•ä¸‹è½½é¡µé¢ï¼š  \n",
    "     [https://googlechromelabs.github.io/chrome-for-testing/#stable](https://googlechromelabs.github.io/chrome-for-testing/#stable)  \n",
    "   - ä¸‹è½½ä¸æ‚¨çš„ Chrome ç‰ˆæœ¬ç›¸åŒ¹é…çš„ ChromeDriverã€‚  \n",
    "   - ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨çš„ Chrome ç‰ˆæœ¬æ˜¯ 133ï¼Œè¯·ä¸‹è½½ **ChromeDriver 133**ã€‚\n",
    "\n",
    "3. **æŸ¥æ‰¾å·²ä¸‹è½½çš„ ChromeDriver**  \n",
    "   - å¦‚æœæ‚¨ä½¿ç”¨ macOSï¼Œå¯åœ¨ç»ˆç«¯ (Terminal) ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥æŸ¥æ‰¾ `chromedriver` çš„ä½ç½®ï¼š\n",
    "     ```bash\n",
    "     mdfind -name chromedriver\n",
    "     ```\n",
    "   - å¦‚æœä½¿ç”¨å…¶ä»–æ“ä½œç³»ç»Ÿï¼Œè¯·æ£€æŸ¥é»˜è®¤çš„ **ä¸‹è½½** ç›®å½•æˆ–æ‚¨ä¿å­˜è¯¥æ–‡ä»¶çš„ç›®å½•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åŒ…\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import requests  # æ–°å¢ï¼šç”¨äºä¸‹è½½å›¾ç‰‡\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### å¯åŠ¨ Chrome æµè§ˆå™¨å®ä¾‹ï¼š\n",
    "\n",
    "æ‰“å¼€ **terminal**, ä¸‹è½½ Chrome Driver (å‡çš„ Google Chrome)\n",
    "\n",
    "```bash\n",
    "brew install chromedriver\n",
    "chmod +x /opt/homebrew/bin/chromedriver\n",
    "```\n",
    "\n",
    "è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼ˆå°† `your Chrome.exe path` æ›¿æ¢ä¸ºæ‚¨çš„ Google Chrome æµè§ˆå™¨è·¯å¾„ï¼‰ï¼š\n",
    "```bash\n",
    "<your Chrome.exe path> --remote-debugging-port=9222 --user-data-dir=\"/Users/<your home folder name>/selenium/AutomationProfile\"\n",
    "```\n",
    "\n",
    "- è¯·å°†your Chrome.exe pathæ›¿æ¢ä¸ºæ‚¨çš„Chromeæµè§ˆå™¨æ‰€åœ¨è·¯å¾„ï¼Œä¾‹å¦‚<br>`C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe`\n",
    "- é…ç½® chromedriver ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š[ChromeDriver](https://developer.chrome.com/docs/chromedriver)\n",
    "- æ¥åšä¸ªæ¯”æ–¹ï¼Œ æˆ‘çš„ *terminal command* ä¼šæ˜¯:\n",
    "\n",
    "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
    "  --remote-debugging-port=9222 \\\n",
    "  --user-data-dir=\"/Users/princess/selenium/AutomationProfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®Chromeæµè§ˆå™¨\n",
    "options = Options()\n",
    "options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument(\"--headless\")\n",
    "browser = webdriver.Chrome(options=options)\n",
    "action = ActionChains(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å³å°†å¼€å§‹æ£€æŸ¥å°çº¢ä¹¦ç™»å½•çŠ¶æ€...\n",
      "çˆ¬å–æ•°æ®æœ‰è´¦æˆ·å°ç¦çš„é£é™©ï¼Œå»ºè®®ä½¿ç”¨éä¸»è´¦å·ç™»å½•ã€‚\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:34 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:35 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:36 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:37 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:38 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:39 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:40 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:41 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:42 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:43 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:44 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:45 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:46 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:47 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:48 2025\n",
      "æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:49 2025\n",
      "ç™»å½•æˆåŠŸ\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:57:50 2025\n",
      "è¯·åœ¨æ–‡æœ¬æ¡†ä¸­æ ¹æ®æç¤ºè¾“å…¥æœç´¢å…³é”®è¯å’Œç¬”è®°çˆ¬å–æ•°é‡ã€‚\n",
      "å³å°†å¼€å§‹æ£€æŸ¥ç½‘é¡µåŠ è½½çŠ¶æ€...\n",
      "å¦‚æœç½‘é¡µè¿›å…¥äººæœºéªŒè¯é¡µé¢ï¼Œè¯·å…ˆæ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚\n",
      "åŠ è½½æˆåŠŸ\n",
      "æ£€æŸ¥æ—¶é—´: Wed Feb 12 19:58:01 2025\n"
     ]
    }
   ],
   "source": [
    "key_word = \"\"\n",
    "num = 0\n",
    "\n",
    "def check_login_status(browser):\n",
    "    print(\"å³å°†å¼€å§‹æ£€æŸ¥å°çº¢ä¹¦ç™»å½•çŠ¶æ€...\")\n",
    "    print(\"çˆ¬å–æ•°æ®æœ‰è´¦æˆ·å°ç¦çš„é£é™©ï¼Œå»ºè®®ä½¿ç”¨éä¸»è´¦å·ç™»å½•ã€‚\")\n",
    "    \n",
    "    while True:\n",
    "        page_source = browser.page_source\n",
    "        if 'ç™»å½•æ¢ç´¢æ›´å¤šå†…å®¹' in page_source:\n",
    "            print('æš‚æœªç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•')\n",
    "            print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print('ç™»å½•æˆåŠŸ')\n",
    "            print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "            time.sleep(1)\n",
    "            break\n",
    "\n",
    "def check_page_load_status(browser, keyword):\n",
    "    print(\"å³å°†å¼€å§‹æ£€æŸ¥ç½‘é¡µåŠ è½½çŠ¶æ€...\")\n",
    "    print(\"å¦‚æœç½‘é¡µè¿›å…¥äººæœºéªŒè¯é¡µé¢ï¼Œè¯·å…ˆæ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚\")\n",
    "    \n",
    "    while True:\n",
    "        if keyword in browser.title:\n",
    "            print('åŠ è½½æˆåŠŸ')\n",
    "            print('æ£€æŸ¥æ—¶é—´:', time.ctime())\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "def selenium_test():\n",
    "    \"\"\"\n",
    "    ç™»å½•çŠ¶æ€æ£€æŸ¥ï¼Œç½‘é¡µåŠ è½½æ£€æŸ¥ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥è¿›è¡Œæœç´¢\n",
    "    \"\"\"\n",
    "    global key_word, num\n",
    "    browser.get('https://www.xiaohongshu.com/explore')\n",
    "    \n",
    "    check_login_status(browser)\n",
    "    \n",
    "    print(\"è¯·åœ¨æ–‡æœ¬æ¡†ä¸­æ ¹æ®æç¤ºè¾“å…¥æœç´¢å…³é”®è¯å’Œç¬”è®°çˆ¬å–æ•°é‡ã€‚\")\n",
    "    keyword = input(\"æœç´¢å…³é”®è¯ï¼š\")\n",
    "    key_word = keyword\n",
    "    \n",
    "    try:\n",
    "        num = int(input(\"ç¬”è®°çˆ¬å–æ•°é‡ï¼š\"))\n",
    "    except ValueError:\n",
    "        print(\"è¯·è¾“å…¥æœ‰æ•ˆçš„æ•´æ•°ä½œä¸ºçˆ¬å–æ•°é‡ã€‚\")\n",
    "        return\n",
    "    \n",
    "    url = f'https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_explore_feed'\n",
    "    browser.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    check_page_load_status(browser, keyword)\n",
    "\n",
    "selenium_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²è‡ªåŠ¨æ›´æ”¹æ¨¡å¼ä¸ºå›¾æ–‡ã€‚\n",
      "è¯·é€‰æ‹©æ’åºæ–¹å¼:\n",
      "1. ç»¼åˆ\n",
      "2. æœ€æ–°\n",
      "3. æœ€çƒ­\n",
      "è¯·è¾“å…¥æœ‰æ•ˆçš„æ’åºæ–¹å¼...\n"
     ]
    }
   ],
   "source": [
    "def change_mode(browser):\n",
    "    # æ›´æ”¹æ¨¡å¼ä¸ºå›¾æ–‡\n",
    "    try:\n",
    "        mode_button = browser.find_element(By.XPATH, '//*[@id=\"search-type\"]/div/div/div[2]')\n",
    "        mode_button.click()\n",
    "        print('å·²è‡ªåŠ¨æ›´æ”¹æ¨¡å¼ä¸ºå›¾æ–‡ã€‚')\n",
    "    except Exception as e:\n",
    "        print(f\"æ›´æ”¹æ¨¡å¼å¤±è´¥: {e}\")\n",
    "\n",
    "selected_order_text = ''\n",
    "def change_sort_order(browser, action):\n",
    "    # æ›´æ”¹æ’åºæ–¹å¼\n",
    "    sort_order = {\n",
    "        'ç»¼åˆ': 1,\n",
    "        'æœ€æ–°': 2,\n",
    "        'æœ€çƒ­': 3\n",
    "    }\n",
    "    print(\"è¯·é€‰æ‹©æ’åºæ–¹å¼:\")\n",
    "    for idx, order in sort_order.items():\n",
    "        print(f'{order}. {idx}')\n",
    "    \n",
    "    try:\n",
    "        global selected_order_text\n",
    "        selected_order_text = input(\"è¯·è¾“å…¥æ’åºæ–¹å¼å¯¹åº”çš„åç§°: \").strip()\n",
    "        if selected_order_text not in sort_order:\n",
    "            print(\"è¯·è¾“å…¥æœ‰æ•ˆçš„æ’åºæ–¹å¼...\")\n",
    "            return\n",
    "        \n",
    "        selected_order_index = sort_order[selected_order_text]\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†æ’åºé€‰æ‹©æ—¶å‡ºé”™: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        element = browser.find_element(By.XPATH, '//*[@id=\"global\"]/div[2]/div[2]/div/div[1]/div[2]')\n",
    "        action.move_to_element(element).perform()# æ¨¡æ‹Ÿé¼ æ ‡æ‚¬åœ\n",
    "        menu = browser.find_element(By.CLASS_NAME, 'dropdown-items')\n",
    "        option = menu.find_element(By.XPATH, f'/html/body/div[4]/div/li[{selected_order_index}]')\n",
    "        option.click()# æ¨¡æ‹Ÿé¼ æ ‡ç‚¹å‡»\n",
    "\n",
    "        print('å·²é€‰æ‹©æ’åºæ–¹å¼ä¸º:',selected_order_text)\n",
    "        print('æ£€æŸ¥æ—¶é—´:',time.ctime())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"æ›´æ”¹æ’åºæ–¹å¼å¤±è´¥: {e}\")\n",
    "\n",
    "change_mode(browser)\n",
    "change_sort_order(browser, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55ba8f19d0247cca1e799d95450e26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "çˆ¬å–è¿›åº¦:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰URL: https://www.xiaohongshu.com/search_result?keyword=%E7%8B%97%E7%B2%AE&source=web_explore_feed&type=51\n",
      "å·²å¯¼èˆªåˆ°æœç´¢ç»“æœé¡µé¢ã€‚\n",
      "æ‰¾åˆ°å…ƒç´ ï¼Œé€‰æ‹©å™¨: //*[contains(text(), 'å›¾æ–‡')]\n",
      "æ­£åœ¨åˆ†æé¡µé¢ç»“æ„...\n",
      "æ‰¾åˆ°å®¹å™¨ï¼Œxpath: //div[contains(@class, \"note-item\")]\n",
      "æ‰¾åˆ° 18 ä¸ªå†…å®¹å…ƒç´ \n",
      "å½“å‰å·²çˆ¬å–æ€»æ•°: 16\n",
      "æ€»å…±æ”¶é›†çš„æ¡ç›®æ•°: 16\n",
      "æ”¶é›†çš„æ•°æ®æ ·æœ¬:\n",
      "URL: 67a9ba5a000000001902e37f\n",
      "URL: 67ac08b6000000001703ab73\n",
      "URL: 673845cc000000001b011357\n",
      "URL: 666fef14000000000e032cb5\n",
      "URL: 640c28810000000013012bb8\n",
      "æˆªæ–­åçš„æ€»æ¡ç›®æ•°: 10\n",
      "æ”¶é›†çš„æ•°æ®æ ·æœ¬:\n",
      "ä½œè€…: ç¥èŠ±æ¤’, ç‚¹èµ: 6, URL: 67a9ba5a000000001902e37f\n",
      "ä½œè€…: è¿˜å¥½è€å­å¯çˆ±, ç‚¹èµ: 2, URL: 67ac08b6000000001703ab73\n",
      "ä½œè€…: ä¸å†·ç‹—ç‹—, ç‚¹èµ: 121, URL: 673845cc000000001b011357\n",
      "ä½œè€…: Hello Pets Plus å® ç‰©æ—¥ç”¨, ç‚¹èµ: 78, URL: 666fef14000000000e032cb5\n",
      "ä½œè€…: æŸ´å¸ƒä¸æ—¥å¸¸, ç‚¹èµ: 565, URL: 640c28810000000013012bb8\n",
      "å¼€å§‹æå–ç”¨æˆ·ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ”¶è—æ•°é‡ã€ç²‰ä¸æ•°é‡å’Œç¬”è®°æ•°é‡...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9497f03e9b4768a5254cacb92f4ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "å·²æå–çš„ç”¨æˆ·ä¿¡æ¯:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\n",
      "å¼€å§‹æå–é™„åŠ å­—æ®µï¼ŒåŒ…æ‹¬å¸–å­å†…å®¹ã€æ—¥æœŸå‘å¸ƒå’Œè¯„è®ºæ•°é‡...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9bb9ae79904fc09505c41322e85570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "å·²è·å–çš„ç¬”è®°æ•°é‡...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 67a9ba5a000000001902e37f\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 67ac08b6000000001703ab73\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 673845cc000000001b011357\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 666fef14000000000e032cb5\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 640c28810000000013012bb8\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 64982849000000001300ab83\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 66978f81000000000d00d491\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 667bcd97000000001f004b81\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 6662df6d000000001303ec3a\n",
      "å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: 66b9db5e0000000009017f85\n",
      "å¼€å§‹ä¸‹è½½ä¸»å›¾ç‰‡å’Œå¤´åƒå›¾ç‰‡...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897c4cae50ea41feb8be6d168584b062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ä¸‹è½½ä¸»å›¾ç‰‡:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4c5f73e2e44313ab8a6b0ee082e97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ä¸‹è½½å¤´åƒå›¾ç‰‡:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å®Œæˆã€‚\n",
      "                                   Author Name Likes Comments  \\\n",
      "Post ID                                                         \n",
      "67a9ba5a000000001902e37f                   ç¥èŠ±æ¤’     6        0   \n",
      "67ac08b6000000001703ab73                è¿˜å¥½è€å­å¯çˆ±     2        0   \n",
      "673845cc000000001b011357                  ä¸å†·ç‹—ç‹—   121        0   \n",
      "666fef14000000000e032cb5  Hello Pets Plus å® ç‰©æ—¥ç”¨    78        0   \n",
      "640c28810000000013012bb8                 æŸ´å¸ƒä¸æ—¥å¸¸   565        0   \n",
      "\n",
      "                                      Post Title          Caption  \\\n",
      "Post ID                                                             \n",
      "67a9ba5a000000001902e37f       å–‚äº†å¾ˆä¹…è¿™å‡ æ¬¾ç‹—ç²®ï¼Œè§‰å¾—æ•ˆæœå¾ˆå¤¸å¼   3 äº¿äººçš„ç”Ÿæ´»ç»éªŒï¼Œéƒ½åœ¨å°çº¢ä¹¦   \n",
      "67ac08b6000000001703ab73      ä¸åŒå“ç§ä¸­å°å‹çŠ¬ç‹—ç²®æ¨è | å®æƒ ç¯‡  3 äº¿äººçš„ç”Ÿæ´»ç»éªŒï¼Œéƒ½åœ¨å°çº¢ä¹¦   \n",
      "673845cc000000001b011357      çœŸå¿ƒå»ºè®®ï¼å¹¼çŠ¬ç²®éƒ½æŒ‰è¿™ä¸ªæ ‡å‡†æŒ‘é€‰ï¼ï¼  3 äº¿äººçš„ç”Ÿæ´»ç»éªŒï¼Œéƒ½åœ¨å°çº¢ä¹¦   \n",
      "666fef14000000000e032cb5  ğŸ‡²ğŸ‡¾é±¼è‚‰çŠ¬ç²® | ä¸å†å¤´ç–¼é€‰ç‹—ç²®ï¼Œå…»ç‹—äººå¿…çœ‹  3 äº¿äººçš„ç”Ÿæ´»ç»éªŒï¼Œéƒ½åœ¨å°çº¢ä¹¦   \n",
      "640c28810000000013012bb8       äº²æµ‹ | å‰ğŸ”Ÿè¿›å£ç‹—ç²®å“ç‰Œæµ‹è¯„æ¨è  3 äº¿äººçš„ç”Ÿæ´»ç»éªŒï¼Œéƒ½åœ¨å°çº¢ä¹¦   \n",
      "\n",
      "                         Date Published  \\\n",
      "Post ID                                   \n",
      "67a9ba5a000000001902e37f            N/A   \n",
      "67ac08b6000000001703ab73            N/A   \n",
      "673845cc000000001b011357            N/A   \n",
      "666fef14000000000e032cb5            N/A   \n",
      "640c28810000000013012bb8            N/A   \n",
      "\n",
      "                                                               Images  \\\n",
      "Post ID                                                                 \n",
      "67a9ba5a000000001902e37f  images_keyword/67a9ba5a000000001902e37f.jpg   \n",
      "67ac08b6000000001703ab73  images_keyword/67ac08b6000000001703ab73.jpg   \n",
      "673845cc000000001b011357  images_keyword/673845cc000000001b011357.jpg   \n",
      "666fef14000000000e032cb5  images_keyword/666fef14000000000e032cb5.jpg   \n",
      "640c28810000000013012bb8  images_keyword/640c28810000000013012bb8.jpg   \n",
      "\n",
      "                                                         Author Avatar Stars  \\\n",
      "Post ID                                                                        \n",
      "67a9ba5a000000001902e37f  avatars_keyword/67a9ba5a000000001902e37f.jpg    52   \n",
      "67ac08b6000000001703ab73  avatars_keyword/67ac08b6000000001703ab73.jpg   175   \n",
      "673845cc000000001b011357  avatars_keyword/673845cc000000001b011357.jpg    37   \n",
      "666fef14000000000e032cb5  avatars_keyword/666fef14000000000e032cb5.jpg  1.6ä¸‡   \n",
      "640c28810000000013012bb8  avatars_keyword/640c28810000000013012bb8.jpg   957   \n",
      "\n",
      "                         Author Collect Nr Author Fans Nr Author Note Nr  \\\n",
      "Post ID                                                                    \n",
      "67a9ba5a000000001902e37f                 0              0              0   \n",
      "67ac08b6000000001703ab73                 0              0              0   \n",
      "673845cc000000001b011357                 0              0              0   \n",
      "666fef14000000000e032cb5                 0              0              0   \n",
      "640c28810000000013012bb8                 0              0              0   \n",
      "\n",
      "                         Video URL  \\\n",
      "Post ID                              \n",
      "67a9ba5a000000001902e37f       N/A   \n",
      "67ac08b6000000001703ab73       N/A   \n",
      "673845cc000000001b011357       N/A   \n",
      "666fef14000000000e032cb5       N/A   \n",
      "640c28810000000013012bb8       N/A   \n",
      "\n",
      "                                                                   User URL  \\\n",
      "Post ID                                                                       \n",
      "67a9ba5a000000001902e37f  /user/profile/590143e450c4b451db18264f?channel...   \n",
      "67ac08b6000000001703ab73  /user/profile/58a5e6ca82ec396b0862a0d8?channel...   \n",
      "673845cc000000001b011357  /user/profile/5b6268e4f64e0b00019326df?channel...   \n",
      "666fef14000000000e032cb5  /user/profile/6142b30c000000000202561e?channel...   \n",
      "640c28810000000013012bb8  /user/profile/6318db20000000002303d312?channel...   \n",
      "\n",
      "                                                                   Note URL  \n",
      "Post ID                                                                      \n",
      "67a9ba5a000000001902e37f  /search_result/67a9ba5a000000001902e37f?xsec_t...  \n",
      "67ac08b6000000001703ab73  /search_result/67ac08b6000000001703ab73?xsec_t...  \n",
      "673845cc000000001b011357  /search_result/673845cc000000001b011357?xsec_t...  \n",
      "666fef14000000000e032cb5  /search_result/666fef14000000000e032cb5?xsec_t...  \n",
      "640c28810000000013012bb8  /search_result/640c28810000000013012bb8?xsec_t...  \n",
      "æ•°æ®å·²ä¿å­˜åˆ° 'scraped_xhs_keyword.csv'\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–æ•°æ®å­˜å‚¨åˆ—è¡¨\n",
    "authorName_list = []\n",
    "likeNr_list = []\n",
    "id_list = []\n",
    "noteURL_list = []\n",
    "userURL_list = []\n",
    "commentNr_list = []\n",
    "post_title_list = [] \n",
    "caption_list = []  \n",
    "datePublished_list = []\n",
    "images_list = []\n",
    "author_avatar_list = [] \n",
    "starNr_list = []\n",
    "authorCollectNr_list = []\n",
    "authorFansNr_list = []\n",
    "authorNoteNr_list = []\n",
    "video_urls = [] \n",
    "\n",
    "def parsePage(page_source):\n",
    "    \"\"\"\n",
    "    è§£æå½“å‰é¡µé¢çš„HTMLå†…å®¹ï¼Œæå–ç¬”è®°çš„åŸºæœ¬ä¿¡æ¯å¹¶æ›´æ–°å¯¹åº”çš„åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "    selector = Selector(response)\n",
    "    print(\"æ­£åœ¨åˆ†æé¡µé¢ç»“æ„...\")\n",
    "\n",
    "    containers = ['//div[contains(@class, \"note-item\")]']\n",
    "    for container in containers:\n",
    "        elements = selector.xpath(container)\n",
    "        if elements:\n",
    "            print(f\"æ‰¾åˆ°å®¹å™¨ï¼Œxpath: {container}\")\n",
    "\n",
    "    content_elements = selector.xpath('//section[contains(@class, \"note-item\")]')\n",
    "    if content_elements:\n",
    "        print(f\"æ‰¾åˆ° {len(content_elements)} ä¸ªå†…å®¹å…ƒç´ \")\n",
    "        for element in content_elements:\n",
    "            try:\n",
    "                # æå–URL\n",
    "                note_url = element.xpath('.//a[contains(@class, \"cover\")]/@href').get()\n",
    "                if note_url:\n",
    "                    note_id = note_url.split('/')[-1].split('?')[0]\n",
    "                    if note_id in id_list:\n",
    "                        continue  # é¿å…é‡å¤\n",
    "                    id_list.append(note_id)\n",
    "                    noteURL_list.append(note_url)\n",
    "\n",
    "                    # æå–ä½œè€…åå­—\n",
    "                    author = element.xpath('.//div[contains(@class, \"author-wrapper\")]//span[contains(@class, \"name\")]/text()').get()\n",
    "                    authorName_list.append(author.strip() if author else \"N/A\")\n",
    "\n",
    "                    # æå–ç‚¹èµæ•°é‡\n",
    "                    likes = element.xpath('.//span[contains(@class, \"count\")]/text()').get()\n",
    "                    likeNr_list.append(likes.strip() if likes else \"0\")\n",
    "\n",
    "                    # æå–å¸–å­æ ‡é¢˜\n",
    "                    post_title = element.xpath('.//a[contains(@class, \"title\")]//span/text()').getall()\n",
    "                    post_title_cleaned = ' '.join([c.strip() for c in post_title if c.strip()])\n",
    "                    post_title_list.append(post_title_cleaned if post_title_cleaned else \"N/A\")\n",
    "\n",
    "                    # æå–å›¾ç‰‡ï¼ˆä¸»å›¾ï¼‰\n",
    "                    main_image = element.xpath('.//a[contains(@class, \"cover\")]/img/@src').get()\n",
    "                    images_list.append(main_image.strip() if main_image else \"N/A\")\n",
    "\n",
    "                    # æå–å¤´åƒå›¾ç‰‡\n",
    "                    avatar_image = element.xpath('.//a[contains(@class, \"author\")]/img/@src').get()\n",
    "                    author_avatar_list.append(avatar_image.strip() if avatar_image else \"N/A\")\n",
    "\n",
    "                    # æå–ç”¨æˆ·URL\n",
    "                    user_url = element.xpath('.//a[contains(@class, \"author\")]/@href').get()# ç”¨æˆ·URL\n",
    "                    userURL_list.append(user_url)\n",
    "\n",
    "                    qbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"å¤„ç†å…ƒç´ æ—¶å‡ºé”™: {str(e)}\")\n",
    "                continue\n",
    "    print(f\"å½“å‰å·²çˆ¬å–æ€»æ•°: {len(id_list)}\")\n",
    "\n",
    "def parseUserPage(user_url):\n",
    "    \"\"\"\n",
    "    è®¿é—®ç”¨æˆ·é¡µé¢ï¼Œæå–é™„åŠ å­—æ®µï¼ŒåŒ…æ‹¬ä½œè€…è·èµ/æ”¶è—æ•°é‡ã€ç²‰ä¸æ•°é‡å’Œç¬”è®°æ•°é‡ã€‚\n",
    "    åŒæ—¶åˆå§‹åŒ–åç»­éœ€è¦æ›´æ–°çš„ note-specific å­—æ®µï¼š\n",
    "        commentNr_list, datePublished_list, starNr_list, video_urls, caption_list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_user_url = f'https://www.xiaohongshu.com{user_url}'\n",
    "        browser.get(full_user_url)\n",
    "\n",
    "        # æ¨¡æ‹Ÿæ»šåŠ¨é¡µé¢ç›´åˆ°åŠ è½½å®Œæˆ\n",
    "        while True:\n",
    "            previous_page_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")# æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "\n",
    "            current_page_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if current_page_height == previous_page_height:\n",
    "                break\n",
    "            previous_page_height = current_page_height\n",
    "\n",
    "        WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"userPostedFeeds\"]//section')))# ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ\n",
    "        html = browser.page_source\n",
    "        selector = Selector(text=html)\n",
    "        \n",
    "        # æå–ä½œè€…çš„è·èµ/æ”¶è—æ•°é‡\n",
    "        author_collect_nr = selector.xpath('//*[@class=\"data-info\"]/div[1]/div[3]/span[@class=\"count\"]/text()').extract_first()# ä½œè€…è·èµä¸æ”¶è—æ•°é‡\n",
    "        author_collect_nr = author_collect_nr.strip() if author_collect_nr else \"0\"\n",
    "\n",
    "        # æå–ä½œè€…çš„ç²‰ä¸æ•°é‡\n",
    "        author_fans_nr = selector.xpath('//*[@class=\"data-info\"]/div[1]/div[2]/span[@class=\"count\"]/text()').extract_first()# ä½œè€…ç²‰ä¸æ•°é‡\n",
    "        author_fans_nr = author_fans_nr.strip() if author_fans_nr else \"0\"\n",
    "\n",
    "        # æå–ä½œè€…çš„ç¬”è®°æ•°é‡ï¼ˆåŸºäºé¡µé¢ä¸­ note é¡¹çš„æ•°é‡ï¼‰\n",
    "        author_note_nr = len(selector.xpath('//*[@id=\"userPostedFeeds\"]//section'))# ä½œè€…ç¬”è®°æ•°é‡\n",
    "        author_note_nr = str(author_note_nr)\n",
    "\n",
    "        # æ›´æ–°å…¨å±€åˆ—è¡¨\n",
    "        authorCollectNr_list.append(author_collect_nr)\n",
    "        authorFansNr_list.append(author_fans_nr)\n",
    "        authorNoteNr_list.append(author_note_nr)\n",
    "\n",
    "        # åˆå§‹åŒ–åç»­å¾…æ›´æ–°çš„ noteâ€‘specific å­—æ®µ\n",
    "        commentNr_list.append(\"0\")\n",
    "        datePublished_list.append(\"N/A\")\n",
    "        starNr_list.append(\"0\")\n",
    "        video_urls.append(\"N/A\")\n",
    "        caption_list.append(\"N/A\")\n",
    "        \n",
    "        print(\"å·²æå–ä½œè€…é™„åŠ å­—æ®µã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"æå–ä½œè€…é™„åŠ å­—æ®µæ—¶å‡ºé”™, é”™è¯¯: {str(e)}\")\n",
    "\n",
    "def parseNotePage(note_url, note_id):\n",
    "    \"\"\"\n",
    "    è®¿é—®æ¯ä¸ªç¬”è®°çš„é¡µé¢ï¼Œæå–é™„åŠ å­—æ®µï¼ŒåŒ…æ‹¬è¯„è®ºæ•°é‡ã€å‘å¸ƒæ—¶é—´ã€æ”¶è—æ•°é‡ã€ç²‰ä¸æ•°é‡ã€ç¬”è®°æ•°é‡ã€è§†é¢‘URLä»¥åŠå¸–å­å†…å®¹ï¼ˆCaptionï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_note_url = f'https://www.xiaohongshu.com{note_url}'\n",
    "        browser.get(full_note_url)\n",
    "        \n",
    "        # æ˜¾å¼ç­‰å¾…metaæ ‡ç­¾åŠ è½½å®Œæˆ\n",
    "        wait = WebDriverWait(browser, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@name=\"description\"]')))\n",
    "        \n",
    "        page_source = browser.page_source\n",
    "        response = TextResponse(url=browser.current_url, body=page_source.encode('utf-8'), encoding='utf-8')\n",
    "        selector = Selector(response)\n",
    "\n",
    "        # æå–å„å­—æ®µ\n",
    "        comments = selector.xpath('//*[@class=\"total\"]/text()').get()\n",
    "        comments = comments.strip() if comments else \"0\"\n",
    "\n",
    "        date_published = selector.xpath('//*[@class=\"date\"]/text()').get()\n",
    "        if not date_published:\n",
    "            date_published = selector.xpath('//time/@datetime').get()\n",
    "        date_published = date_published.strip() if date_published else \"N/A\"\n",
    "\n",
    "        stars = selector.xpath('//*[@class=\"count\"]/text()').get()\n",
    "        stars = stars.strip() if stars else \"0\"\n",
    "\n",
    "        collect_nr = selector.xpath('//span[contains(@class, \"collect\") or contains(@class, \"saved\")]/text()').get()\n",
    "        collect_nr = collect_nr.strip() if collect_nr else \"0\"\n",
    "\n",
    "        fans_nr = selector.xpath('//span[contains(@class, \"fans\") or contains(@class, \"followers\")]/text()').get()\n",
    "        fans_nr = fans_nr.strip() if fans_nr else \"0\"\n",
    "\n",
    "        note_nr = selector.xpath('//span[contains(@class, \"notes\") or contains(@class, \"posts\")]/text()').get()\n",
    "        note_nr = note_nr.strip() if note_nr else \"0\"\n",
    "\n",
    "        video_url = selector.xpath('//video/@src').get()\n",
    "        video_url = video_url.strip() if video_url else \"N/A\"\n",
    "\n",
    "        caption = selector.xpath('//*[@name=\"description\"]/@content').get()\n",
    "        caption = caption.strip() if caption else \"N/A\"\n",
    "\n",
    "        # æ›´æ–°å…¨å±€åˆ—è¡¨\n",
    "        if note_id in id_list:\n",
    "            index = id_list.index(note_id)\n",
    "            commentNr_list[index] = comments\n",
    "            datePublished_list[index] = date_published\n",
    "            starNr_list[index] = stars\n",
    "            authorCollectNr_list[index] = collect_nr\n",
    "            authorFansNr_list[index] = fans_nr\n",
    "            authorNoteNr_list[index] = note_nr\n",
    "            video_urls[index] = video_url\n",
    "            caption_list[index] = caption\n",
    "            print(f\"å·²æå–é™„åŠ å­—æ®µï¼Œç¬”è®°ID: {note_id}\")\n",
    "        else:\n",
    "            print(f\"ç¬”è®°ID {note_id} æœªåœ¨id_listä¸­æ‰¾åˆ°ã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"æå–é™„åŠ å­—æ®µæ—¶å‡ºé”™ï¼Œç¬”è®°ID: {note_id}, é”™è¯¯: {str(e)}\")\n",
    "\n",
    "def ensure_search_results():\n",
    "    \"\"\"\n",
    "    ç¡®ä¿å·²å¯¼èˆªåˆ°æœç´¢ç»“æœé¡µé¢ï¼Œå¹¶é€‰æ‹©â€œå›¾æ–‡â€æ¨¡å¼ã€‚\n",
    "    \"\"\"\n",
    "    current_url = browser.current_url\n",
    "    print(f\"å½“å‰URL: {current_url}\")\n",
    "\n",
    "    search_url = f'https://www.xiaohongshu.com/search_result?keyword={key_word}&source=web_explore_feed'\n",
    "    browser.get(search_url)\n",
    "\n",
    "    try:\n",
    "        wait = WebDriverWait(browser, 10)\n",
    "        wait.until(EC.title_contains(key_word))\n",
    "        print(\"å·²å¯¼èˆªåˆ°æœç´¢ç»“æœé¡µé¢ã€‚\")\n",
    "    except:\n",
    "        print(\"å¯¼èˆªåˆ°æœç´¢ç»“æœé¡µé¢æ—¶è¶…æ—¶ã€‚\")\n",
    "        browser.quit()\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        selectors = [\n",
    "            \"//div[text()='å›¾æ–‡']\",\n",
    "            \"//div[contains(@class, 'tab')]//span[text()='å›¾æ–‡']\",\n",
    "            \"//div[contains(@class, 'filter')]//div[text()='å›¾æ–‡']\",\n",
    "            \"//*[contains(text(), 'å›¾æ–‡')]\"\n",
    "        ]\n",
    "        for sel in selectors:\n",
    "            try:\n",
    "                element = WebDriverWait(browser, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, sel))\n",
    "                )\n",
    "                print(f\"æ‰¾åˆ°å…ƒç´ ï¼Œé€‰æ‹©å™¨: {sel}\")\n",
    "                element.click()\n",
    "                time.sleep(1)\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            print(\"æœªæ‰¾åˆ°â€œå›¾æ–‡â€æ ‡ç­¾ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²æ›´æ”¹ã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"åˆ‡æ¢è§†å›¾æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"\n",
    "    ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ä¸‹è½½å›¾ç‰‡æ—¶å‡ºé”™ï¼ŒURL: {url}, é”™è¯¯: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"\n",
    "    åˆ›å»ºç”¨äºå­˜å‚¨ä¸»å›¾ç‰‡å’Œå¤´åƒå›¾ç‰‡çš„ç›®å½•ã€‚\n",
    "    \"\"\"\n",
    "    if not os.path.exists('images_keyword'):\n",
    "        os.makedirs('images_keyword')\n",
    "    if not os.path.exists('avatars_keyword'):\n",
    "        os.makedirs('avatars_keyword')\n",
    "\n",
    "# åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•\n",
    "create_directories()\n",
    "\n",
    "# å®šä¹‰è¿›åº¦æ¡ï¼šç”¨äºå®æ—¶è·Ÿè¸ªå·²çˆ¬å–çš„ç¬”è®°æ•°é‡\n",
    "qbar = tqdm(total=num, desc=\"çˆ¬å–è¿›åº¦\")\n",
    "\n",
    "ensure_search_results()\n",
    "\n",
    "while len(id_list) < num:\n",
    "    for _ in range(3):\n",
    "        browser.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(1)\n",
    "    parsePage(browser.page_source)\n",
    "    if '- THE END -' in browser.page_source or 'No more content' in browser.page_source:\n",
    "        print(f\"å·²åˆ°è¾¾å†…å®¹æœ«å°¾ã€‚æ€»å…±æ”¶é›†: {len(id_list)} æ¡\")\n",
    "        break\n",
    "    time.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "print(f\"æ€»å…±æ”¶é›†çš„æ¡ç›®æ•°: {len(id_list)}\")\n",
    "if id_list:\n",
    "    print(\"æ”¶é›†çš„æ•°æ®æ ·æœ¬:\")\n",
    "    for i in range(min(5, len(id_list))):\n",
    "        print(f\"URL: {id_list[i]}\")\n",
    "\n",
    "if len(id_list) > num:\n",
    "    id_list = id_list[:num]\n",
    "    authorName_list = authorName_list[:num]\n",
    "    likeNr_list = likeNr_list[:num]\n",
    "    userURL_list = userURL_list[:num]\n",
    "    noteURL_list = noteURL_list[:num]  \n",
    "    commentNr_list = commentNr_list[:num]\n",
    "    post_title_list = post_title_list[:num]\n",
    "    datePublished_list = datePublished_list[:num]\n",
    "    images_list = images_list[:num]\n",
    "    author_avatar_list = author_avatar_list[:num]\n",
    "    starNr_list = starNr_list[:num]\n",
    "    authorCollectNr_list = authorCollectNr_list[:num]\n",
    "    authorFansNr_list = authorFansNr_list[:num]\n",
    "    authorNoteNr_list = authorNoteNr_list[:num]\n",
    "    video_urls = video_urls[:num]\n",
    "    caption_list = caption_list[:num]\n",
    "\n",
    "print(f\"æˆªæ–­åçš„æ€»æ¡ç›®æ•°: {len(id_list)}\")\n",
    "print(\"æ”¶é›†çš„æ•°æ®æ ·æœ¬:\")\n",
    "for i in range(min(5, len(id_list))):\n",
    "    print(f\"ä½œè€…: {authorName_list[i]}, ç‚¹èµ: {likeNr_list[i]}, URL: {id_list[i]}\")\n",
    "qbar.close()\n",
    "\n",
    "# æå–é™„åŠ å­—æ®µï¼ˆå¸–å­å†…å®¹ã€æ—¥æœŸå‘å¸ƒã€è¯„è®ºæ•°é‡ç­‰ï¼‰\n",
    "print(\"å¼€å§‹æå–ç”¨æˆ·ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ”¶è—æ•°é‡ã€ç²‰ä¸æ•°é‡å’Œç¬”è®°æ•°é‡...\")\n",
    "qbar = tqdm(total=len(userURL_list), desc=\"å·²æå–çš„ç”¨æˆ·ä¿¡æ¯\")\n",
    "for user_url in userURL_list:\n",
    "    parseUserPage(user_url)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(0.5, 1))\n",
    "qbar.close()\n",
    "\n",
    "print(\"å¼€å§‹æå–é™„åŠ å­—æ®µï¼ŒåŒ…æ‹¬å¸–å­å†…å®¹ã€æ—¥æœŸå‘å¸ƒå’Œè¯„è®ºæ•°é‡...\")\n",
    "qbar = tqdm(total=len(id_list), desc=\"å·²è·å–çš„ç¬”è®°æ•°é‡...\")\n",
    "for note_id, note_url in zip(id_list, noteURL_list):\n",
    "    parseNotePage(note_url, note_id)\n",
    "    qbar.update(1)\n",
    "    time.sleep(random.uniform(0.5, 1))\n",
    "qbar.close()\n",
    "\n",
    "# ä¸‹è½½å›¾ç‰‡\n",
    "print(\"å¼€å§‹ä¸‹è½½ä¸»å›¾ç‰‡å’Œå¤´åƒå›¾ç‰‡...\")\n",
    "\n",
    "# ä¸‹è½½ä¸»å›¾ç‰‡\n",
    "image_download_bar = tqdm(total=len(images_list), desc=\"ä¸‹è½½ä¸»å›¾ç‰‡\")\n",
    "for idx, image_url in enumerate(images_list):\n",
    "    if image_url == \"N/A\":\n",
    "        image_download_bar.update(1)\n",
    "        continue\n",
    "    image_extension = os.path.splitext(image_url)[1].split('?')[0]\n",
    "    if image_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        image_extension = '.jpg'\n",
    "    image_filename = f\"images_keyword/{id_list[idx]}{image_extension}\"\n",
    "    success = download_image(image_url, image_filename)\n",
    "    if not success:\n",
    "        image_filename = \"N/A\"\n",
    "    images_list[idx] = image_filename\n",
    "    image_download_bar.update(1)\n",
    "image_download_bar.close()\n",
    "\n",
    "# ä¸‹è½½å¤´åƒå›¾ç‰‡\n",
    "avatar_download_bar = tqdm(total=len(author_avatar_list), desc=\"ä¸‹è½½å¤´åƒå›¾ç‰‡\")\n",
    "for idx, avatar_url in enumerate(author_avatar_list):\n",
    "    if avatar_url == \"N/A\":\n",
    "        avatar_download_bar.update(1)\n",
    "        continue\n",
    "    avatar_extension = os.path.splitext(avatar_url)[1].split('?')[0]\n",
    "    if avatar_extension.lower() not in ['.jpg', '.jpeg', '.png', '.gif']:\n",
    "        avatar_extension = '.jpg'\n",
    "    avatar_filename = f\"avatars_keyword/{id_list[idx]}{avatar_extension}\"\n",
    "    success = download_image(avatar_url, avatar_filename)\n",
    "    if not success:\n",
    "        avatar_filename = \"N/A\"\n",
    "    author_avatar_list[idx] = avatar_filename\n",
    "    avatar_download_bar.update(1)\n",
    "avatar_download_bar.close()\n",
    "\n",
    "print(\"æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å®Œæˆã€‚\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®å­—å…¸\n",
    "data = {\n",
    "    'Author Name': authorName_list,\n",
    "    'Likes': likeNr_list,\n",
    "    'Comments': commentNr_list,\n",
    "    'Post Title': post_title_list, \n",
    "    'Caption': caption_list,\n",
    "    'Date Published': datePublished_list,\n",
    "    'Images': images_list,\n",
    "    'Author Avatar': author_avatar_list,\n",
    "    'Stars': starNr_list,\n",
    "    'Author Collect Nr': authorCollectNr_list,\n",
    "    'Author Fans Nr': authorFansNr_list,\n",
    "    'Author Note Nr': authorNoteNr_list,\n",
    "    'Video URL': video_urls,\n",
    "    'Post ID': id_list,\n",
    "    'User URL': userURL_list,\n",
    "    'Note URL': noteURL_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Post ID', inplace=True)\n",
    "print(df.head())\n",
    "df.to_csv('scraped_xhs_keyword.csv', encoding='utf-8-sig')\n",
    "print(\"æ•°æ®å·²ä¿å­˜åˆ° 'scraped_xhs_keyword.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
