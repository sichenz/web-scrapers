{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaa2f57241c46858d12d500405f1dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=10, description='Number of Posts:', max=1000, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f2345368304c3f80f98a607652a2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='https://www.linkedin.com/school/nyushanghai/posts/?feedView=all', description='Profile URL:', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c0d892d505463ca9e86f8cd2abf468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Start Scraping', icon='check', style=ButtonStyle(), tooltip='Click…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb3589b4b454ac5b3431e0637947371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from seleniumwire import webdriver  \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import csv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import requests  \n",
    "from urllib.parse import urlparse\n",
    "import hashlib  \n",
    "import json  \n",
    "import datetime  \n",
    "import subprocess\n",
    "\n",
    "# Function to load cookies from a Netscape format cookies.txt file\n",
    "def load_cookies(browser, file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.startswith('#') and line.strip():\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) == 7:\n",
    "                    cookie = {\n",
    "                        'domain': fields[0],\n",
    "                        'flag': fields[1],\n",
    "                        'path': fields[2],\n",
    "                        'secure': fields[3],\n",
    "                        'expiration': fields[4],\n",
    "                        'name': fields[5],\n",
    "                        'value': fields[6]\n",
    "                    }\n",
    "                    # Adjust domain if it starts with a dot\n",
    "                    domain = cookie['domain'].lstrip('.')\n",
    "                    browser.add_cookie({\n",
    "                        'name': cookie['name'],\n",
    "                        'value': cookie['value'],\n",
    "                        'domain': domain,\n",
    "                        'path': cookie['path'],\n",
    "                        'expiry': int(cookie['expiration']) if cookie['expiration'] else None\n",
    "                    })\n",
    "\n",
    "# Function to get cookies from Selenium Wire browser and convert them to requests-compatible format\n",
    "def get_requests_cookies(browser):\n",
    "    selenium_cookies = browser.get_cookies()\n",
    "    requests_cookies = {}\n",
    "    for cookie in selenium_cookies:\n",
    "        requests_cookies[cookie['name']] = cookie['value']\n",
    "    return requests_cookies\n",
    "\n",
    "# Function to extract images with \"media\" in their src, excluding those with 'company-logo'\n",
    "def extract_images(post_content_container):\n",
    "    post_images = []\n",
    "    try:\n",
    "        post_media_container = post_content_container.find_next(\"div\", {\"class\": \"feed-shared-actor__container\"}) if post_content_container else None\n",
    "        if post_media_container:\n",
    "            # Extract images\n",
    "            images = post_media_container.find_all(\"img\", {\"class\": \"feed-shared-image__image\"})\n",
    "            for img in images:\n",
    "                img_url = img.get('src') or img.get('data-delayed-url')\n",
    "                if img_url and 'media' in img_url.lower() and 'company-logo' not in img_url.lower():\n",
    "                    post_images.append(img_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting media images: {e}\")\n",
    "    \n",
    "    # Alternative extraction \n",
    "    if not post_images:\n",
    "        try:\n",
    "            post_container = post_content_container.find_parent(\"div\", {\"class\": \"occludable-update\"}) if post_content_container else None\n",
    "            if post_container:\n",
    "                images = post_container.find_all(\"img\")\n",
    "                for img in images:\n",
    "                    img_url = img.get('src') or img.get('data-delayed-url')\n",
    "                    if img_url and 'media' in img_url.lower() and 'company-logo' not in img_url.lower() and 'data:image' not in img_url:\n",
    "                        post_images.append(img_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Alternative media image extraction failed: {e}\")\n",
    "    \n",
    "    # Remove duplicates and return\n",
    "    return list(set(post_images))\n",
    "\n",
    "# Function to download images \n",
    "def download_images(image_urls, folder_path, post_number):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    for idx, img_url in enumerate(image_urls, start=1):\n",
    "        try:\n",
    "            response = requests.get(img_url, timeout=10)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            # Extract image extension from URL\n",
    "            parsed_url = urlparse(img_url)\n",
    "            _, ext = os.path.splitext(parsed_url.path)\n",
    "            if not ext:\n",
    "                ext = '.jpg'  # Default extension if none found\n",
    "            image_name = f\"image_{idx}{ext}\"\n",
    "            image_path = os.path.join(folder_path, image_name)\n",
    "            with open(image_path, 'wb') as img_file:\n",
    "                img_file.write(response.content)\n",
    "            print(f\"Downloaded {image_name} for Post {post_number}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download image from {img_url}: {e}\")\n",
    "\n",
    "# Function to extract video URLs via network requests\n",
    "def extract_m3u8_via_network(requests):\n",
    "    \"\"\"\n",
    "    Looks for an HLS (.m3u8) playlist in the network requests.\n",
    "    Returns the first .m3u8 URL found, or None if none is found.\n",
    "    \"\"\"\n",
    "    for request in requests:\n",
    "        if request.response:\n",
    "            content_type = request.response.headers.get('Content-Type', '').lower()\n",
    "            if 'application/vnd.apple.mpegurl' in content_type or 'application/x-mpegurl' in content_type:\n",
    "                # We found an HLS playlist\n",
    "                return request.url\n",
    "    return None\n",
    "\n",
    "def download_hls_video(m3u8_url, folder_path, video_name, requests_cookies):\n",
    "    \"\"\"\n",
    "    Downloads an HLS video from the given m3u8_url using ffmpeg,\n",
    "    saving it into folder_path as video_name.\n",
    "    Includes cookies/headers for authentication.\n",
    "    \"\"\"\n",
    "    cookie_string = '; '.join([f'{k}={v}' for k, v in requests_cookies.items()])\n",
    "\n",
    "    # Example: \"Cookie: name1=value1; name2=value2\"\n",
    "    cookie_header = f\"Cookie: {cookie_string}\"\n",
    "    referer_header = \"Referer: https://www.linkedin.com/\"\n",
    "\n",
    "    output_path = os.path.join(folder_path, video_name)\n",
    "\n",
    "    # ffmpeg command:\n",
    "    # -y: overwrite output\n",
    "    # -headers: supply custom HTTP headers (cookie + referer)\n",
    "    # -i: input is the .m3u8 URL\n",
    "    # -c copy: copy the original video/audio data without re-encoding\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-y\",\n",
    "        \"-headers\", cookie_header,\n",
    "        \"-headers\", referer_header,\n",
    "        \"-i\", m3u8_url,\n",
    "        \"-c\", \"copy\",\n",
    "        output_path\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nAttempting to download HLS video to {output_path}\")\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Successfully downloaded HLS video as {output_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"ffmpeg error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading HLS video: {e}\")\n",
    "\n",
    "# Scraping function\n",
    "def scrape_linkedin_posts(max_posts, user_profile_url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    print(\"Initializing the Chrome driver with Selenium Wire...\")\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Verify that 'requests' attribute exists\n",
    "    if not hasattr(browser, 'requests'):\n",
    "        print(\"Error: 'requests' attribute not found in the WebDriver. Ensure Selenium Wire is correctly installed and imported.\")\n",
    "        browser.quit()\n",
    "        return\n",
    "\n",
    "    # Initialize a set to keep track of processed post contents\n",
    "    processed_contents = set()\n",
    "\n",
    "    # Tracking: Load processed_contents from a file to persist across runs\n",
    "    processed_file = 'processed_posts.json'\n",
    "    if os.path.exists(processed_file):\n",
    "        with open(processed_file, 'r') as f:\n",
    "            try:\n",
    "                processed_contents = set(json.load(f))\n",
    "                print(f\"Loaded {len(processed_contents)} previously processed posts.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Processed posts file is empty or corrupted. Starting fresh.\")\n",
    "                processed_contents = set()\n",
    "\n",
    "    try:\n",
    "        # Set the window size\n",
    "        browser.set_window_size(1920, 1080)\n",
    "\n",
    "        # Open LinkedIn login page\n",
    "        print(\"Opening LinkedIn login page...\")\n",
    "        browser.get('https://www.linkedin.com/')\n",
    "\n",
    "        # Wait for the page to load\n",
    "        time.sleep(5)  \n",
    "\n",
    "        # Load cookies from the file\n",
    "        print(\"Loading cookies...\")\n",
    "        load_cookies(browser, 'cookies.txt')  # Ensure 'cookies.txt' is in the same directory\n",
    "        print(\"Cookies loaded.\")\n",
    "\n",
    "        # Refresh the page to apply cookies\n",
    "        browser.refresh()\n",
    "\n",
    "        # Wait for the main navigation bar to be visible\n",
    "        print(\"Waiting for the main navigation bar after applying cookies...\")\n",
    "        try:\n",
    "            WebDriverWait(browser, 30).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, '#global-nav .global-nav__me'))\n",
    "            )\n",
    "            print(\"Navigation bar found. Proceeding with scraping...\")\n",
    "        except TimeoutException:\n",
    "            print(\"TimeoutException: Navigation bar not found after applying cookies. Check if the cookies are correct.\")\n",
    "            return\n",
    "\n",
    "        # Navigate to the user's recent activity page\n",
    "        print(f\"Navigating to the user's recent activity page: {user_profile_url}...\")\n",
    "        browser.get(user_profile_url)\n",
    "\n",
    "        # Wait for the page to load completely\n",
    "        print(\"Waiting for the user's recent activity page to load completely...\")\n",
    "        time.sleep(5)  \n",
    "\n",
    "        # Generate a unique run identifier based on the current datetime\n",
    "        # run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # parent_folder = f\"Scrape_{run_id}\"\n",
    "        # os.makedirs(parent_folder, exist_ok=True)\n",
    "        # print(f\"Created parent folder: {parent_folder}\")\n",
    "        parent_folder = f\"data\"\n",
    "\n",
    "        # Set parameters for scrolling through the page\n",
    "        SCROLL_PAUSE_TIME = 2  # Time to pause between scrolls\n",
    "        LOAD_PAUSE_TIME = 10   # Time to wait after scrolling to load new posts\n",
    "\n",
    "        # Initialize a list to store posts data\n",
    "        posts_data = []\n",
    "        post_count = 0\n",
    "\n",
    "        # Create CSV\n",
    "        csv_file = f\"scraped_linkedin_data.csv\"  \n",
    "        with open(csv_file, mode='w', encoding='utf-8', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Post Number', 'Content', 'Reactions', 'Comments', 'Media Type', 'Image URL', 'Video URL', 'Article URL'])\n",
    "\n",
    "        # Get session cookies for requests\n",
    "        requests_cookies = get_requests_cookies(browser)\n",
    "\n",
    "        while post_count < max_posts:\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            user_page = browser.page_source\n",
    "            linkedin_soup = bs(user_page.encode(\"utf-8\"), \"html.parser\")\n",
    "\n",
    "            # Extract post containers from the HTML\n",
    "            containers = linkedin_soup.find_all(\"div\", {\"class\": \"occludable-update\"})\n",
    "\n",
    "            # Clear previous network requests\n",
    "            browser.requests.clear()\n",
    "\n",
    "            # Main loop to process each container\n",
    "            print(\"Processing each container to extract post data...\")\n",
    "            for container in containers:\n",
    "                if post_count >= max_posts:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    # Extract post content\n",
    "                    post_content_container = container.find(\"div\", {\"class\": \"update-components-text\"})\n",
    "                    post_content = post_content_container.get_text(separator=' ').strip() if post_content_container else \"No content\"\n",
    "\n",
    "                    # Check if the post has no content and skip if so\n",
    "                    if post_content == \"No content\":\n",
    "                        print(\"Post has no content. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Generate a hash of the post content for duplication detection\n",
    "                    post_hash = hashlib.md5(post_content.encode('utf-8')).hexdigest()\n",
    "\n",
    "                    if post_hash in processed_contents:\n",
    "                        print(\"Duplicate post found based on content. Skipping.\")\n",
    "                        continue  # Skip duplicate posts\n",
    "\n",
    "                    # Add the hash to the set to mark it as processed\n",
    "                    processed_contents.add(post_hash)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting post content: {e}\")\n",
    "                    post_content = \"No content\"\n",
    "                    print(\"Post has no content due to an error. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Extract Reactions\n",
    "                try:\n",
    "                    reactions_button = container.find(\"li\", {\"class\": \"social-details-social-counts__reactions\"}).find(\"button\")\n",
    "                    post_reactions = reactions_button[\"aria-label\"].split(\" \")[0].replace(',', '') if reactions_button else \"0\"\n",
    "                except:\n",
    "                    post_reactions = \"0\"\n",
    "\n",
    "                # Extract Comments\n",
    "                try:\n",
    "                    comments_button = container.find(\"li\", {\"class\": \"social-details-social-counts__comments\"}).find(\"button\")\n",
    "                    post_comments = comments_button[\"aria-label\"].split(\" \")[0].replace(',', '') if comments_button else \"0\"\n",
    "                except:\n",
    "                    post_comments = \"0\"\n",
    "\n",
    "                # Initialize media content lists\n",
    "                post_images = []\n",
    "                post_videos = []\n",
    "                post_links = []\n",
    "\n",
    "                # Extract images\n",
    "                if post_content_container:\n",
    "                    post_images = extract_images(post_content_container)\n",
    "\n",
    "                # Extract links\n",
    "                try:\n",
    "                    if post_content_container:\n",
    "                        links = post_content_container.find_all(\"a\", href=True)\n",
    "                        for link in links:\n",
    "                            href = link['href']\n",
    "                            if href.startswith('http'):  # Ensure it's an absolute URL\n",
    "                                post_links.append(href)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting links: {e}\")\n",
    "\n",
    "                # Remove duplicates\n",
    "                post_images = list(set(post_images))\n",
    "                post_links = list(set(post_links))\n",
    "\n",
    "                # Determine Media Type (default = 'Text')\n",
    "                media_type = 'Text'\n",
    "                videos_str = \"No videos\"\n",
    "                images_str = \"No images\" if not post_images else '; '.join(post_images)\n",
    "                links_str = \"No links\" if not post_links else '; '.join(post_links)\n",
    "\n",
    "                # Increment post count\n",
    "                post_count += 1\n",
    "                post_number = post_count\n",
    "\n",
    "                # Create a folder for the current post\n",
    "                folder_name = f\"Post_{post_number}\"\n",
    "                folder_path = os.path.join(parent_folder, folder_name)\n",
    "                os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "                # Download images if any\n",
    "                if post_images:\n",
    "                    download_images(post_images, folder_path, post_number)\n",
    "\n",
    "                # Scroll to the post to ensure potential videos are loaded\n",
    "                try:\n",
    "                    selenium_post_element = browser.find_element(\n",
    "                        By.XPATH, f\"(//div[contains(@class, 'occludable-update')])[{post_number}]\"\n",
    "                    )\n",
    "                    browser.execute_script(\"arguments[0].scrollIntoView();\", selenium_post_element)\n",
    "                    time.sleep(2)  # Wait for dynamic content\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scrolling to post {post_number}: {e}\")\n",
    "\n",
    "                has_video_element = bool(container.find(\"video\"))  \n",
    "\n",
    "                # Clear previous requests again, just to capture fresh requests for the current post\n",
    "                browser.requests.clear()\n",
    "\n",
    "                if has_video_element:\n",
    "                    print(\"Video element detected in the DOM. Waiting for network requests...\")\n",
    "                    # Wait a bit to capture video requests\n",
    "                    time.sleep(LOAD_PAUSE_TIME)\n",
    "\n",
    "                    # Extract video URLs from network requests\n",
    "                    m3u8_url = extract_m3u8_via_network(browser.requests)\n",
    "                    if m3u8_url:\n",
    "                        print(f\"Found M3U8 link for Post {post_number}: {m3u8_url}\")\n",
    "                        # We'll treat it as a video post\n",
    "                        media_type = 'Video'\n",
    "                        videos_str = m3u8_url\n",
    "                        # Download using ffmpeg\n",
    "                        download_hls_video(\n",
    "                            m3u8_url=m3u8_url,\n",
    "                            folder_path=folder_path,\n",
    "                            video_name=\"video_hls.mp4\",\n",
    "                            requests_cookies=requests_cookies\n",
    "                        )\n",
    "                    else:\n",
    "                        print(f\"No valid MP4 or M3U8 link found for Post {post_number}.\")\n",
    "                        # This post might not have a playable video\n",
    "                        media_type = \"Text\" \n",
    "                        videos_str = \"No videos\"\n",
    "                else:\n",
    "                    print(f\"No <video> element found for Post {post_number}. Skipping video extraction.\")\n",
    "\n",
    "                # If there's no video, check if it might be an article link or image\n",
    "                if media_type != 'Video':\n",
    "                    if any('articleshare-shrink' in img_url for img_url in post_images):\n",
    "                        media_type = 'Article Link'\n",
    "                    elif post_images:\n",
    "                        media_type = 'Image'\n",
    "                    else:\n",
    "                        media_type = 'Text'\n",
    "\n",
    "                # Append post data\n",
    "                posts_data.append({\n",
    "                    'Post Number': post_number,\n",
    "                    'Content': post_content,\n",
    "                    'Reactions': post_reactions,\n",
    "                    'Comments': post_comments,\n",
    "                    'Media Type': media_type,\n",
    "                    'Images URL': images_str,\n",
    "                    'Videos URL': videos_str,\n",
    "                    'Links': links_str,\n",
    "                })\n",
    "\n",
    "                print(f\"Post {post_count} processed and data saved.\")\n",
    "\n",
    "                # Save the post data to the CSV file immediately\n",
    "                with open(csv_file, mode='a', encoding='utf-8', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([\n",
    "                        post_number,\n",
    "                        post_content,\n",
    "                        post_reactions,\n",
    "                        post_comments,\n",
    "                        media_type,\n",
    "                        images_str,\n",
    "                        videos_str,\n",
    "                        links_str\n",
    "                    ])\n",
    "\n",
    "                # Tracking: Save the updated processed_contents to persist across runs\n",
    "                with open(processed_file, 'w') as f:\n",
    "                    json.dump(list(processed_contents), f)\n",
    "\n",
    "            if post_count < max_posts:\n",
    "                # Scroll down to load more posts\n",
    "                print(\"Scrolling down to load more posts...\")\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(LOAD_PAUSE_TIME)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        print(\"Finished scrolling and processing posts.\")\n",
    "        print(f\"Data exported to {csv_file}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        browser.quit()\n",
    "\n",
    "# Function to handle user input and initiate scraping\n",
    "def start_scraping(button):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        try:\n",
    "            max_posts = int(post_count_widget.value)\n",
    "            user_profile_url = profile_url_widget.value.strip()\n",
    "            if not user_profile_url:\n",
    "                print(\"Please enter a valid LinkedIn profile URL.\")\n",
    "                return\n",
    "            print(f\"Starting scraping for {max_posts} posts from {user_profile_url}...\")\n",
    "            scrape_linkedin_posts(max_posts, user_profile_url)\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number for the number of posts.\")\n",
    "\n",
    "# Create interactive widgets\n",
    "post_count_widget = widgets.BoundedIntText(\n",
    "    value=10,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Number of Posts:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "profile_url_widget = widgets.Textarea(\n",
    "    value='https://www.linkedin.com/school/nyushanghai/posts/?feedView=all',\n",
    "    placeholder='Enter LinkedIn Profile URL',\n",
    "    description='Profile URL:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='60px')  # Adjust size as needed\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Start Scraping',\n",
    "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to start scraping',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "submit_button.on_click(start_scraping)\n",
    "\n",
    "# Display the widgets\n",
    "display(post_count_widget, profile_url_widget, submit_button, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
